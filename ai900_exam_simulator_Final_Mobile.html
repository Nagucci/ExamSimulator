
<!DOCTYPE html>
<html lang="en" data-theme="dark" data-view="desktop">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>AI-900 Exam Simulator</title>
  <style>
    :root {
      --bg: #0b1a2b;
      --panel: #11263d;
      --text: #eaf3ff;
      --muted: #b5c7dc;
      --accent: #4da3ff;
      --border: #1f3850;
      --danger: #ff6b6b;
      --success: #48d597;
      --img-border: #1f3850;
    }
    [data-theme="dark"] {
      --bg: #0b1a2b;
      --panel: #11263d;
      --text: #eaf3ff;
      --muted: #b5c7dc;
      --accent: #4da3ff;
      --border: #1f3850;
      --danger: #ff6b6b;
      --success: #48d597;
      --img-border: #1f3850;
    }
    [data-theme="light"] {
      --bg: #f7fbff;
      --panel: #ffffff;
      --text: #0b1a2b;
      --muted: #47627f;
      --accent: #0067c0;
      --border: #cbd6e2;
      --danger: #c62828;
      --success: #2e7d32;
      --img-border: #cbd6e2;
    }
    html, body { height: 100%; margin: 0; }
    body {
      background-color: var(--bg);
      color: var(--text);
      display: flex; flex-direction: column;
      font-family: system-ui, -apple-system, Segoe UI, Roboto, Arial, sans-serif;
    }
    header {
      padding: .75rem 1rem;
      border-bottom: 1px solid var(--border);
      background: linear-gradient(180deg, rgba(0,0,0,.06) 0%, transparent 100%);
      position: sticky; top: 0; z-index: 10;
      display: grid; grid-template-columns: 1fr auto; gap: .75rem; align-items: center;
    }
    header h1 { font-size: 1.1rem; margin: 0; }
    header .sub { color: var(--muted); font-size: .9rem; }
    .header-actions { display: flex; gap: .5rem; flex-wrap: wrap; align-items: center; }
    .container {
      flex: 1;
      display: grid;
      grid-template-columns: 1fr;
      gap: 1rem;
      padding: 1rem;
    }
    .card { background: var(--panel); border: 1px solid var(--border); border-radius: .75rem; padding: 1rem; }
    .question-text { font-weight: 600; margin-bottom: .75rem; }
    .options { display: grid; gap: .5rem; }
    .option { background: color-mix(in srgb, var(--panel) 85%, var(--bg)); border: 1px solid var(--border); border-radius: .5rem; padding: .5rem .75rem; }
    .option input { margin-right: .5rem; }
    .select-wrap select {
      width: 100%;
      padding: .5rem .75rem;
      border: 1px solid var(--border);
      border-radius: .5rem;
      background: var(--panel);
      color: var(--text);
      outline: none;
    }
    .yn-group { display: grid; gap: .75rem; }
    .yn-item { display: grid; gap: .5rem; margin-top: .25rem; }
    .yn-options { display: flex; gap: 1rem; align-items: center; flex-wrap: wrap; }
    .yn-option { display: inline-flex; align-items: center; gap: .4rem; }
    .hotspot-prompt { margin-bottom: .5rem; color: var(--text); }
    .hotspot-area { margin-top: .5rem; }
    .hotspot-area-title { font-weight: 600; margin-bottom: .35rem; }
    .q-images { display: grid; gap: .5rem; margin: .5rem 0 .75rem 0; }
    .q-images img { max-width: 100%; border-radius: .5rem; border: 1px solid var(--img-border); }
    .dnd-list, .dnd-buckets { display: grid; gap: .5rem; }
    .dnd-item {
      background: color-mix(in srgb, var(--panel) 85%, var(--bg));
      border: 1px solid var(--border);
      border-radius: .5rem; padding: .5rem .75rem;
      cursor: grab; user-select: none;
    }
    .dnd-item:active { cursor: grabbing; }
    .dnd-item.dragging { opacity: .6; }
    .dnd-item.drag-over { outline: 2px dashed var(--accent); }
    .dnd-bucket {
      border: 2px dashed var(--border);
      border-radius: .5rem; padding: .5rem .75rem;
      min-height: 3rem; background: color-mix(in srgb, var(--panel) 65%, var(--bg));
    }
    .dnd-bucket-title { font-weight: 600; margin-bottom: .35rem; }
    .dnd-bucket.over { border-color: var(--accent); }
    .footer {
      position: sticky; bottom: 0; z-index: 15;
      background: linear-gradient(180deg, rgba(0,0,0,.06) 0%, transparent 100%);
      border-top: 1px solid var(--border);
      padding: .75rem 1rem;
      display: grid; grid-template-columns: 1fr auto; gap: .75rem;
    }
    .footer .left, .footer .right { display: flex; gap: .5rem; align-items: center; flex-wrap: wrap; }
    button {
      background: var(--accent); color: #fff; border: none; border-radius: .5rem;
      padding: .5rem .75rem; font-weight: 600; cursor: pointer;
    }
    button.secondary { background: transparent; color: var(--text); border: 1px solid var(--border); }
    button.danger { background: var(--danger); }
    button:disabled { opacity: .6; cursor: not-allowed; }
    .status { color: var(--muted); font-size: .9rem; }
    .pill { display: inline-block; padding: .15rem .45rem; border-radius: 999px; border: 1px solid var(--border); }
    .alert { padding: .75rem; border-radius: .5rem; margin-top: .5rem; }
    [data-theme="dark"] .alert.ok {
      background: #113a2b;
      border: 1px solid #1f5f4b;
      color: #48d597;
    }
    [data-theme="light"] .alert.ok {
      background: #eafaf1;
      color: var(--success);
      border-color: #b2e2c7;
    }
    .alert.bad { background: #ffeaea; color: var(--danger); border-color: #f5b2b2; }
    .mark-review { margin-top: .5rem; font-size: .95rem; }

    /* PASS/FAIL pill styles */
    .pill.pass {
      border-color: var(--success);
      color: var(--success);
      background: color-mix(in srgb, var(--success) 12%, transparent);
    }
    .pill.fail {
      border-color: var(--danger);
      color: var(--danger);
      background: color-mix(in srgb, var(--danger) 12%, transparent);
    }

    #metaPanel { min-width: 260px; }
    #metaPanel .status, #metaPanel #resultSummary, #metaPanel #reviewPanel {
      margin-bottom: 1rem;
      display: block;
    }
    #metaPanel #reviewPanel button {
      width: 100%;
      margin-top: .5rem;
    }
    @media (min-width: 900px) {
      .container { grid-template-columns: 1fr 320px; }
      #metaPanel { margin-left: 0; }
    }
    @media (max-width: 899px) {
      .container { grid-template-columns: 1fr; }
      #metaPanel { margin-top: 1rem; }
    }

    /* Bucket clones + invalid item visualization */
    .dnd-item.clone {
      display: inline-flex;
      align-items: center;
      gap: .5rem;
    }
    .dnd-item.invalid {
      border-color: var(--danger);
      box-shadow: 0 0 0 2px color-mix(in srgb, var(--danger) 35%, transparent);
    }
    .dnd-item .remove-btn {
      margin-left: auto;
      background: transparent;
      color: var(--muted);
      border: 1px solid var(--border);
      border-radius: .35rem;
      padding: .15rem .35rem;
      font-size: .8rem;
      cursor: pointer;
    }
    .dnd-item .remove-btn:hover { color: var(--text); }

    /* ----- Non-sticky header/footer on Mobile ----- */
    html[data-view="mobile"] header {
      position: static;
      top: auto;
      z-index: auto;
      background: var(--panel);
      border-bottom: 1px solid var(--border);
    }
    html[data-view="mobile"] .footer {
      position: static;
      bottom: auto;
      z-index: auto;
    }
    html[data-view="desktop"] header {
      background: linear-gradient(180deg, rgba(0,0,0,.06) 0%, transparent 100%);
    }
    html[data-view="desktop"] .footer {
      background: linear-gradient(180deg, rgba(0,0,0,.06) 0%, transparent 100%);
    }

    /* ====================== MOBILE VIEW OVERRIDES ====================== */
    html[data-view="mobile"] .container { grid-template-columns: 1fr; }
    html[data-view="mobile"] header .sub { display: none; }
    html[data-view="mobile"] .footer { grid-template-columns: 1fr; }
    html[data-view="mobile"] .footer .left,
    html[data-view="mobile"] .footer .right { flex-direction: column; align-items: stretch; gap: .5rem; }
    html[data-view="mobile"] .footer button { width: 100%; }
    html[data-view="mobile"] #metaPanel { display: none; }
    html[data-view="mobile"] .option,
    html[data-view="mobile"] .dnd-item,
    html[data-view="mobile"] .dnd-bucket { padding: .75rem 1rem; }
  </style>
</head>
<body>
  <header>
    <div>
      <h1>AI-900 Exam Simulator</h1>
      <span id="timerBadge" class="pill" style="margin-left:.5rem;">Time left: --:--</span>
      <button id="pauseBtn" class="btn" disabled>Pause</button>
    </div>
    <div class="header-actions">
      <button id="themeBtn" class="secondary" title="Toggle light/dark">Theme: Dark</button>
      <button id="viewBtn" class="secondary" title="Toggle desktop/mobile UI">UI: Desktop</button>
      <button id="shuffleBtn" class="secondary" disabled>Shuffle Questions</button>
      <button id="loadJsonBtn" class="secondary" disabled>Load Embedded JSON</button>
      <button id="pickJsonBtn" class="secondary" enabled>Load JSON File…</button>
      <input id="fileInput" type="file" accept="application/json" style="display:none" />
    </div>
  </header>

  <!-- Timer panel -->
  <div id="timerPanel" class="card" style="margin: 1rem;">
    <label>
      Set exam duration:
      <input type="number" id="durationInput" min="1" max="300" value="180" style="width:4em;" /> minutes
    </label>
    <button id="startExamBtn">Start Exam</button>
    
    <span id="timerDisplay" style="margin-left:2em;font-weight:bold;"></span>
  </div>

  <main class="container">
    <section class="card" id="questionPanel" style="display:none;">
      <div id="questionContainer"></div>
      <div id="feedbackContainer"></div>
    </section>

    <aside class="card" id="metaPanel" style="display:none;">
      <div class="status"><span class="pill" id="progressPill">Question 1 / 1</span></div>
      <div id="resultSummary" style="margin-top: .75rem;"></div>
      <div id="reviewPanel">
        <button id="reviewMarkedBtn" class="secondary" disabled>Review Marked Questions</button>
      </div>
    </aside>
  </main>

  <footer class="footer" style="display:none;">
    <div class="left">
      <button id="prevBtn" class="secondary" disabled>◀ Prev</button>
      <button id="nextBtn" class="secondary" disabled>Next ▶</button>
    </div>
    <div class="right">
      <button id="submitBtn" disabled>Submit</button>
      <button id="calcScoreBtn" class="secondary" disabled>Calculate Score</button>
      <button id="reviewMarkedBtnMobile" class="secondary mobile-only">Review Marked</button>
      <button id="resetBtn" class="danger" disabled>Reset</button>
    </div>
  </footer>

  <!-- Embedded questions JSON (demo) -->
  <script id="embedded-questions" type="application/json">
  {
    "title": "AI-900 Exam Simulator",
    "passScore": 70,
    "questions":
[
    {
        "id": 1,
        "type": "single_choice",
        "question": "Which business benefit should the company expect as a result of creating the webchat bot solution?",
        "options": [
            "A. increased sales",
            "B. a reduced workload for the customer service agents",
            "C. improved product reliability"
        ],
        "answer": "B. a reduced workload for the customer service agents",
        "explanation": "Correct Answer is B. A webchat bot is designed to automate responses to common customer queries, reducing the workload for customer service agents.",
        "images": [],
        "interactivity": null
    },
    {
        "id": 2,
        "type": "single_choice",
        "question": "For a machine learning progress, how should you split data for training and evaluation?",
        "options": [
            "A. Use features for training and labels for evaluation.",
            "B. Randomly split the data into rows for training and rows for evaluation.",
            "C. Use labels for training and features for evaluation.",
            "D. Randomly split the data into columns for training and columns for evaluation."
        ],
        "answer": "B. Randomly split the data into rows for training and rows for evaluation.",
        "explanation": "Correct Answer is B. You split rows not columns: The Split Data module is particularly useful when you need to separate data into training and testing sets. Use the Split Rows option if you want to divide the data into two parts.",
        "images": [],
        "interactivity": null
    },


    {
        "id": 3,
        "type": "hotspot",
        "question": "You are developing a model to predict events by using classification. You have a confusion matrix for the model scored on test data as shown in the following exhibit",
        "options": [
            "11",
            "5",
            "1033",
            "13951"
        ],
        "answer": [],
        "explanation": "Correct Answer is 11, 1033: because 11 represent 1 as Actual and 1 as predicted. while the second question is 1033 as it representes, 1 as actual and 0 as predicted which is the False Negative Feature = Input. Label = Output.",
        "images": [
            "https://bongawat.wordpress.com/wp-content/uploads/2026/01/ai900_q40.png?w=NaN&h="
        ],
        "interactivity": {
            "hotspot_areas": [
                {
                    "prompt": "There are [Answer Choice] correctly predicted positives.",
                    "answer": "11"
                },
                {
                    "prompt": "There are [Answer Choice] false negatives.",
                    "answer": "1033"
                }
            ]
        }
    },
    {
        "id": 4,
        "type": "single_choice",
        "question": "You build a machine learning model by using the automated machine learning user interface (UI). You need to ensure that the model meets the Microsoft transparency principle for responsible AI. What should you do?",
        "options": [
            "A. Set Validation type to Auto.",
            "B. Enable Explain best model.",
            "C. Set Primary metric to accuracy.",
            "D. Set Max concurrent iterations to 0."
        ],
        "answer": "B. Enable Explain best model.",
        "explanation": "Correct answer is B. Model Explain Ability. Most businesses run on trust and being able to open the ML black box helps build transparency and trust.",
        "interactivity": null
    },
    {
        "id": 5,
        "type": "boolean",
        "question": "For each of the following statements, select Yes if the statement is true. Otherwise, select No.",
        "subQuestions": [
            {
                "prompt": "Forecasting housing prices based on historical data is an example of anomaly detection.",
                "correctAnswer": false
            },
            {
                "prompt": "Identifying suspicious sign-ins by looking for deviations from usual patterns is an example of anomaly detection.",
                "correctAnswer": true
            },
            {
                "prompt": "Predicting whether a patient will develop diabetes based on the patient's medical history is an example of anomaly detection.",
                "correctAnswer": false
            }
        ],
        "explanation": "Correct Answer is No, Yes, No. A - Regression B - Anomly C - Cassification Anomaly detection encompasses many important tasks in machine learning: Identifying transactions that are potentially fraudulent. Learning patterns that indicate that a network intrusion has occurred. Finding abnormal clusters of patients. Checking values entered into a system. Reference: https://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/anomaly-detection",
        "images": []
    },
{
        "id": 6,
        "type": "dropdown",
        "question": "The handleing of unsual or missing values provided to an AI system is a consideration for the Microsoft _____________  principle for responsible AI",
        "options": [
          { "label": "Reliability and safety", "value": "Reliability and safety" },
          { "label": "Inclusiveness", "value":  "Inclusiveness" },
          { "label": "privacy and security", "value": "privacy and security" },
{ "label": "transparency", "value": "transparency" }
        ],
        "correctAnswer": "Reliability and safety",
        "placeholder": "— Select an option —",
        "explanation": "Correct Answer is Reliability and Safety. AI systems need to be reliable and safe in order to be trusted. It is important for a system to perform as it was originally designed and for it to respond safely to new situations."
      },

  {
        "id": 7,
        "type": "bucket",
        "layout": "bucket",
        "question": "To answer, drag the appropriate workload type from the column on the left to its scenario on the right. Each workload type may be used once, more than once, or not at all.",
        "options": [
          { "id": "Computer vision", "label": "Computer vision" },
          { "id": "Anomaly detection", "label": "Anomaly detection" },
          { "id": "Conversational AI", "label": "Conversational AI" },
          { "id": "Knowledge mining", "label": "Knowledge mining" },
          { "id": "Natural language processing", "label": "Natural language processing" }
        ],
        "buckets": [
          { "id": "An automated chat to answer questions about refunds and exchanges", "accepts": ["Conversational AI"] },
          { "id": "Determining whether a photo contains a person", "accepts": ["Computer vision"] },
          { "id": "Determining whether a review is positive or negative", "accepts": ["Natural language processing"] }
        ],
        "explanation": "Box 1: Conversational AI, Box 2: Computer Vision, Box 3: NLP."
      },
    {
        "id": 8,
        "type": "single_choice",
        "question": "You are designing an AI system that empowers everyone, including people who have hearing, visual, and other impairments. This is an example of which Microsoft guiding principle for responsible AI?",
        "options": [
            "A. fairness",
            "B. inclusiveness",
            "C. reliability and safety",
            "D. accountability"
        ],
        "answer": "B. inclusiveness",
        "explanation": "Answer is B. Inclusiveness: At Microsoft, we firmly believe everyone should benefit from intelligent technology, meaning it must incorporate and address a broad range of human needs and experiences. For the 1 billion people with disabilities around the world, AI technologies can be a game-changer. Reference: https://docs.microsoft.com/en-us/learn/modules/responsible-ai-principles/4-guiding-principles",
        "images": [],
        "interactivity": null
    },
    {
        "id": 9,
        "type": "bucket",
        "layout": "bucket",

        "question": "Match the Microsoft guiding principles for responsible AI to the appropriate descriptions.",
 "options": [
          { "id": "Accountability", "label": "Accountability" },
          { "id": "Fairness", "label": "Fairness" },
          { "id": "Inclusiveness", "label": "Inclusiveness" },
          { "id": "Privacy and Security", "label": "Privacy and Security" },
          { "id": "Reliability and Safety", "label": "Reliability and Safety" }
        ],
        "buckets": [
          { "id": "Ensure that AI systems operate as they were orginally designed, respond to unanticipated conditions, and resist harmful manipulation.", "accepts": ["Reliability and Safety"] },
          { "id": "Implementing processes to ensure that decisions made by AI systems can be overridden by humans.", "accepts": ["Accountability"] },
          { "id": "Provide consumers with information and controls over the collection, use, and storage of their data.", "accepts": ["Privacy and Security"] }
        ],        

        "explanation": "Box 1: Reliability and safety -To build trust, it's critical that AI systems operate reliably, safely, and consistently under normalcircumstances and in unexpected conditions. These systems should be able to operate as they were originally designed, respond safely to unanticipated conditions, and resist harmful manipulation.  Box 2: Accountability - The people who design and deploy AI systems must be accountable for how their systems operate. Organizations should draw upon industry standards to develop accountability norms. These norms can ensure that AI systems are not the final authority on any decision that impacts people's lives and that humans maintain meaningful control over otherwise highly autonomous AI systems. Box 3: Privacy and security - As AI becomes more prevalent, protecting privacy and securing important personal and business information is becoming more critical and complex. With AI, privacy and data security issues require especially close attention because access to data is essential for AI systems to make accurate and informed predictions and decisions about people. AI systems must comply with privacy laws that require transparency about the collection, use, and storage of data and mandate that consumers have appropriate controls to choose how their data is used"
    },
       

{
"id": 10,
        "type": "dropdown",
        "question": "When developing an AI system or self-driving cars, the Microsoft _____________ principle for responsible AI should be applied to ensure consistent operation system during unexpected circumstances.",

        "options": [
          { "label": "Reliability and safety", "value": "Reliability and safety" },
          { "label": "Accountability", "value":  "Accountability" },
          { "label": "Fairness", "value": "Fairness" },
          { "label": "Inclusiveness", "value": "Inclusiveness" }
        ],
        "correctAnswer": "Reliability and safety",
        "placeholder": "— Select an option —",
        "explanation": "Reliability and safety: To build trust, it's critical that AI systems operate reliably, safely, and consistently under normal circumstances and in unexpected conditions."
      },

     {
        "id": 11,
        "type": "single_choice",
        "question": "You are building an AI system. Which task should you include to ensure that the service meets the Microsoft transparency principle for responsible AI?",
        "options": [
            "A. Ensure that all visuals have an associated text that can be read by a screen reader.",
            "B. Enable autoscaling to ensure that a service scales based on demand.",
            "C. Provide documentation to help developers debug code.",
            "D. Ensure that a training dataset is representative of the population."
        ],
        "answer": "C. Provide documentation to help developers debug code.",
        "explanation": "Answer is C. The Microsoft transparency principle for responsible AI emphasizes that AI systems should be understandable and explainable to users, developers, and other stakeholders.",
        "images": [],
        "interactivity": null
    },
    {
        "id": 12,
        "type": "bucket",
        "layout": "bucket",

        "question": "Match the types of AI workloads to the appropriate scenarios.",
        
 "options": [
          { "id": "Computer vision", "label": "Computer vision" },
          { "id": "NLP", "label": "NLP" },
          { "id":  "Anomaly Detection", "label":  "Anomaly Detection" },
          { "id":  "Machine Learning (regression)", "label":  "Machine Learning (regression)" }

        ],
        "buckets": [
          { "id": "Identify handwritten letters.", "accepts": ["Computer vision"] },
          { "id": "Predict the sentiment of a social media post.", "accepts": ["NLP"] },
          { "id": "Identify a fraudulent credit card payment.", "accepts": ["Anomaly Detection"] },
          { "id": "Predict next month's toy sales.", "accepts": ["Machine Learning (regression)"] }
        ],        


        "explanation": "Box1: Computer Vision, Box2: NLP, Box3: Anomaly Detection, Box4: Regression Reference: https://docs.microsoft.com/en-us/learn/paths/get-started-with-artificial-intelligence-on-azure/"
    },
 
           {
        "id": 13,
        "type": "single_choice",
        "question": "Your company is exploring the use of voice recognition technologies in its smart home devices. The company wants to identify any barriers that might unintentionally leave out specific user groups. This is an example of which Microsoft guiding principle for responsible AI?",
        "options": [
            "A. accountability",
            "B. fairness",
            "C. inclusiveness",
            "D. privacy and security"
        ],
        "answer": "C. inclusiveness",
        "explanation": "Answer is C. Inclusiveness. No one is left out (disabled, gender, ethnicity, LGBTQIA+ etc etc)",
        "images": [],
        "interactivity": null
    },
    {
        "id": 14,
        "type": "multiple_choice",
        "question": "What are three Microsoft guiding principles for responsible AI? Each correct answer presents a complete solution.",
        "options": [
            "A. knowledgeability",
            "B. decisiveness",
            "C. inclusiveness",
            "D. fairness",
            "E. opinionatedness",
            "F. reliability and safety"
        ],
        "answer": [
            "C. inclusiveness",
            "D. fairness",
            "F. reliability and safety"
        ],
        "allow_multiple": true,
        "expected_selection_count": 3,
        "explanation": "The six guiding principles are: 1. Fairness 2. Inclusiveness 3. Transparency 4. Privacy and Security 5. Reliability and Safety 6. Accountability",
        "images": [],
        "interactivity": null
    },

{
"id": 15,
        "type": "dropdown",
        "question": "Returning a bounding box that indicates the location of a vehicle in an image is an example of _____________ ",

        "options": [
          { "label": "Image clssification", "value": "Image clssification" },
          { "label": "Object detection", "value":  "Object detection" },
          { "label": "Optical character recognizer (OCR)", "value": "Optical character recognizer (OCR)" },
          { "label": "semantic segmentation", "value": "semantic segmentation" }
        ],
        "correctAnswer": "Object detection",
        "placeholder": "— Select an option —",
        "explanation": "Object detection is correct. Semantic segmentation can seem tempting at first but that is more about classifying individual pixels based on their objects."
      },

    {
	"id": 16,
        "type": "dropdown",
        "question": "_____________  is used to generate additional features.",

        "options": [
          { "label": "Feature Engineering", "value": "Feature Engineering" },
          { "label": "Feature selection", "value":  "Feature selection"},
          { "label": "Model evaluation", "value": "Model evaluation"},
          { "label": "Model training", "value": "Model training" }
        ],
        "correctAnswer": "Feature Engineering",
        "placeholder": "— Select an option —",
        "explanation": "Feature engineering is applied first to generate additional features, and then feature selection is done to eliminate irrelevant, redundant, or highly correlated features."
      },
        
    {
        "id": 17,
        "type": "single_choice",
        "question": "You run a charity event that involves posting photos of people wearing sunglasses on Twitter. You need to ensure that you only retweet photos that meet the following requirements: Include one or more faces. Contain at least one person wearing sunglasses. What should you use to analyze the images?",
        "options": [
            "A. the Verify operation in the Face service",
            "B. the Detect operation in the Face service",
            "C. the Describe Image operation in the Computer Vision service",
            "D. the Analyze Image operation in the Computer Vision service"
        ],
        "answer": "B. the Detect operation in the Face service",
        "explanation": "Answer is B. Face detect can be requested to detect also glasses attribute Reference: https://docs.microsoft.com/en-us/azure/cognitive-services/face/overview",
        "images": [],
        "interactivity": null
    },
    {
        "id": 18,
        "type": "single_choice",
        "question": "When you design an AI system to assess whether loans should be approved, the factors used to make the decision should be explainable. This is an example of which Microsoft guiding principle for responsible AI?",
        "options": [
            "A. transparency",
            "B. inclusiveness",
            "C. fairness",
            "D. privacy and security"
        ],
        "answer": "A. transparency",
        "explanation": "Answer is A. Achieving transparency helps the team to understand the data and algorithms used to train the model, what transformation logic was applied to the data, the final model generated, and its associated assets. This information offers insights about how the model was created, which allows it to be reproduced in a transparent way. Incorrect Answers: B: Inclusiveness mandates that AI should consider all human races and experiences, and inclusive design practices can help developers to understand and address potential barriers that could unintentionally exclude people. Where possible, speech-to-text, text-to-speech, and visual recognition technology should be used to empower people with hearing, visual, and other impairments.C: Fairness is a core ethical principle that all humans aim to understand and apply. This principle is even more important when AI systems are being developed. Key checks and balances need to make sure that the system's decisions don't discriminate or run a gender, race, sexual orientation, or religion bias toward a group or individual. D: A data holder is obligated to protect the data in an AI system, and privacy and security are an integral part of this system. Personal needs to be secured, and it should be accessed in a way that doesn't compromise an individual's privacy. Reference: https://docs.microsoft.com/en-us/azure/cloud-adoption-framework/innovate/best-practices/trusted-ai https://docs.microsoft.com/en-us/azure/cloud-adoption-framework/strategy/responsible-ai",
        "images": [],
        "interactivity": null
    },
    {
        "id": 19,
        "type": "boolean",
        "question": "For each of the following statements, select Yes if the statement is true. Otherwise, select No.",
        "subQuestions": [
            {
                "prompt": "Providing an explanation of the outcome of a credit loan is an example of the Microsoft transparency principle of responsible AI.",
                "correctAnswer": true
            },
            {
                "prompt": "A triage bot that prioritizes insurance claims based on injuries is an example of the Microsoft reliability and safety principle of responsible AI.",
                "correctAnswer": false
            },
            {
                "prompt": "An AI solution that is offered at different prices for different sales territories is an example of the Microsoft inclusiveness principle of responsible AI.",
                "correctAnswer": false
            }
        ],
        "explanation": "Box 1: Yes - Achieving transparency helps the team to understand the data and algorithms used to train the model, what transformation logic was applied to the data, the final model generated, and its associated assets. This information offers insights about how the model was created, which allows it to be reproduced in a transparent way. Box 2: No - A data holder is obligated to protect the data in an AI system, and privacy and security are an integral part of this system. Personal needs to be secured, and it should be accessed in a way that doesn't compromise an individual's privacy. Box 3: No - Inclusiveness mandates that AI should consider all human races and experiences, and inclusive design practices can help developers to understand and address potential barriers that could unintentionally exclude people. Where possible, speech-to-text, text-to-speech, and visual recognition technology should be used to empower people with hearing, visual, and other impairments. So correct answer should be fairness. Reference: https://docs.microsoft.com/en-us/azure/cloud-adoption-framework/innovate/best-practices/trusted-ai",
        "images": []
    },
    {
        "id": 20,
                "type": "bucket",
        "layout": "bucket",
        "question": "Match the principles of responsible AI to appropriate requirements.",
  

 "options": [
          { "id": "Reliability and safety", "label": "Reliability and safety" },
          { "id": "Fairness", "label": "Fairness" },
          { "id":  "Transparency", "label":  "Transparency" },
          { "id": "Privacy and Security", "label": "Privacy and Security" }
        ],
        "buckets": [
          { "id": "The System must not discriminate based on gender, race.", "accepts": ["Fairness"] },
          { "id": "Personal data must be visible only to approve.", "accepts": ["Privacy and security"] },
          { "id": "Automated decision-making processes must be recorded so that approved users can identify why a decision was made.", "accepts": ["Transparency"] }
        ],        
                "explanation": "Box1: Fairness, Box2: Privacy and Security, Box3: Transparency. Reference: https://docs.microsoft.com/en-us/azure/cloud-adoption-framework/innovate/best-practices/trusted-ai https://docs.microsoft.com/en-us/learn/modules/responsible-ai-principles/4-guiding-principles"
       
    },

    {
        "id": 21,
  "type": "bucket",
        "layout": "bucket",

         "question": "You plan to deploy an Azure Machine Learning model as a service that will be used by client applications. Which three processes should you perform in sequence before you deploy the model? To answer, move the appropriate processes from the list of processes to the answer area and arrange them in the correct order. Select and Place:",
       
 "options": [
          { "id": "Data encryption", "label": "Data encryption" },
          { "id": "Model retraining", "label": "Model retraining" },
          { "id": "Model training", "label": "Model training" },
          { "id": "Data preparation", "label":  "Data preparation" },
          { "id": "Model evaluation", "label": "Model evaluation" }
        ],
        "buckets": [
          { "id": "Process number 1.", "accepts": ["Data preparation"] },
          { "id": "Process number 2.", "accepts": ["Model training"] },
          { "id": "Process number 3.", "accepts": ["Model evaluation"] }
        ],  
       
       
                "explanation": "Box1: Data Preparation, Box2: Model Training, Box 3: Model Evaluation. Reference: https://docs.microsoft.com/en-us/azure/machine-learning/concept-ml-pipelines",
        "images": []

    },
    {
        "id": 22,
        "type": "multiple_choice",
        "question": "You are building an AI-based app. You need to ensure that the app uses the principles for responsible AI. Which two principles should you follow?",
        "options": [
            "A. Implement an Agile software development methodology",
            "B. Implement a process of AI model validation as part of the software review process",
            "C. Establish a risk governance committee that includes members of the legal team, members of the risk management team, and a privacy officer",
            "D. Prevent the disclosure of the use of AI-based algorithms for automated decision making"
        ],
        "answer": [
            "B. Implement a process of AI model validation as part of the software review process",
            "C. Establish a risk governance committee that includes members of the legal team, members of the risk management team, and a privacy officer"
        ],
        "allow_multiple": true,
        "expected_selection_count": 2,
        "explanation": "B ensures reliability and safety principle and C ensures privacy and security principle of AI. Reference: https://docs.microsoft.com/en-us/azure/cloud-adoption-framework/innovate/best-practices/trusted-ai https://docs.microsoft.com/en-us/learn/modules/responsible-ai-principles/3-implications-responsible-aipractical",
        "images": [],
        "interactivity": null
    },
 {
	"id": 23,
        "type": "dropdown",
        "question": "According to Microsoft's _____________  Principle of responsible AI, AI systems should not reflect biases from the data sets that are used to train the systems.",

        "options": [
          { "label":  "Accountability", "value":  "Accountability" },
          { "label": "Fairness", "value":  "Fairness"},
          { "label":  "Inclusiveness", "value":  "Inclusiveness"},
          { "label": "Model training", "value": "Model training" }
        ],
        "correctAnswer": "Fairness",
        "placeholder": "— Select an option —",
        "explanation": "Fairness is a core ethical principle that all humans aim to understand and apply. This principle is even more important when AI systems are being developed. Reference: https://docs.microsoft.com/en-us/azure/cloud-adoption-framework/innovate/best-practices/trusted-ai"
      },
 
        
    {
        "id": 25,
        "type": "bucket",
        "layout": "bucket",
        "question": "Match the types of AI workloads to the appropriate scenarios.",
"options": [
          { "id":  "Knowledge mining", "label":  "Knowledge mining" },
          { "id":  "Computer vision", "label":  "Computer vision" },
          { "id":  "Anomaly detection", "label": "Anomaly detection" },
          { "id":  "Natural language processing", "label":  "Natural language processing"}
        ],
        "buckets": [
          { "id": "An automated chatbot to answer questions about refunds and exchanges.", "accepts": ["Knowledge mining"] },
          { "id": "Determining whether a photo contains a person.", "accepts": ["Computer vision"] },
          { "id": "Determining whether a review is positive or negative.", "accepts": ["Natural Language processing"] }
        ],  

     
        "explanation": "Box 1: Knowledge mining - You can use Azure Cognitive Search's knowledge mining results and populate your knowledge base of your chatbot. Box 2: Computer vision - Box 3: Natural language processing Natural language processing (NLP) is used for tasks such as sentiment analysis. Reference: https://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/natural-language-processing"
        
    },
    {
        "id": 26,
        "type": "bucket",
        "layout": "bucket",
        "question": "Match the machine learning tasks to the appropriate scenarios.To answer, drag the appropriate task from the column on the left to its scenario on the right. Each task may be used once, more than once, or not at all.",
 "options": [
          { "id": "Model evaluation", "label": "Model evaluation"},
          { "id":  "Model deployment", "label":  "Model deployment" },
          { "id": "Model training", "label": "Model training"},
          { "id": "Feature engineering", "label": "Feature engineering"},
          { "id":  "Feature selection", "label":  "Feature selection" }
        ],
        "buckets": [
          { "id": "Examining the values of a confusion matrix.", "accepts": ["Model evaluation"] },
          { "id": "Splitting a date into month, day and year fields.", "accepts": ["Feature engineering"] },
          { "id": "Picking temperature and pressure to train a weather model.", "accepts": [ "Feature selection"] }
        ],  

        "explanation": "Box 1: Model evaluation - The Model evaluation module outputs a confusion matrix showing the number of true positives, false negatives, false positives, and true negatives, as well as ROC, Precision/Recall, and Lift curves. Box 2: Feature engineering - Feature engineering is the process of using domain knowledge of the data to create features that help ML algorithms learn better. In Azure Machine Learning, scaling and normalization techniques are applied to facilitate feature engineering. Collectively, these techniques and feature engineering are referred to as featurization. Note: Often, features are created from raw data through a process of feature engineering. For example, a time stamp in itself might not be useful for modeling until the information is transformed into units of days, months, or categories that are relevant to the problem, such as holiday versus working day. Box 3: Feature selection - In machine learning and statistics, feature selection is the process of selecting a subset of relevant, useful features to use in building an analytical model. Feature selection helps narrow the field of data to the most valuable inputs. Narrowing the field of data helps reduce noise and improve training performance.Reference: https://docs.microsoft.com/en-us/azure/machine-learning/studio/evaluate-model-performance"
            },

{
	"id": 27,
        "type": "dropdown",
        "question": "Data Values that influence the prediction of a model are called _____________ .",

        "options": [
          { "label":  "Dependant variables", "value":  "Dependant variables" },
          { "label": "features", "value":  "features"},
          { "label":  "identifiers", "value":  "identifiers"},
          { "label":  "labels", "value":  "labels" }
        ],
        "correctAnswer": "features",
        "placeholder": "— Select an option —",
        "explanation": "Correct Answer: Features. Reference: https://www.baeldung.com/cs/feature-vs-label and https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/"
      },

          
    {
        "id": 28,
        "type": "single_choice",
        "question": "Which type of model is the chart used to evaluate?",
        "options": [
            "A. Classification",
            "B. Regression",
            "C. Clustering"
        ],
        "answer": [
            "B. Regression"
        ],
        "explanation": "Answer is Regression. What is a Predicted vs. True chart? Predicted vs. True shows the relationship between a predicted value and its correlating true value for a regression problem. This graph can be used to measure performance of a model as the closer to the y=x line the predicted values are, the better the accuracy of a predictive model. Reference: https://docs.microsoft.com/en-us/azure/machine-learning/how-to-understand-automated-m",
        "images": [
            "https://bongawat.wordpress.com/wp-content/uploads/2026/01/ai900_q41.png?w=NaN&h="
        ],
        "interactivity": null
    },
    {
        "id": 29,
        "type": "single_choice",
        "question": "Which type of machine learning should you use to predict the number of gift cards that will be sold next month?",
        "options": [
            "A. classification",
            "B. regression",
            "C. clustering"
        ],
        "answer": "B. regression",
        "explanation": "Answer is B. In the most basic sense, regression refers to prediction of a numeric target. Linear regression attempts to establish a linear relationship between one or more independent variables and a numeric outcome, or dependent variable. You use this module to define a linear regression method, and then train a model using a labeled dataset. The trained model can then be used to make predictions. Reference: https://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/linear-regression",
        "images": [],
        "interactivity": null
    },
    {
        "id": 30,
        "type": "single_choice",
        "question": "You have a dataset that contains information about taxi journeys that occurred during a given period. You need to train a model to predict the fare of a taxi journey. What should you use as a feature?",
        "options": [
            "A. the number of taxi journeys in the dataset",
            "B. the trip distance of individual taxi journeys",
            "C. the fare of individual taxi journeys",
            "D. the trip ID of individual taxi journeys"
        ],
        "answer": "B. the trip distance of individual taxi journeys",
        "explanation": "Answer B. The label is the column you want to predict. The identified Featuresare the inputs you give the model to predict the Label. Example: The provided data set contains the following columns: vendor_id: The ID of the taxi vendor is a feature. rate_code: The rate type of the taxi trip is a feature. passenger_count: The number of passengers on the trip is a feature. trip_time_in_secs: The amount of time the trip took. You want to predict the fare of the trip before the trip is completed. At that moment, you don't know how long the trip would take. Thus, the trip time is not a feature and you'll exclude this column from the model. trip_distance: The distance of the trip is a feature. payment_type: The payment method (cash or credit card) is a feature. fare_amount: The total taxi fare paid is the label. Reference: https://docs.microsoft.com/en-us/dotnet/machine-learning/tutorials/predict-prices",
        "images": [],
        "interactivity": null
    },
    {
        "id": 31,
        "type": "single_choice",
        "question": "You need to predict the sea level in meters for the next 10 years. Which type of machine learning should you use?",
        "options": [
            "A. classification",
            "B. regression",
            "C. clustering"
        ],
        "answer": "B. regression",
        "explanation": " Answer is B. Regression refers to prediction of a numeric target.",
        "images": [],
        "interactivity": null
    },
    {
        "id": 32,
        "type": "boolean",
        "question": "For each of the following statements, select Yes if the statement is true. Otherwise, select No.",
        "subQuestions": [
            {
                "prompt": "Automated machine learning is the process of automating the time-consuming, iterative tasks of machine learning model development.",
                "correctAnswer": true
            },
            {
                "prompt": "Automated machine learning can automatically infer the training data from the use case provided.",
                "correctAnswer": false
            },
            {
                "prompt": "Automated machine learning works by running multiple training iterations that are scored and ranked by the metrics you specify.",
                "correctAnswer": true
            },
            {
                "prompt": "Automated machine learning enables you to specify a dataset and will automatically understand which label to predict.",
                "correctAnswer": false
            }
        ],
        "explanation": "Box 1: Yes - Automated machine learning, also referred to as automated ML or AutoML, is the process of automating the time consuming, iterative tasks of machine learning model development. It allows data scientists, analysts, and developers to build ML models with high scale, efficiency, and productivity all while sustaining model quality. Box 2: No -Box 3: Yes - During training, Azure Machine Learning creates a number of pipelines in parallel that try different algorithms and parameters for you. The service iterates through ML algorithms paired with feature selections, where each iteration produces a model with a training score. The higher the score, the better the model is considered to fit your data. It will stop once it hits the exit criteria defined in the experiment. Box 4: No - Apply automated ML when you want Azure Machine Learning to train and tune a model for you using the target metric you specify. The label is the column you want to predict. Reference: https://azure.microsoft.com/en-us/services/machine-learning/automatedml/#features",
        "images": []
    },

{
        "id": 33,
        "type": "dropdown",
        "question": "A banking system that predicts whether a loan will be repaid is an example of the _____________  type of machine learning.",
        "options": [
          { "label": "classification", "value": "classification" },
          { "label": "regression", "value":  "regression" },  
	  { "label": "clustering", "value": "clustering" }
        ],
        "correctAnswer": "classification",
        "placeholder": "— Select an option —",
        "explanation": "Answer: Classification. Two-class classification provides the answer to simple two-choice questions such as Yes/No or True/False."
            },
    {
        "id": 34,
        "type": "boolean",
        "question": "For each of the following statements, select Yes if the statement is true. Otherwise, select No.",
        "subQuestions": [
            {
                "prompt": "Labeling is the process of taggin training data with known values.",
                "correctAnswer": true
            },
            {
                "prompt": "You should evaluate a model by using the same data used to train the model.",
                "correctAnswer": false
            },
            {
                "prompt": "Accuracy is always the primary metric used to measure a model's performance.",
                "correctAnswer": false
            }
        ],
        "explanation": "Box 1: Yes - In machine learning, if you have labeled data, that means your data is marked up, or annotated, to show the target, which is the answer you want your machine learning model to predict. In general, data labeling can refer to tasks that include data tagging, annotation, classification, moderation, transcription, or processing. Box 2: No - Box 3: No - Accuracy is simply the proportion of correctly classified instances. It is usually the first metric you look at when evaluating a classifier. However, when the test data is unbalanced (where most of the instances belong to one of the classes), or you are more interested in the performance on either one of the classes, accuracy doesn't really capture the effectiveness of a classifier. Reference: https://docs.microsoft.com/en-us/azure/machine-learning/studio/evaluate-model-performance",
        "images": []
    },
    {
        "id": 35,
        "type": "single_choice",
        "question": "Which service should you use to extract text, key/value pairs, and table data automatically from scanned documents?",
        "options": [
            "A. Form Recognizer",
            "B. Text Analytics",
            "C. Language Understanding",
            "D. Custom Vision"
        ],
        "answer": "A. Form Recognizer",
        "explanation": "Answer is A. Accelerate your business processes by automating information extraction. Form Recognizer applies advanced machine learning to accurately extract text, key/ value pairs, and tables from documents. With just a few samples, Form Recognizer tailors its understanding to your documents, both on-premises and in the cloud. Turn forms into usable data at a fraction of the time and cost, so you can focus more time acting on the information rather than compiling it. Reference: https://azure.microsoft.com/en-us/services/cognitive-services/form-recognizer/",
        "images": [],
        "interactivity": null
    },

{
        "id": 36,
        "type": "dropdown",
        "question": "The ability to extract subtotals and totals from a receipt is a capability of the  _____________ .",
        "options": [
          { "label": "Custom Vision", "value": "Custom Vision" },
          { "label": "Form Recognizer", "value":  "Form Recognizer" },
          { "label": "Ink Recogonizer", "value": "Ink Recogonizer" },
	  { "label": "Text Analytics", "value": "Text Analytics" }
        ],
        "correctAnswer": "Form Recognizer",
        "placeholder": "— Select an option —",
        "explanation": "Answer Form Recognizer.ccelerate your business processes by automating information extraction. Form Recognizer applies advanced machine learning to accurately extract text, key/ value pairs, and tables from documents. With just a few samples, Form Recognizer tailors its understanding to your documents, both on-premises and in the cloud. Turn forms into usable data at a fraction of the time and cost, so you can focus more time acting on the information rather than compiling it. Reference: https://azure.microsoft.com/en-us/services/cognitive-services/form-recognizer/"
            },
    {
        "id": 37,
        "type": "multiple_choice",
        "question": "You use Azure Machine Learning designer to publish an inference pipeline. Which two parameters should you use to access the web service?",
        "options": [
            "A. the model name",
            "B. the training endpoint",
            "C. the authentication key",
            "D. the REST endpoint"
        ],
        "answer": [
            "C. the authentication key",
            "D. the REST endpoint"
        ],
        "explanation": "Answer C,D. You can consume a published pipeline in the Published pipelines page. Select a published pipeline and find the REST endpoint of it. To consume the pipeline, you need: - The REST endpoint for your service -The Primary Key for your service Reference: https://docs.microsoft.com/en-in/learn/modules/create-regression-model-azure-machine-learning-designer/deploy-service",
        "images": [],
        "interactivity": null
    },
{
        "id": 38,
        "type": "dropdown",
        "question": "From Azure Machine learning designer, to deploy a real-time inference pipeline as a service for others to consume, you must deploy the model to _____________ .",
        "options": [
          { "label": "a local web service", "value": "a local web service" },
          { "label": "Azure Container Instances", "value":  "Azure Container Instances" },
          { "label": "Azure Kubernetes Service (AKS)", "value": "Azure Kubernetes Service (AKS)"},
	  { "label": "Azure Machine Learning compute", "value": "Azure Machine Learning compute" }
        ],
        "correctAnswer": "Azure Kubernetes Service (AKS)",
        "placeholder": "— Select an option —",
        "explanation": "For Prod - AKS and for Dev/Test - use Azure Container Service. To perform real-time inferencing, you must deploy a pipeline as a real-time endpoint. Real-time endpoints must be deployed to an Azure Kubernetes Service cluster. Reference: https://docs.microsoft.com/en-us/azure/machine-learning/concept-designer#deploy"
       },

{
        "id": 39,
        "type": "dropdown",
        "question": "Predicting how many hours of overtime a delivery person will work based on the number of order received is an example of  _____________ .",
           "options": [
          { "label": "classification", "value": "classification" },
	  { "label": "clustering", "value": "clustering" },
	  { "label": "regression", "value":  "regression" }
        ],
        "correctAnswer": "regression",
        "placeholder": "— Select an option —",
        "explanation": "Regression refers to prediction of a numeric target."
            },
    {
        "id": 40,
        "type": "boolean",
        "question": "For each of the following statements, select Yes if the statement is true. Otherwise, select No.",
        "subQuestions": [
            {
                "prompt": "Azure Machine learning designer provides a drag-and-drop visual canvas to build, test, and deply machine learning models.",
                "correctAnswer": true
            },
            {
                "prompt": "Azure Machine learning designer enables you to save your progress as a pipeline draft.",
                "correctAnswer": true
            },
            {
                "prompt": "Azure Machine learning designer enables you to include custom Javascript functions.",
                "correctAnswer": false
            }
        ],
        "explanation": "Box 1: Yes - Azure Machine Learning designer lets you visually connect datasets and modules on an interactive canvas to create machine learning models. Box 2: Yes - With the designer you can connect the modules to create a pipeline draft. As you edit a pipeline in the designer, your progress is saved as a pipeline draft. Box 3: No - Reference: https://docs.microsoft.com/en-us/azure/machine-learning/concept-designer",
        "images": []
    },
    {
        "id": 41,
        "type": "hotspot",
        "question": "You have the following dataset. You plan to use the dataset to train a model that will predict the house price categories of houses. What are Household Income and House Price Category?",
        "options": [
            "Feature",
            "Label"
        ],
        "explanation": "Feature = Input. Label = Output.",
        "images": [
            "https://bongawat.wordpress.com/wp-content/uploads/2026/01/ai900_q38.png?w=NaN&h="
        ],
        "interactivity": {
            "hotspot_areas": [
                {
                    "prompt": "Household Income [Answer Choice].",
                    "answer": "Feature"
                },
                {
                    "prompt": "House Price category [Answer Choice].",
                    "answer": "Label"
                }
            ]
        }
    },

 {
        "id": 42,
        "type": "dropdown",
        "question": "Azure Machine learning designer lets you create machine learning models by _____________ ",
        "options": [
          { "label": "adding and connecting modules on a visual canavas", "value": "adding and connecting modules on a visual canavas" },
          { "label": "automatically performing common data preparation tasks", "value":  "automatically performing common data preparation tasks" },
          { "label": "automatically selecting an algorithm to build the most accurate model", "value": "automatically selecting an algorithm to build the most accurate model" },
          { "label": "using a code-first notebook experience", "value": "using a code-first notebook experience" }
        ],
        "correctAnswer": "adding and connecting modules on a visual canavas",
        "placeholder": "— Select an option —",
        "explanation": "Answer is adding and connecting modules on a visual canavas. Reference: https://docs.microsoft.com/en-us/azure/machine-learning/concept-designer"
   
    },
    {
        "id": 43,
        "type": "boolean",
        "question": "For each of the following statements, select Yes if the statement is true. Otherwise, select No.",
        "subQuestions": [
            {
                "prompt": "Automated machine learning provides you with the ability to include custom Python scripts in a training pipeline.",
                "correctAnswer": false
            },
            {
                "prompt": "Automated machine learning implemnts machine learning solutions without the need for programming experience.",
                "correctAnswer": true
            },
            {
                "prompt": "Automated machine learning provides you with the ability to visually connect datasets and modules on an interactive canvas.",
                "correctAnswer": false
            }
        ],
        "explanation": "No - Automated machine learning only requires you to choose between Python SDK and studio web experience. Yes - Automated machine learning is a no code solution. No - This is done in the Azure Machine Learning studio web experience. Source: https://docs.microsoft.com/en-us/azure/machine-learning/concept-automated-ml Reference: https://docs.microsoft.com/en-us/azure/machine-learning/how-to-designer-python https://docs.microsoft.com/en-us/azure/machine-learning/concept-automated-ml",
        "images": []
    },
    {
        "id": 44,
        "type": "single_choice",
        "question": "A medical research project uses a large anonymized dataset of brain scan images that are categorized into predefined brain haemorrhage types. You need to use machine learning to support early detection of the different brain haemorrhage types in the images before the images are reviewed by a person. This is an example of which type of machine learning?",
        "options": [
            "A. clustering",
            "B. regression",
            "C. classification"
        ],
        "answer": "C. classification",
        "explanation": "Answer is C. This is classification. There are multiple reasons it's classification - The training data is already tagged as with the correct type of hemorrhage - Classification can be done for more than two classes (which people seem to not realize based on the comments) - You do clustering on a group of inputs. For example, the scans of 10 people. You can't cluster a single input. Clearly you get a new scan of a new patient and you want to know what that scan shows, you don't have a group of scans to cluster. - Clustering gives NO labels. You just get groups and don't know what the label is, but in this question it's very clear they want to know the label that belongs to the new scan -> classification Reference: https://docs.microsoft.com/en-us/learn/modules/create-classification-model-azure-machine-learningdesigner/introduction",
        "images": [],
        "interactivity": null
    },
    {
        "id": 45,
        "type": "single_choice",
        "question": "When training a model, why should you randomly split the rows into separate subsets?",
        "options": [
            "A. to train the model twice to attain better accuracy",
            "B. to train multiple models simultaneously to attain better performance",
            "C. to test the model by using data that was not used to train the model"
        ],
        "answer": "C. to test the model by using data that was not used to train the model",
        "explanation": "Answer is C. First for training then evaluation. This refers to the famous train_test_split that everyone uses to split the dataset into train and test sets.",
        "images": [],
        "interactivity": null
    },
    {
        "id": 46,
        "type": "multiple_choice",
        "question": "You are evaluating whether to use a basic workspace or an enterprise workspace in Azure Machine Learning. What are two tasks that require an enterprise workspace?",
        "options": [
            "A. Use a graphical user interface (GUI) to run automated machine learning experiments.",
            "B. Create a compute instance to use as a workstation.",
            "C. Use a graphical user interface (GUI) to define and run machine learning experiments from Azure Machine Learning designer.",
            "D. Create a dataset from a comma-separated value (CSV) file."
        ],
        "answer": [
            "A. Use a graphical user interface (GUI) to run automated machine learning experiments.",
            "C. Use a graphical user interface (GUI) to define and run machine learning experiments from Azure Machine Learning designer."
        ],
        "explanation": "Answer A,C. Enterprise workspaces are no longer available as of September 2020. The basic workspace now has all the functionality of the enterprise workspace. Reference: https://docs.microsoft.com/en-us/azure/machine-learning/concept-workspace",
        "images": [],
        "interactivity": null
    },
    {
        "id": 47,
        "type": "multiple_choice",
        "question": "You need to predict the income range of a given customer by using the following dataset. Which two fields should you use as features?",
        "options": [
            "A. Education Level",
            "B. Last Name",
            "C. Age",
            "D. Income Range",
            "E. First Name"
        ],
        "answer": [
            "A. Education Level",
            "C. Age"
        ],
        "explanation": "Answer: A,C. Age and Education level are the features you should use. Income range is a label (what you want to predict).First Name and Last Name are irrelevant in that they have no bearing on income. Age and Education level are the features you should use.",
        "images": [
            "https://bongawat.wordpress.com/wp-content/uploads/2026/01/ai900_q46.png?w=NaN&h="
        ],
        "interactivity": null
    },
    {
        "id": 48,
        "type": "single_choice",
        "question": "You are building a tool that will process images from retail stores and identify the products of competitors. The solution will use a custom model. Which Azure Cognitive Services service should you use?",
        "options": [
            "A. Custom Vision",
            "B. Form Recognizer",
            "C. Face",
            "D. Computer Vision"
        ],
        "answer": "A. Custom Vision",
        "explanation": "Answer is A. Azure Custom Vision is an image recognition service that lets you build, deploy, and improve your own image identifier models. An image identifier applies labels (which represent classifications or objects) to images, according to their detected visual characteristics. Unlike the Computer Vision service, Custom Vision allows you to specify your own labels and train custom models to detect them. Reference: https://docs.microsoft.com/en-us/azure/cognitive-services/custom-vision-service/overview",
        "images": [],
        "interactivity": null
    },
    {
        "id": 49,
        "type": "boolean",
        "question": "For each of the following statements, select Yes if the statement is true. Otherwise, select No.",
        "subQuestions": [
            {
                "prompt": "Organizing documents into groups based on similarities of the text contained in the documents is an example of clustering.",
                "correctAnswer": true
            },
            {
                "prompt": "Grouping similar patients based on symptoms and diagnostic test results is an example of clustering.",
                "correctAnswer": true
            },
            {
                "prompt": "Predicting whether a person will develop mild, moderate, or severe allergy symptoms based on pollen count is an example of clustering.",
                "correctAnswer": false
            }
        ],
        "explanation": "Answer is Yes, Yes, No. Clustering is a machine learning task that is used to group instances of data into clusters that contain similar characteristics. Regression is a machine learning task that is used to predict the value of the label from a set of related features.",
        "images": []
    },
    {
        "id": 50,
        "type": "boolean",
        "question": "For each of the following statements, select Yes if the statement is true. Otherwise, select No.",
        "subQuestions": [
            {
                "prompt": "A validation set includes the set of input examples that will be used to train a mode.",
                "correctAnswer": false
            },
            {
                "prompt": "A validation set can be used to determine how well a model predicts labels.",
                "correctAnswer": true
            },
            {
                "prompt": "A validation set can be used to verify that all the training data was used to train the model.",
                "correctAnswer": false
            }
        ],
        "explanation": "Box 1: No - The validation dataset is different from the test dataset that is held back from the training of the model. Box 2: Yes - A validation dataset is a sample of data that is used to give an estimate of model skill while tuning model's hyperparameters. Box 3: No - The Test Dataset, not the validation set, used for this. The Test Dataset is a sample of data used to provide an unbiased evaluation of a final model fit on the training dataset. Reference: https://machinelearningmastery.com/difference-test-validation-datasets",
        "images": []
    },
    {
        "id": 51,
        "type": "multiple_choice",
        "question": "What are two metrics that you can use to evaluate a regression model?",
        "options": [
            "A. coefficient of determination (R2)",
            "B. F1 score",
            "C. root mean squared error (RMSE)",
            "D. area under curve (AUC)",
            "E. balanced accuracy"
        ],
        "answer": [
            "A. coefficient of determination (R2)",
            "C. root mean squared error (RMSE)"
        ],
        "explanation": "Answer: A,C. If its Regression model then remember 'R' and the corresponding answer will be R2 and RMSE. A: R-squared (R2), or Coefficient of determination represents the predictive power of the model as a value between -inf and 1.00. 1.00 means there is a perfect fit, and the fit can be arbitrarily poor so the scores can be negative. C: RMS-loss or Root Mean Squared Error (RMSE) (also called Root Mean Square Deviation, RMSD), measures the difference between values predicted by a model and the values observed from the environment that is being modeled. Incorrect Answers: B: F1 score also known as balanced F-score or F-measure is used to evaluate a classification model. D: aucROC or area under the curve (AUC) is used to evaluate a classification model. Reference: https://docs.microsoft.com/en-us/dotnet/machine-learning/resources/metrics",
        "images": [],
        "interactivity": null
    },


 {
        "id": 52,
        "type": "dropdown",
        "question": "Predicting how many vehicles will travel across a bridge on a given day is an example of _____________ ",
        "options": [
          
          { "label": "Classification", "value":  "Classification" },
          { "label": "Clustering", "value": "Clustering" },
	  { "label": "Regression", "value": "Regression" }
        ],
        "correctAnswer": "Regression",
        "placeholder": "— Select an option —",
         "explanation": "Regression is a machine learning task that is used to predict the value of the label from a set of related features."
    },

       
 
    {
        "id": 53,
               "type": "bucket",
        "layout": "bucket",
        "question": "You need to use Azure Machine Learning designer to build a model that will predict automobile prices. Which type of modules should you use to complete the model?",
"options": [
          { "id": "Convert to CSV", "label": "Convert to CSV"},
          { "id": "K-Means clustering", "label": "K-Means clustering" },
          { "id": "Linear Regression", "label": "Linear Regression" },
          { "id": "Split data", "label": "Split data"},
          { "id": "Select Columns in Dataset", "label": "Select Columns in Dataset" },
          { "id": "Summarize Data", "label": "Summarize Data" }
        ],
        "buckets": [
          { "id": "Box Number 1.", "accepts": ["Select Columns in Dataset"] },
          { "id": "Box Number 2.", "accepts": ["Split data"] },
          { "id": "Box Number 3.", "accepts": [ "Linear Regression"] }
        ],  
    
        "explanation": "Box 1: Select Columns in Dataset For Columns to be cleaned, choose the columns that contain the missing values you want to change. You can choose multiple columns, but you must use the same replacement method in all selected columns.Box 2: Split data - Splitting data is a common task in machine learning. You will split your data into two separate datasets. One dataset will train the model and the other will test how well the model performed. Box 3: Linear regression - Because you want to predict price, which is a number, you can use a regression algorithm. For this example, you use a linear regression model. Reference: https://docs.microsoft.com/en-us/azure/machine-learning/tutorial-designer-automobile-price-train-score",
        "images": [
            "https://bongawat.wordpress.com/wp-content/uploads/2026/01/ai900_q52.png?w=NaN&h="
        ]
        
    },
    {
        "id": 54,
        "type": "single_choice",
        "question": "Which type of machine learning should you use to identify groups of people who have similar purchasing habits?",
        "options": [
            "A. classification",
            "B. regression",
            "C. clustering"
        ],
        "answer": "C. clustering",
        "explanation": "Answer is C. Clustering is a machine learning task that is used to group instances of data into clusters that contain similar characteristics. Reference: https://docs.microsoft.com/en-us/dotnet/machine-learning/resources/tasks",
        "images": [],
        "interactivity": null
    },

 {
        "id": 55,
        "type": "dropdown",
        "question": " _____________  models can be used to predict the sale price of auctioned items.",
        "options": [
          { "label": "Classification", "value": "Classification" },
          { "label": "Regression", "value": "Regression" },
	  { "label": "Clustering", "value": "Clustering" }
        ],
        "correctAnswer": "Regression",
        "placeholder": "— Select an option —",
       "explanation": "Regression is a machine learning task that is used to predict the value of the label from a set of related features. Reference: https://docs.microsoft.com/en-us/dotnet/machine-learning/resources/tasks"
      },
    {
        "id": 56,
        "type": "single_choice",
        "question": "Which metric can you use to evaluate a classification model?",
        "options": [
            "A. true positive rate",
            "B. mean absolute error (MAE)",
            "C. coefficient of determination (R2)",
            "D. root mean squared error (RMSE)"
        ],
        "answer": "A. true positive rate",
        "explanation": "Answer is A. An ROC curve that approaches the top left corner with 100% true positive rate and 0% false positive rate will be the best model. A random model would display as a flat line from the bottom left to the top right corner. Worse than random would dip below the y=x line. MAE, RMSE and R2 are metris for regression: https://docs.microsoft.com/en-us/azure/machine-learning/algorithm-module-reference/evaluate-model",
        "images": [],
        "interactivity": null
    },
    {
        "id": 57,
        "type": "multiple_choice",
        "question": "Which two components can you drag onto a canvas in Azure Machine Learning designer?",
        "options": [
            "A. dataset",
            "B. compute",
            "C. pipeline",
            "D. module"
        ],
        "answer": [
            "A. dataset",
            "D. module"
        ],
        "explanation": "Answer A,D. You can drag-and-drop datasets and modules onto the canvas. Azure Machine Learning designer lets you visually connect datasets and modules on an interactive canvas to create machine learning models. Reference: https://docs.microsoft.com/en-us/azure/machine-learning/concept-designer",
        "images": [],
        "interactivity": null
    },
    {
        "id": 58,
        "type": "single_choice",
        "question": "You need to create a training dataset and validation dataset from an existing dataset. Which module in the Azure Machine Learning designer should you use?",
        "options": [
            "A. Select Columns in Dataset",
            "B. Add Rows",
            "C. Split Data",
            "D. Join Data"
        ],
        "answer": "C. Split Data",
        "explanation": "Answer is C. A common way of evaluating a model is to divide the data into a training and test set by using Split Data, and then validate the model on the training data.Use the Split Data module to divide a dataset into two distinct sets. The studio currently supports training/validation data splits. Reference: https://docs.microsoft.com/en-us/azure/machine-learning/how-to-configure-cross-validation-data-splits",
        "images": [],
        "interactivity": null
    },
  
    {
        "id": 59,
       "type": "bucket",
        "layout": "bucket",
        "question": "Match the types of machine learning to the appropriate scenarios.",

 "options": [
          { "id": "Regression", "label": "Regression" },
          { "id":  "Clustering", "label":  "Clustering" },
          { "id":  "Classification", "label":  "Classification" }
                  ],
        "buckets": [
          { "id": "Predict how many minutes late a flight will arrive based on the amount of snowfall at an airport.", "accepts": ["Regression"] },
          { "id": "Segmant customers into different groups to support a marketing department.", "accepts": [ "Clustering"] },
          { "id": "Predict whether a student will complete a university course.", "accepts": ["Classification"] }
        ],
                "explanation": "Box 1: Regression - In the most basic sense, regression refers to prediction of a numeric target. Linear regression attempts to establish a linear relationship between one or more independent variables and a numeric outcome, or dependent variable. You use this module to define a linear regression method, and then train a model using a labeled dataset. The trained model can then be used to make predictions. Box 2: Clustering - Clustering, in machine learning, is a method of grouping data points into similar clusters. It is also called segmentation. Over the years, many clustering algorithms have been developed. Almost all clustering algorithms use the features of individual items to find similar items. For example, you might apply clustering to find similar people by demographics. You might use clustering with text analysis to group sentences with similar topics or sentiment. Box 3: Classification - Two-class classification provides the answer to simple two-choice questions such as Yes/No or True/False. Reference: https://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/linear-regression"
        
    },

 {
        "id": 60,
        "type": "dropdown",
        "question": " _____________  is the calculated probability of a correct classification.",
        "options": [
          { "label": "Accuracy", "value": "Accuracy" },
          { "label": "Confidence", "value":  "Confidence" },
          { "label": "Root Mean Square Error", "value": "Root Mean Square Error" },
	  { "label": "Sentiment", "value": "Sentiment" }
        ],
        "correctAnswer": "Confidence",
        "placeholder": "— Select an option —",
        "explanation": "Confidence is the right answer. ...The probability score of the object classification (which you can interpret as the confidence of the predicted class being correct)... Source: https://docs.microsoft.com/en-us/learn/modules/detect-objects-images-custom-vision/1a-what-isobject- detection Reference: https://docs.microsoft.com/en-us/azure/cognitive-services/custom-vision-service/getting-started-build-aclassifier"
          
    },
 {
        "id": 61,
        "type": "dropdown",
        "question": "Ensuring an AI system does not provide a predictuin when important fields contain unusual or missing values is _____________  principle for responsible AI.",
        "options": [
		{ "label": "an inclusiveness", "value":  "an inclusiveness" },       
		{ "label": "a privacy and security", "value": "a privacy and security" },  
	        { "label": "a reliability and safety", "value": "a reliability and safety" },
          	{ "label": "a transparency", "value": "a transparency" }
        ],
        "correctAnswer": "a reliability and safety",
        "placeholder": "— Select an option —",
         "explanation": "Answer is reliability and safety. Reference: https://docs.microsoft.com/en-us/azure/cloud-adoption-framework/innovate/best-practices/trusted-ai"
  
    },

{
        "id": 62,
        "type": "dropdown",
        "question": "Ensuring that the numeric variables in training data are on a similar scale is an example of _____________ .",
        "options": [
          { "label": "data ingestion", "value": "data ingestion" },
          { "label": "feature engineering", "value":  "feature engineering" },
          { "label":  "feature selection", "value":  "feature selection" },
	  { "label": "model training", "value": "model training" }
        ],
        "correctAnswer": "feature engineering",
        "placeholder": "— Select an option —",
       
        "explanation": "Feature engineering is the correct answer. In Azure Machine Learning, data-scaling and normalization techniques are applied to make feature engineering easier. Collectively, these techniques and this feature engineering are called featurization in automated ML experiments. Feature selection is only about selection. Modifying features = Feature engineering https://docs.microsoft.com/en-us/azure/machine-learning/how-to-configure-auto-features Reference: https://docs.microsoft.com/en-us/azure/architecture/data-science-process/create-features"
           },

{
        "id": 63,
        "type": "dropdown",
        "question": "Assigning classes to images before training classification model is an example of _____________ .",
        "options": [
          { "label": "evaluation", "value": "evaluation" },
          { "label": "feature engineering", "value":  "feature engineering" },
          { "label": "hyperprameter tuning", "value": "hyperprameter tuning" },
	  { "label": "labeling", "value": "labeling" }
        ],
        "correctAnswer": "labeling",
        "placeholder": "— Select an option —",
        "explanation": "Answer is Labeling. Reference: https://docs.microsoft.com/en-us/azure/machine-learning/how-to-label-data"
            },
    {
        "id": 64,
        "type": "boolean",
        "question": "You have an Azure Machine Learning model that predicts product quality. The model has a training dataset that contains 50,000 records. A sample of the data is shown in the following table. For each of the following statements, select Yes if the statement is true. Otherwise, select No.",
        "subQuestions": [
            {
                "prompt": "Mass (kg) is a feature.",
                "correctAnswer": true
            },
            {
                "prompt": "Quality test is a label.",
                "correctAnswer": true
            },
            {
                "prompt": "Temperature (C) is a label.",
                "correctAnswer": false
            }
        ],
        "explanation": "Answer is Yes, Yes, No. Reference: https://docs.microsoft.com/en-us/azure/machine-learning/component-reference/filter-based-feature-selection",
        "images": [
            "https://bongawat.wordpress.com/wp-content/uploads/2026/01/ai900_q63.png?w=NaN&h="
        ]
    },
    {
        "id": 65,
        "type": "boolean",
        "question": "For each of the following statements, select Yes if the statement is true. Otherwise, select No.",
        "subQuestions": [
            {
                "prompt": "You train a regression model by using unlabeled data.",
                "correctAnswer": false
            },
            {
                "prompt": "A triage bot that prioritizes insurance claims based on injuries is an example of the Microsoft reliability and safety principle of responsible AI.",
                "correctAnswer": false
            },
            {
                "prompt": "Grouping items by their common characteristics is an example of clustering.",
                "correctAnswer": true
            }
        ],
        "explanation": "Correct answer: No, No, Yes Reference: https://docs.microsoft.com/en-us/learn/modules/create-regression-model-azure-machine-learning-designer/5-create-training-pipeline https://docs.microsoft.com/en-us/learn/modules/create-classification-model-azure-machine-learning-designer/introduction https://docs.microsoft.com/en-us/learn/modules/create-clusteringmodel-azure-machine-learning-designer/1-introduction",
        "images": []
    },
    {
        "id": 66,
        "type": "multiple_choice",
        "question": "Which two actions are performed during the data ingestion and data preparation stage of an Azure Machine Learning process?",
        "options": [
            "A. Calculate the accuracy of the model.",
            "B. Score test data by using the model.",
            "C. Combine multiple datasets.",
            "D. Use the model for real-time predictions.",
            "E. Remove records that have missing values."
        ],
        "answer": [
            "C. Combine multiple datasets.",
            "E. Remove records that have missing values."
        ],
        "explanation": "Correct answer: C,E Reference: https://docs.microsoft.com/en-us/azure/machine-learning/concept-data-ingestion https://docs.microsoft.com/en-us/azure/architecture/data-science-process/prepare-data",
        "images": [],
        "interactivity": null
    },
    {
        "id": 67,
        "type": "single_choice",
        "question": "You need to predict the animal population of an area. Which Azure Machine Learning type should you use?",
        "options": [
            "A. regression",
            "B. clustering",
            "C. classification"
        ],
        "answer": "A. regression",
        "explanation": "Answe is A. Regression is a supervised machine learning technique used to predict numeric values.",
        "images": [],
        "interactivity": null
    },
    {
        "id": 68,
        "type": "multiple_choice",
        "question": "Which two languages can you use to write custom code for Azure Machine Learning designer?",
        "options": [
            "A. Python",
            "B. R",
            "C. C#",
            "D. Scala"
        ],
        "answer": [
            "A. Python",
            "B. R"
        ],
        "explanation": "Answer is A,R. Azure Machine Learning designer supports Python and R for custom code. Reference: https://azure.microsoft.com/en-us/services/machine-learning/designer/#features",
        "interactivity": null
    },
    {
        "id": 69,
        "type": "boolean",
        "question": "For each of the following statements, select Yes if the statement is true. Otherwise, select No.",
        "subQuestions": [
            {
                "prompt": "For a regression model, labels must be numeric.",
                "correctAnswer": true
            },
            {
                "prompt": "For a clustering model, labels must be used.",
                "correctAnswer": false
            },
            {
                "prompt": "For a classification model, labels must be numeric.",
                "correctAnswer": false
            }
        ],
        "explanation": "Box 1: Yes - For regression problems, the label column must contain numeric data that represents the response variable. Ideally the numeric data represents a continuous scale. Box 2: No - K-Means Clustering - Because the K-means algorithm is an unsupervised learning method, a label column is optional. If your data includes a label, you can use the label values to guide selection of the clusters and optimize the model. If your data has no label, the algorithm creates clusters representing possible categories, based solely on the data. Box 3: No - For classification problems, the label column must contain either categorical values or discrete values. Some examples might be a yes/no rating, a disease classification code or name, or an income group. If you pick a noncategorical column, the component will return an error during training. Reference: https://docs.microsoft.com/en-us/azure/machine-learning/component-reference/train-model https://docs.microsoft.com/en-us/azure/machine-learning/component-reference/k-means-clustering",
        "images": []
    },
    {
        "id": 70,
        "type": "single_choice",
        "question": "Your company wants to build a recycling machine for bottles. The recycling machine must automatically identify bottles of the correct shape and reject all other items. Which type of AI workload should the company use?",
        "options": [
            "A. anomaly detection",
            "B. conversational AI",
            "C. computer vision",
            "D. natural language processing"
        ],
        "answer": "C. computer vision",
        "explanation": "Answer is C. Azure's Computer Vision service gives you access to advanced algorithms that process images and return information based on the visual features you're interested in. For example, Computer Vision can determine whether an image contains adult content, find specific brands or objects, or find human faces.",
        "interactivity": null
    },
    {
        "id": 71,
        "type": "boolean",
        "question": "For each of the following statements, select Yes if the statement is true. Otherwise, select No.",
        "subQuestions": [
            {
                "prompt": "When creating an object detection model in the custom vision service, you must choose a classification type of either Multilabel or Multiclass.",
                "correctAnswer": false
            },
            {
                "prompt": "You can create an object detection model in the custom vision service to find the location of content within an image.",
                "correctAnswer": true
            },
            {
                "prompt": "When creating an object detection model in the custom vision service, you can select from a set of predefined domains.",
                "correctAnswer": true
            }
        ],
        "explanation": "Correct answer: No, Yes, Yes. Computer vision can assign color pixels in an image to object names and describe the contents of an image.Reference: https://docs.microsoft.com/en-us/azure/cognitive-services/custom-vision-service/get-started-build-detector"
    },
    {
        "id": 72,
        "type": "multiple_choice",
        "question": "In which two scenarios can you use the Form Recognizer service? Each correct answer presents a complete solution.",
        "options": [
            "A. Extract the invoice number from an invoice.",
            "B. Translate a form from French to English.",
            "C. Find image of product in a catalog.",
            "D. Identify the retailer from a receipt."
        ],
        "answer": [
            "A. Extract the invoice number from an invoice.",
            "D. Identify the retailer from a receipt."
        ],
        "explanation": "Correct Scenarios: A. Extract the invoice number from an invoice: Reason: Form Recognizer is designed to analyze documents like invoices and extract specific fields, such as invoice numbers, dates, or total amounts. This is one of its primary use cases. D. Identify the retailer from a receipt: Reason: Form Recognizer can extract key information from receipts, such as the retailer's name, transaction dates, or itemized totals. This aligns with its capabilities. Incorrect Scenarios: B. Translate a form from French to English: Reason: Translating text is not a feature of Form Recognizer. Translation tasks fall under services like Azure.Translator or similar language translation tools. C. Find an image of a product in a catalog: Reason: Form Recognizer processes text and data from documents but does not work with image-based object detection. For identifying images, you would use services like Computer Vision or Custom Vision.",
        "interactivity": null
    },
 {
        "id": 73,
        "type": "dropdown",
        "question": "Counting the number of animals in an area based on a video feed is an example of _____________ .",
        "options": [
          { "label": "forecasting", "value": "forecasting" },
          { "label": "Computer vision", "value":  "Computer vision" },
          { "label": "conversational AI", "value": "conversational AI" },
	  { "label": "anomaly detection", "value": "anomaly detection" }
        ],
        "correctAnswer": "Computer vision",
        "placeholder": "— Select an option —",
         "explanation": " Correct answer: Computer Vision Reference: https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision/overview https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision/intro-to-spatial-analysis-public-preview"
       
    },
 {
        "id": 74,
        "type": "boolean",
        "question": "You have a database that contains a list of employees and their photos. You are tagging new photos of the employees. For each of the following statements select Yes if the statement is true. Otherwise, select No.",
        "subQuestions": [
            {
                "prompt": "The Face service can be used to perform facial recognition for employees.",
                "correctAnswer": true
            },
            {
                "prompt": "The Face service will be more accurate if you provide more sample photos of each employee from different angles.",
                "correctAnswer": true
            },
            {
                "prompt": "If an employee is wearing sunglasses, the Face service will always fail to recognize the employee.",
                "correctAnswer": false
            }
 ],
           "explanation": "Correct answer: Yes, Yes, No. Reference: https://docs.microsoft.com/en-us/azure/cognitive-services/face/overview https://docs.microsoft.com/en-us/azure/cognitive-services/face/concepts/face-detection"
                  
},
    {
        "id": 75,
        "type": "single_choice",
        "question": "You need to develop a mobile app for employees to scan and store their expenses while travelling. Which type of computer vision should you use?",
        "options": [
            "A. face detection",
            "B. image classification",
            "C. object detection",
            "D. optical character recognition (OCR)"
        ],
        "answer": "D. optical character recognition (OCR)",
        "explanation": "Correct answer is D. Azure's Computer Vision API includes Optical Character Recognition (OCR) capabilities that extract printed or handwritten text from images. You can extract text from images, such as photos of license plates or containers with serial numbers, as well as from documents - invoices, bills, financial reports, articles, and more. Reference: https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision/concept-recognizing-text",
        "images": [],
        "interactivity": null
    },
    {
        "id": 76,
        "type": "boolean",
        "question": "For each of the following statements, select Yes if the statement is true. Otherwise, select No.",
        "subQuestions": [
            {
                "prompt": "The custom vision service can be used to detect objects in an image.",
                "correctAnswer": true
            },
            {
                "prompt": "The custom vision service requires that you provide your own data to train the model.",
                "correctAnswer": true
            },
            {
                "prompt": "The custom vision service can be used to analyze video files.",
                "correctAnswer": false
            }
        ],
        "explanation": "Box 1: Yes - Custom Vision functionality can be divided into two features. Image classification applies one or more labels to an image. Object detection is similar, but it also returns the coordinates in the image where the applied label(s) can be found. Box 2: Yes - The Custom Vision service uses a machine learning algorithm to analyze images. You, the developer, submit groups of images that feature and lack the characteristics in question. You label the images yourself at the time of submission. Then, the algorithm trains to this data and calculates its own accuracy by testing itself on those same images. Box 3: No - Custom Vision service can be used only on graphic files. Reference: https://docs.microsoft.com/en-us/azure/cognitive-services/Custom-Vision-Service/overview",
        "images": []
    },
    {
        "id": 77,
        "type": "single_choice",
        "question": "You are processing photos of runners in a race. You need to read the numbers on the runners' shirts to identity the runners in the photos.? Which type of computer vision should you use?",
        "options": [
            "A. facial recognition",
            "B. optical character recognition (OCR)",
            "C. image classification",
            "D. object detection"
        ],
        "answer": [
            "B. optical character recognition (OCR)"
        ],
        "explanation": "Correct Answer is B. Optical character recognition (OCR) allows you to extract printed or handwritten text from images and documents. Reference: https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision/overview-ocr",
        "images": [],
        "interactivity": null
    },
    {
        "id": 78,
      
"type": "bucket",
        "layout": "bucket",  
        "question": "Match the types of machine learning to the appropriate scenarios.",

 "options": [
          { "id": "Facial detection", "label": "Facial detection"},
          { "id": "Facial recognition", "label": "Facial recognition"},
          { "id":  "Image classification", "label":  "Image classification" },
          { "id": "Object detection", "label": "Object detection"},
          { "id": "Optical character recognition (OCR)", "label": "Optical character recognition (OCR)" },
          { "id": "Semantic segmentation", "label": "Semantic segmentation" }
        ],
        "buckets": [
          { "id": "Separate images of polar bears and brown bears.", "accepts": [ "Image classification"] },
          { "id": "Determine the location of a bear in a photo.", "accepts": ["Object detection"] },
          { "id": "Determine which pixels in an image are part of a bear.", "accepts": ["Semantic segmentation"] }
        ],
      
        "explanation": "Box 1: Image classification - Image classification is a supervised learning problem: define a set of target classes (objects to identify in images), and train a model to recognize them using labeled example photos.Box 2: Object detection - Object detection is a computer vision problem. While closely related to image classification, object detection performs image classification at a more granular scale. Object detection both locates and categorizes entities within images. Box 3: Semantic Segmentation - Semantic segmentation achieves fine-grained inference by making dense predictions inferring labels for every pixel, so that each pixel is labeled with the class of its enclosing object ore region. Reference: https://developers.google.com/machine-learning/practica/image-classification https://docs.microsoft.com/en-us/dotnet/machine-learning/tutorials/object-detection-model-builder https://nanonets.com/blog/how-to-dosemantic-segmentation-using-deep-learning/"
       
    },
    {
        "id": 79,
        "type": "single_choice",
        "question": "You use drones to identify where weeds grow between rows of crops to send an instruction for the removal of the weeds. This is an example of which type of computer vision?",
        "options": [
            "A. object detection",
            "B. optical character recognition (OCR)",
            "C. scene segmentation"
        ],
        "answer": [
            "A. object detection"
        ],
        "explanation": "Correct Answer is A. Object detection is similar to tagging, but the API returns the bounding box coordinates for each tag applied. For example, if an image contains a dog, cat and person, the Detect operation will list those objects together with their coordinates in the image. Incorrect Answers: B: Optical character recognition (OCR) allows you to extract printed or handwritten text from images and documents. C: Scene segmentation determines when a scene changes in video based on visual cues. A scene depicts a single event and it's composed by a series of consecutive shots, which are semantically related.",
        "images": [],
        "interactivity": null
    },
    {
        "id": 80,
       "type": "bucket",
        "layout": "bucket",
        "question": "Match the facial recognition tasks to the appropriate questions. To answer, drag the appropriate task from the column on the left to its question on the right. Each task may be used once, more than once, or not at all.",
        "options": [
          { "id": "Grouping", "label": "Grouping" },
          { "id":  "Identification", "label":  "Identification"},
          { "id": "Similarity", "label": "Similarity" },
          { "id":  "Verification", "label": "Verification"}
        ],
        "buckets": [
          { "id": "Do two images of a face belong to the same person.", "accepts": [ "Verification"] },
          { "id": "Does this person look like other people?.", "accepts": ["Similarity"] },
          { "id": "Do all the faces belong together?.", "accepts": ["Grouping"] },
          { "id": "Who is this person in this group of people?.", "accepts": [ "Identification"] }
        ],  
        
        "explanation": "Box 1: verification - Face verification: Check the likelihood that two faces belong to the same person and receive a confidence score. Box 2: similarity - Box 3: Grouping - Box 4: identification - Face detection: Detect one or more human faces along with attributes such as: age, emotion, pose, smile, and facial hair, including 27 landmarks for each face in the image. ~Face verification: The Verify API does an authentication against two detected faces or from one detected face to one person object. Practically, it evaluates whether two faces belong to the same person. ~Person identification: The Identify API is used to identify a detected face against a database of people (facial recognition search). This feature might be useful for automatic image tagging in photo management software. You create the database in advance, and you can edit it over time. Tips to remember:  verification = same person? similarity = look like? grouping = belong together? identification = who is this person? Reference: https://azure.microsoft.com/en-us/services/cognitive-services/face/#features"
      
    },
    {
        "id": 81,
        "type": "bucket",
        "layout": "bucket",
        "question": "Match the types of computer vision workloads to the appropriate scenarios. To answer, drag the appropriate task from the column on the left to its question on the right. Each task may be used once, more than once, or not at all.",
 "options": [
          { "id":  "Facial recognition", "label":  "Facial recognition" },
          { "id":  "Image classification", "label":  "Image classification" },
          { "id": "Object detection", "label": "Object detection"},
          { "id":  "Optical character recognition (OCR)", "label":  "Optical character recognition (OCR)" }
        
        ],
        "buckets": [
          { "id": "Identify celebrities in images.", "accepts": [ "Facial recognition"] },
          { "id": "Extract movie title names from movie poster images.", "accepts": [ "Optical character recognition (OCR)"] },
          { "id": "Locate vehicles in images.", "accepts": ["Object detection"] }
        ],  
                "explanation": "Box 1: Facial recognition - Face detection that perceives faces and attributes in an image; person identification that matches an individual in your private repository of up to 1 million people; perceived emotion recognition that detects a range of facial expressions like happiness, contempt, neutrality, and fear; and recognition and grouping of similar faces in images. Box 2: OCR - Box 3: Objection detection - Object detection is similar to tagging, but the API returns the bounding box coordinates (in pixels) for each object found. For example, if an image contains a dog, cat and person, the Detect operation will list those objects together with their coordinates in the image. You can use this functionality to process the relationships between the objects in an image. It also lets you determine whether there are multiple instances of the same tag in an image. The Detect API applies tags based on the objects or living things identified in the image. There is currently no formal relationship between the tagging taxonomy and the object detection taxonomy. At a conceptual level, the Detect API only finds objects and living things, while the Tag API can also include contextual terms like indoor, which can't be localized with bounding boxes. Reference: https://azure.microsoft.com/en-us/services/cognitive-services/face/ https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision/concept-object-detection"
        
    },
    {
        "id": 82,
        "type": "single_choice",
        "question": "You need to determine the location of cars in an image so that you can estimate the distance between the cars. Which type of computer vision should you use?",
        "options": [
            "A. optical character recognition (OCR)",
            "B. object detection",
            "C. image classification",
            "D. face detection"
        ],
        "answer": [
            "B. object detection"
        ],
        "explanation": "Correct Answer: B. Object detection is similar to tagging, but the API returns the bounding box coordinates (in pixels) for each object found. For example, if an image contains a dog, cat and person, the Detect operation will list those objects together with their coordinates in the image. You can use this functionality to process the relationships between the objects in an image. It also lets you determine whether there are multiple instances of the same tag in an image. The Detect API applies tags based on the objects or living things identified in the image. There is currently no formal relationship between the tagging taxonomy and the object detection taxonomy. At a conceptual level, the Detect API only finds objects and living things, while the Tag API can also include contextual terms like indoor, which can't be localized with bounding boxes. Reference: https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision/concept-object-detection"
    },
 {
        "id": 83,
        "type": "dropdown",
        "question": "You can use the _____________  service to train an object detection model by using your own images.",
        "options": [
          { "label": "Computer Vision", "value": "Computer Vision" },
          { "label": "Custom Vision", "value":  "Custom Vision" },
          { "label": "Form Recogonizer", "value": "Form Recogonizer" },
	  { "label": "Video Indexer", "value": "Video Indexer" }
        ],
        "correctAnswer": "Custom Vision",
        "placeholder": "— Select an option —",
        "explanation": "Correct Answer: Custom Vision. Azure Custom Vision is a cognitive service that lets you build, deploy, and improve your own image classifiers. An image classifier is an AI service that applies labels (which represent classes) to images, according to their visual characteristics. Unlike the Computer Vision service, Custom Vision allows you to specify the labels to apply. Note: The Custom Vision service uses a machine learning algorithm to apply labels to images. You, the developer, must submit groups of images that feature and lack the characteristics in question. You label the images yourself at the time of submission. Then the algorithm trains to this data and calculates its own accuracy by testing itself on those same images. Once the algorithm is trained, you can test, retrain, and eventually use it to classify new images according to the needs of your app. You can also export the model itself for offline use. Incorrect Answers: Computer Vision: Azure's Computer Vision service provides developers with access to advanced algorithms that process images and return information based on the visual features you're interested in. For example, Computer Vision can determine whether an image contains adult content, find specific brands or objects, or find human faces. Reference: https://docs.microsoft.com/en-us/azure/cognitive-services/custom-vision-service/home"
        
    },
    {
        "id": 84,
        "type": "single_choice",
        "question": "You send an image to a Computer Vision API and receive back the annotated image shown in the exhibit. Which type of computer vision was used?.",
        "options": [
            "A. object detection",
            "B. face detection",
            "C. optical character recognition (OCR)",
            "D. image classification"
        ],
        "answer": [
            "A. object detection"
        ],
        "explanation": "Correct Answer is A. Object detection is similar to tagging, but the API returns the bounding box coordinates for each tag applied. For example, if an image contains a dog, cat and person, the Detect operation will list those objects together with their coordinates in the image. Incorrect Answers: B: Optical character recognition (OCR) allows you to extract printed or handwritten text from images and documents. C: Scene segmentation determines when a scene changes in video based on visual cues. A scene depicts a single event and it's composed by a series of consecutive shots, which are semantically related.",
        "images": [
            "https://bongawat.wordpress.com/wp-content/uploads/2026/01/ai900_q84.png?w=NaN&h="
        ],
        "interactivity": null
    },
    {
        "id": 85,
        "question": "What are two tasks that can be performed by using the Computer Vision service?",
        "type": "multiple_choice",
        "options": [
            "A. Train a custom image classification model",
            "B. Detect faces in an image",
            "C. Recognize handwritten text",
            "D. Translate the text in an image between languages"
        ],
        "answer": [
            "B. Detect faces in an image",
            "C. Recognize handwritten text"
        ],
        "explanation": "B: Azure's Computer Vision service provides developers with access to advanced algorithms that process images and return information based on the visual features you're interested in. For example, Computer Vision can determine whether an image contains adult content, find specific brands or objects, or find human faces. C: Computer Vision includes Optical Character Recognition (OCR) capabilities. You can use the new Read API to extract printed and handwritten text from images and documents. Reference: https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision/home"
    },
    {
        "id": 86,
        "type": "single_choice",
        "question": "What is a use case for classification?",
        "options": [
            "A. Predicting how many cups of coffee a person will drink based on how many hours the person slept the previous night.",
            "B. Analyzing the contents of images and grouping images that have similar colors.",
            "C. Predicting whether someone uses a bicycle to travel to work based on the distance from home to work.",
            "D. Predicting how many minutes it will take someone to run a race based on past race times."
        ],
        "answer": "C. Predicting whether someone uses a bicycle to travel to work based on the distance from home to work.",
        "explanation": "Answe is C. Two-class classification provides the answer to simple two-choice questions such as Yes/No or True/False. Incorrect Answers: A: This is Regression. B: This is Clustering. D: This is Regression."
    },
    {
        "id": 87,
        "type": "multiple_choice",
        "question": "What are two tasks that can be performed by using computer vision?",
        "options": [
            "A. Predict stock prices",
            "B. Detect brands in an image",
            "C. Detect the color scheme in an image",
            "D. Translate text between languages",
            "E. Extract key phrases"
        ],
        "answer": [
            "B. Detect brands in an image",
            "C. Detect the color scheme in an image"
        ],
        "explanation": "B: Identify commercial brands in images or videos from a database of thousands of global logos. You can use this feature, for example, to discover which brands are most popular on social media or most prevalent in media product placement. C: Analyze color usage within an image. Computer Vision can determine whether an image is black & white or color and, for color images, identify the dominant and accent colors. Reference: https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision/overview"
    },
    {
        "id": 88,
        "type": "single_choice",
        "question": "You need to build an image tagging solution for social media that tags images of your friends automatically. Which Azure Cognitive Services service should you use?",
        "options": [
            "A. Face",
            "B. Form Recognizer",
            "C. Text Analytics",
            "D. Computer Vision"
        ],
        "answer": "A. Face",
        "explanation": "Correct Answer: A. Face service is designed for detecting and recognizing human faces in images. Reference: https://docs.microsoft.com/en-us/azure/cognitive-services/face/overview https://docs.microsoft.com/enus/azure/cognitive-services/face/face-api-how-to-topics/howtodetectfacesinimage"
    },
    {
        "id": 89,
        "type": "multiple_choice",
        "question": "In which two scenarios can you use the Form Recognizer service? Each correct answer presents a complete solution.",
        "options": [
            "A. Identify the retailer from a receipt",
            "B. Translate from French to English",
            "C. Extract the invoice number from an invoice",
            "D. Find images of products in a catalog"
        ],
        "answer": [
            "A. Identify the retailer from a receipt",
            "C. Extract the invoice number from an invoice"
        ],
        "explanation": "Correct Scenarios: A. Identify the retailer from a receipt: Why: Form Recognizer is designed to analyze receipts and extract key details such as the retailer's name,address, transaction amounts, or dates. This is a common use case for the service. C. Extract the invoice number from an invoice: Why: Another key feature of Form Recognizer is its ability to process invoices and extract fields like invoice numbers, dates, amounts, or customer details. This aligns perfectly with its capabilities. Incorrect Scenarios: B. Translate from French to English: Why Not: Translation is not a feature of Form Recognizer. Language translation tasks are handled by services like Azure Translator, not Form Recognizer. D. Find images of products in a catalog: Why Not: Form Recognizer processes text and structured data from documents, not images or product catalogs. Tasks involving images are better suited for services like Computer Vision or Custom Vision. Key Tip: Use Form Recognizer for extracting text-based data from documents like receipts, invoices, or forms. Focus on tasks related to data extraction and ignore tasks involving translation or image analysis. Reference: https://docs.microsoft.com/en-us/azure/applied-ai-services/form-recognizer/overview?tabs=v2-1"
    },
    {
        "id": 90,
        "type": "bucket",
        "question": "Match the facial recognition tasks to the appropriate questions.",
        "options": [
          { "id": "Grouping", "label": "Grouping" },
          { "id":  "Identification", "label":  "Identification"},
          { "id": "Similarity", "label": "Similarity" },
          { "id":  "Verification", "label": "Verification"}
        ],
        "buckets": [
          { "id": "Do two images of a face belong to the same person.", "accepts": [ "Verification"] },
          { "id": "Does this person look like other people?.", "accepts": ["Similarity"] },
          { "id": "Who is this person in this group of people?.", "accepts": [ "Identification"] }
        ],  
        "explanation": "Box 1: verification - Face verification: Check the likelihood that two faces belong to the same person and receive a confidence score. Box 2: similarity - Box 3: Grouping - Box 4: identification - Face detection: Detect one or more human faces along with attributes such as: age, emotion, pose, smile, and facial hair, including 27 landmarks for each face in the image. ~Face verification: The Verify API does an authentication against two detected faces or from one detected face to one person object. Practically, it evaluates whether two faces belong to the same person. ~Person identification: The Identify API is used to identify a detected face against a database of people (facial recognition search). This feature might be useful for automatic image tagging in photo management software. You create the database in advance, and you can edit it over time. Tips to remember:  verification = same person similarity = look like? grouping = belong together? identification = who is this person? Reference: https://azure.microsoft.com/en-us/services/cognitive-services/face/#features"
        
    },
    {
        "id": 91,
        "type": "single_choice",
        "question": "Which Computer Vision feature can you use to generate automatic captions for digital photographs?",
        "options": [
            "A. Recognize text",
            "B. Identify the areas of interest",
            "C. Detect objects",
            "D. Describe the images"
        ],
        "answer": "D. Describe the images",
        "explanation": "correct Answer: D. Describe images with human-readable language Computer Vision can analyze an image and generate a human-readable phrase that describes its contents. The algorithm returns several descriptions based on different visual features, and each description is given a confidence score. The final output is a list of descriptions ordered from highest to lowest confidence. The image description feature is part of the Analyze Image API. Reference: https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision/concept-describing-images"
    },
    {
        "id": 92,
        "type": "single_choice",
        "question": "Which service should you use to extract text, key/value pairs, and table data automatically from scanned documents?",
        "options": [
            "A. Custom Vision",
            "B. Face",
            "C. Form Recognizer",
            "D. Language"
        ],
        "answer": "C. Form Recognizer",
        "explanation": "Answer is C. Form Recognizer applies advanced machine learning to accurately extract text, key-value pairs, tables, and structures from documents. Reference: https://azure.microsoft.com/en-us/services/form-recognizer/"
    },

 {
        "id": 93,
        "type": "dropdown",
        "question": "_____________  extracts text from handwritten documents.",
        "options": [
          { "label": "Object detection", "value": "Object detection" },
          { "label": "Facial recognition", "value":  "Facial recognition" },
          { "label": "Image classification", "value": "Image classification"},
	  { "label": "Optical character recognition (OCR)", "value": "Optical character recognition (OCR)" }
        ],
        "correctAnswer": "Optical character recognition (OCR)",
        "placeholder": "— Select an option —",
        "explanation": "Handwriting OCR (optical character recognition) is the process of automatically extracting handwritten information from paper, scans and other low-quality digital documents."         
    },
    {
        "id": 94,
        "type": "single_choice",
        "question": "You are developing a solution that uses the Text Analytics service. You need to identify the main talking points in a collection of documents. Which type of natural language processing should you use?",
        "options": [
            "A. Entity recognition",
            "B. Key phrase extraction",
            "C. Sentiment analysis",
            "D. Language detection"
        ],
        "answer": "B. Key phrase extraction",
        "explanation": "Correct Answer B: Broad entity extraction: Identify important concepts in text, including key phrase extraction/ Broad entity extraction: Identify important concepts in text, including key phrases and named entities such as people, places, and organizations. Reference: https://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/natural-language-processing"
    },
    {
        "id": 95,
        "type": "multiple_choice",
        "question": "In which two scenarios can you use speech recognition? Each correct answer presents a complete solution.",
        "options": [
            "A. An in-car system that reads text messages aloud",
            "B. Providing closed captions for recorded or live videos",
            "C. Creating an automated public address system for a train station",
            "D. Creating a transcript of a telephone call or meeting"
        ],
        "answer": [
            "B. Providing closed captions for recorded or live videos",
            "D. Creating a transcript of a telephone call or meeting"
        ],
        "explanation": "B, D is correct as LUIS interpret meaning of text whereas text analytics is for sentiment or key Phrase extraction. Reference: https://azure.microsoft.com/en-gb/services/cognitive-services/speech-to-text/#features "
    },

 {
        "id": 96,
        "type": "dropdown",
        "question": "While presenting at a conference, your session in transcribed into subtitles for the audience. This is an example of _____________ .",
        "options": [
          { "label": "Sentiment analysis", "value": "Sentiment analysis" },
          { "label": "Speech recognition", "value":  "Speech recognition" },
          { "label": "Speech synthesis", "value": "Speech synthesis" },
	  { "label": "Translation", "value": "Translation"}
        ],
        "correctAnswer": "Speech recognition",
        "placeholder": "— Select an option —",
        "explanation": "Answer is: Speech recognition. Reference: https://azure.microsoft.com/en-gb/services/cognitive-services/speech-to-text/#features"
            },
    {
        "id": 97,
        "type": "single_choice",
        "question": "You need to build an app that will read recipe instructions aloud to support users who have reduced vision. Which version service should you use?",
        "options": [
            "A. Text Analytics",
            "B. Translator",
            "C. Speech",
            "D. Language Understanding (LUIS)"
        ],
        "answer": "C. Speech",
        "explanation": "Correct Answer: C. Speech service provides text-to-speech capabilities.Speech Recognition is actually Speech-to-Text. Reference https://azure.microsoft.com/en-us/products/ai-foundry/tools/speech#features"
    },
    {
        "id": 98,
        "type": "boolean",
        "question": "For each of the following statements, select Yes if the statement is true. Otherwise, select No.",
        "subQuestions": [
            {
                "prompt": "Transcribe a call to text - Speech Service: Speech to Text Service.",
                "correctAnswer": true
            },
            {
                "prompt": "Extract call Transcription to find key entity - Text Analytic: Entity Recognition.",
                "correctAnswer": true
            },
            {
                "prompt": "Translate a call to different language: Speech Service: Speech Translations.",
                "correctAnswer": true
            }
        ],
        "explanation": " Correct Answer Yes, Yes, Yes:  Transcribe a call to text -Speech Service: Speech to Text Service Extract call Transcription to find key entity - Text Analytic : Entity Recognition Translate a call to different language : Speech Service : Speech Translations Reference: https://docs.microsoft.com/en-gb/azure/cognitive-services/text-analytics/overview https://azure.microsoft.com/en-gb/services/cognitive-services/speech-services/",
        "images": []
    },
    {
        "id": 99,
        "type": "single_choice",
        "question": "Your website has a chatbot to assist customers. You need to detect when a customer is upset based on what the customer types in the chatbot. Which type of AI workload should you use?",
        "options": [
            "A. Anomaly detection",
            "B. Computer vision",
            "C. Regression",
            "D. Natural language processing"
        ],
        "answer": "D. Natural language processing",
        "explanation": "Coorect Answer: D. Natural language processing (NLP) is used for tasks such as sentiment analysis, topic detection, language detection, key phrase extraction, and document categorization. Sentiment Analysis is the process of determining whether a piece of writing is positive, negative or neutral. Reference: https://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/natural-language-processing"
    },
    {
        "id": 100,
        "type": "multiple_choice",
        "question": "You plan to develop a bot that will enable users to query a knowledge base by using natural language processing. Which two services should you include in the solution?",
        "options": [
            "A. QnA Maker",
            "B. Azure Bot Service",
            "C. Form Recognizer",
            "D. Anomaly Detector"
        ],
        "answer": [
            "A. QnA Maker",
            "B. Azure Bot Service"
        ],
        "explanation": "Correct Answer A, B. Reference: https://docs.microsoft.com/en-us/azure/bot-service/bot-service-overview-introduction?view=azure-bot-service-4.0 https://docs.microsoft.com/en-us/azure/cognitive-services/luis/choose-natural-language-processing-service",
        "interactivity": null
    },
    {
        "question_number": 101,
        "type": "multiple_choice",
        "question": "In which two scenarios can you use a speech synthesis solution?",
        "options": [
            "A. An automated voice that reads back a credit card number entered into a telephone by using a numeric keypad",
            "B. Generating live captions for a news broadcast",
            "C. Extracting key phrases from the audio recording of a meeting",
            "D. An AI character in a computer game that speaks audibly to a player"
        ],
        "answer": [
            "A. An automated voice that reads back a credit card number entered into a telephone by using a numeric keypad",
            "D. An AI character in a computer game that speaks audibly to a player"
        ],
        "explanation": "Coreect Answer A,D. Azure Text to Speech is a Speech service feature that converts text to lifelike speech. Incorrect Answers: C: Extracting key phrases is not speech synthesis. Reference: https://azure.microsoft.com/en-in/services/cognitive-services/text-to-speech/"
    },
    {
        "id": 102,
        "type": "boolean",
        "question": "For each of the following statements, select Yes if the statement is true. Otherwise, select No.",
        "subQuestions": [
            {
                "prompt": "You can use the translator service to translate text between languages.",
                "correctAnswer": true
            },
            {
                "prompt": "You can use the Translator service to detect the language of a given text.",
                "correctAnswer": true
            },
            {
                "prompt": "You can use the translator service to transcribe audible speech into text.",
                "correctAnswer": false
            }
        ],
        "explanation": "Correct Answer Yes, Yes, No. The translator service provides multi-language support for text translation, transliteration, language detection, and dictionaries. Speech-to-Text, also known as automatic speech recognition (ASR), is a feature of Speech Services that provides transcription. Reference: https://docs.microsoft.com/en-us/azure/cognitive-services/Translator/translator-info-overview https://docs.microsoft.com/en-us/legal/cognitive-services/speech-service/speech-to-text/transparency-note",
        "images": []
    },
    {
        "id": 103,
        "type": "bucket",
        "layout": "bucket",
        "question": "You need to scan the news for articles about your customers and alert employees when there is a negative article. Positive articles must be added to a press book. Which natural language processing tasks should you use to complete the process? To answer, drag the appropriate tasks to the correct locations. Each task may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.",
    
 "options": [
          { "id": "Entity recognition", "label": "Entity recognition" },
          { "id": "Sentiment analysis", "label": "Sentiment analysis"},
          { "id": "Speech synthesis", "label": "Speech synthesis"},
          { "id": "Translation", "label": "Translation" }
        ],
        "buckets": [
          { "id": "Box Number 1.", "accepts": ["Entity recognition"] },
          { "id": "Box Number 2.", "accepts": ["Sentiment analysis"] }
          
        ],  

               "explanation": "Box 1: Entity recognition - the Named Entity Recognition module in Machine Learning Studio (classic), to identify the names of things, such as people, companies, or locations in a column of text. Named entity recognition is an important area of research in machine learning and natural language processing (NLP), because it can be used to answer many real-world questions, such as: Which companies were mentioned in a news article? Does a tweet contain the name of a person? Does the tweet also provide his current location? Were specified products mentioned in complaints or reviews? Box 2: Sentiment Analysis - The Text Analytics API's Sentiment Analysis feature provides two ways for detecting positive and negative sentiment. If you send a Sentiment Analysis request, the API will return sentiment labels (such as negative, neutral and positive) and confidence scores at the sentence and document-level. Reference: https://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/named-entity-recognition https://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/how-tos/text-analytics-how-to-sentiment-analysis",
        "images": [
            "https://bongawat.wordpress.com/wp-content/uploads/2026/01/ai900_q103.png?w=NaN&h="
        ]
       
    },
    {
        "id": 104,
        "type": "single_choice",
        "question": "You are building a knowledge base by using QnA Maker. Which file format can you use to populate the knowledge base?",
        "options": [
            "A. PPTX",
            "B. XML",
            "C. ZIP",
            "D. PDF"
        ],
        "answer": "D. PDF",
        "explanation": "Correct answer is D. QnA Maker supports PDF, DOCX, and TXT for knowledge base ingestion.Reference: https://docs.microsoft.com/en-us/azure/cognitive-services/qnamaker/concepts/data-sources-and-content"
    },
    {
        "id": 105,
        "type": "single_choice",
        "question": "In which scenario should you use key phrase extraction?",
        "options": [
            "A. Identifying whether reviews of a restaurant are positive or negative",
            "B. Generating captions for a video based on the audio track",
            "C. Identifying which documents provide information about the same topics",
            "D. Translating a set of documents from English to German"
        ],
        "answer": "C. Identifying which documents provide information about the same topics",
        "explanation": "Correct Answer is C. Key phrase extraction identifies main concepts for topic grouping."
    },
    {
        "id": 106,
        "type": "single_choice",
        "question": "You have insurance claim reports that are stored as text. You need to extract key terms from the reports to generate summaries. Which type of AI workload should you use?",
        "options": [
            "A. Natural language processing",
            "B. Conversational AI",
            "C. Anomaly detection",
            "D. Computer vision"
        ],
        "answer": "A. Natural language processing",
        "explanation": "Correct Answer is A. NLP extracts key terms from text for summaries. Reference: https://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/natural-languageprocessing"
    },

 {
        "id": 107,
        "type": "dropdown",
        "question": "Natural language processing can be used to _____________ .",
        "options": [
          { "label": "classify email messages as work-related or prsonal", "value": "classify email messages as work-related or prsonal" },
          { "label": "predict the number of future car rentals", "value":  "predict the number of future car rentals" },
          { "label": "predict which website visitors will make a transaction", "value": "predict which website visitors will make a transaction" },
	  { "label": "stop a process in a factory when extremely high tempreatures are registed", "value": "stop a process in a factory when extremely high tempreatures are registed" }
        ],
        "correctAnswer": "classify email messages as work-related or prsonal",
        "placeholder": "— Select an option —",
        "explanation": "Answer is: classify email messages as work-related or prsonal. Natural language processing (NLP) is used for tasks such as sentiment analysis, topic detection, language detection, key phrase extraction, and document categorization. Reference: https://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/natural-language-processing"
             },
    {
        "id": 108,
        "type": "single_choice",
        "question": "Which AI service can you use to interpret the meaning of a user input such as 'Call me back later?'",
        "options": [
            "A. Translator",
            "B. Text Analytics",
            "C. Speech",
            "D. Language Understanding (LUIS)"
        ],
        "answer": "D. Language Understanding (LUIS)",
        "explanation": "Answer: D. Language Understanding (LUIS) is a cloud-based AI service, that applies custom machine-learning intelligence to a user's conversational, natural language text to predict overall meaning, and pull out relevant, detailed information. Reference: https://docs.microsoft.com/en-us/azure/cognitive-services/luis/what-is-luis"
    },
    {
        "id": 109,
        "type": "single_choice",
        "question": "You are developing a chatbot solution in Azure. Which service should you use to determine a user's intent?'",
        "options": [
            "A. Translator",
            "B. Text Analytics",
            "C. Speech",
            "D. Language Understanding (LUIS)"
        ],
        "answer": "D. Language Understanding (LUIS)",
        "explanation": "Answer: D. Language Understanding (LUIS) is a cloud-based AI service, that applies custom machine-learning intelligence to a user's conversational, natural language text to predict overall meaning, and pull out relevant, detailed information. Reference: https://docs.microsoft.com/en-us/azure/cognitive-services/luis/what-is-luis"
    },
    {
        "id": 110,
        "type": "single_choice",
        "question": "You need to make the written press releases of your company available in a range of languages. Which service should you use?",
        "options": [
            "A. Translator",
            "B. Text Analytics",
            "C. Speech",
            "D. Language Understanding (LUIS)"
        ],
        "answer": "A. Translator",
        "explanation": "Answer: A. Translator is a cloud-based machine translation service you can use to translate text in near real-time through a simple REST API call. The service uses modern neural machine translation technology and offers statistical machine translation technology. Custom Translator is an extension of Translator, which allows you to build neural translation systems. Reference: https://docs.microsoft.com/en-us/azure/cognitive-services/translator/"
    },
    {
        "id": 111,
        "type": "boolean",
        "question": "For each of the following statements, select Yes if the statement is true. Otherwise, select No.",
        "subQuestions": [
            {
                "prompt": "The Text Analytics service can identify in which language text is written.",
                "correctAnswer": true
            },
            {
                "prompt": "The Text Analytics service can detect handwritten signatures in a document.",
                "correctAnswer": false
            },
            {
                "prompt": "The Text Analytics service can identify companies and organizations mentioned in a document.",
                "correctAnswer": true
            }
        ],
        "explanation": "The Text Analytics API is a cloud-based service that provides advanced natural language processing over raw text, and includes four main functions: sentiment analysis, key phrase extraction, named entity recognition, and language detection. Box 1: Yes - You can detect which language the input text is written in and report a single language code for every document submitted on the request in a wide range of languages, variants, dialects, and some regional/cultural languages. The language code is paired with a score indicating the strength of the score. Box 2: No - Box 3: Yes - Named Entity Recognition: Identify and categorize entities in your text as people, places, organizations, date/time, quantities, percentages, currencies, and more. Well-known entities are also recognized and linked to more information on the web. Reference: https://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/overview",
        "images": []
    },

    {
        "id": 112,
        "type": "bucket",
        "layout": "bucket",
        "question": "Match the types of natural languages processing workloads to the appropriate scenarios. To answer, drag the appropriate workload type from the column on the left to its scenario on the right. Each workload type may be used once, more than once, or not at all.",
 "options": [
          { "id": "Entity recognition", "label": "Entity recognition" },
          { "id": "Key Phrase extraction", "label": "Key Phrase extraction" },
          { "id": "Sentiment analysis", "label": "Sentiment analysis"},
          { "id": "Translation", "label": "Translation"},
          { "id": "Speech recognition and speech synthesis", "label": "Speech recognition and speech synthesis" }
        ],
        "buckets": [
          { "id": "Extracts persons, locations, and organizations from the text.", "accepts": ["Entity recognition"] },
          { "id": "Evaluates text along a positive-negative scale.", "accepts": ["Sentiment analysis"] },
          { "id": "Converts text to a different language.", "accepts": ["Translation"] }
        ],  
      
        "explanation": "Box 1: Entity recognition - Named Entity Recognition (NER) is the ability to identify different entities in text and categorize them into pre-defined classes or types such as: person, location, event, product, and organization. Box 2: Sentiment analysis - Sentiment Analysis is the process of determining whether a piece of writing is positive, negative or neutral. Box 3: Translation - Using Microsoft's Translator text API This versatile API from Microsoft can be used for the following: Translate text from one language to another. Transliterate text from one script to another. Detecting language of the input text. Find alternate translations to specific text. Determine the sentence length. Reference: https://docs.microsoft.com/en-in/azure/cognitive-services/text-analytics/how-tos/text-analytics-how-to-entity-linking?tabs=version-3-preview https://azure.microsoft.com/en-us/services/cognitive-services/text-analytics"
    },
    {
        "id": 113,
        "type": "boolean",
        "question": "For each of the following statements, select Yes if the statement is true. Otherwise, select No.",
        "subQuestions": [
            {
                "prompt": "Monitoring Online service reviews for profanities is an example of natural language processing.",
                "correctAnswer": true
            },
            {
                "prompt": "Identifying brand logos in an image is an example of natural languages processing.",
                "correctAnswer": false
            },
            {
                "prompt": "Monitoring public news sites for negative mentions of a product is an example of natural language processing.",
                "correctAnswer": true
            }
        ],
        "explanation": "Box 1: Yes - Content Moderator is part of Microsoft Cognitive Services allowing businesses to use machine assisted moderation of text, images, and videos that augment human review. The text moderation capability now includes a new machine-learning based text classification feature which uses a trained model to identify possible abusive, derogatory or discriminatory language such as slang, abbreviated words, offensive, and intentionally misspelled words for review.Box 2: No - Azure's Computer Vision service gives you access to advanced algorithms that process images and return information based on the visual features you're interested in. For example, Computer Vision can determine whether an image contains adult content, find specific brands or objects, or find human faces. Box 3: Yes - Natural language processing (NLP) is used for tasks such as sentiment analysis, topic detection, language detection, key phrase extraction, and document categorization. Sentiment Analysis is the process of determining whether a piece of writing is positive, negative or neutral. Reference: https://azure.microsoft.com/es-es/blog/machine-assisted-text-classification-on-content-moderator-public-preview/ https://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/natural-language-processing",
        "images": []
    },
    {
        "id": 114,
        "type": "single_choice",
        "question": "You are developing a natural language processing solution in Azure. The solution will analyze customer reviews and determine how positive or negative each review is. This is an example of which type of natural language processing workload?",
        "options": [
            "A. language detection",
            "B. sentiment analysis",
            "C. key phrase extraction",
            "D. entity recognition"
        ],
        "answer": "B. sentiment analysis",
        "explanation": "Answer: B. Sentiment Analysis is the process of determining whether a piece of writing is positive, negative or neutral. Reference: https://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/natural-language-processing"
    },
    {
        "id": 115,
        "type": "single_choice",
        "question": "You use natural language processing to process text from a Microsoft news story. You receive the output shown in the following exhibit. Which type of natural languages processing was performed?",
        "options": [
            "entity recognition",
            "key phrase extraction",
            "sentiment analysis",
            "translation"
        ],
        "answer": "entity recognition",
        "images": [
            "https://bongawat.wordpress.com/wp-content/uploads/2026/01/ai900_q115.png?w=NaN&h="
        ],
        "explanation": "Correct answer A. Named Entity Recognition (NER) is the ability to identify different entities in text and categorize them into pre-defined classes or types such as: person, location, event, product, and organization. In this question, the square brackets indicate the entities such as DateTime, PersonType, Skill. Reference: https://docs.microsoft.com/en-in/azure/cognitive-services/text-analytics/how-tos/text-analytics-how-to-entity-linking?tabs=version-3-preview"
    },
    {
        "id": 116,
        "type": "bucket",
        "layout": "bucket",
        "question": "You plan to apply Text Analytics API features to a technical support ticketing system. Match the Text Analytics API features to the appropriate natural language processing scenarios. To answer, drag the appropriate feature from the column on the left to its scenario on the right. Each feature may be used once, more than once, or not at all.",
 "options": [
          { "id": "Entity Recognition", "label": "Entity Recognition" },
          { "id": "Key phrase extraction", "label": "Key phrase extraction"},
          { "id": "Language detection", "label": "Language detection" },
          { "id": "Sentiment analysis", "label": "Sentiment analysis" }
        ],
        "buckets": [
          { "id": "Understand how upset a customer is based on text contained in the support ticket.", "accepts": ["Sentiment analysis"] },
          { "id": "Summarize important information from the support ticket.", "accepts": ["Key phrase extraction"] },
          { "id": "Extract key dates from the support ticket.", "accepts": ["Entity Recognition"] }
        ],  
        "explanation": "Box1: Sentiment analysis - Sentiment Analysis is the process of determining whether a piece of writing is positive, negative or neutral. Box 2: Broad entity extraction - Broad entity extraction: Identify important concepts in text, including Key phrase extraction/ Broad entity extraction: Identify important concepts in text, including key phrases and named entities such as people, places, and organizations. Box 3: Entity Recognition - Named Entity Recognition: Identify and categorize entities in your text as people, places, organizations, date/time, quantities, percentages, currencies, and more. Well-known entities are also recognized and linked to more information on the web. Reference: https://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/natural-language-processing https://azure.microsoft.com/en-us/services/cognitive-services/text-analytics"
      
    },
    {
        "id": 117,
        "type": "single_choice",
        "question": "You are authoring a Language Understanding (LUIS) application to support a music festival. You want users to be able to ask questions about scheduled shows, such as: 'Which act is playing on the main stage?' The question 'Which act is playing on the main stage?' is an example of which type of element?",
        "options": [
            "A. an intent",
            "B. an utterance",
            "C. a domain",
            "D. an entity"
        ],
        "answer": "B. an utterance",
        "explanation": "Answer is B. Utterances are user input that the app needs to interpret. Reference: https://docs.microsoft.com/en-us/azure/cognitive-services/LUIS/luis-concept-utterance"
    },
    {
        "id": 118,
        "type": "single_choice",
        "question": "You build a QnA Maker bot by using a frequently asked questions (FAQ) page. You need to add professional greetings and other responses to make the bot more user friendly. What should you do?",
        "options": [
            "A. Increase the confidence threshold of responses",
            "B. Enable active learning",
            "C. Create multi-turn questions",
            "D. Add chit-chat"
        ],
        "answer": "D. Add chit-chat",
        "explanation": "Answer is D. Chitchat is correct. You can choose between different personas. Reference: https://docs.microsoft.com/en-us/azure/cognitive-services/qnamaker/how-to/chit-chat-knowledge-base?tabs=v1"
    },
    {
        "id": 119,
        "type": "single_choice",
        "question": "You need to develop a chatbot for a website. The chatbot must answer users' questions based on the information in the following documents: A product troubleshooting guide in a Microsoft Word document, A frequently asked questions (FAQ) list on a webpage. Which service should you use to process the documents?",
        "options": [
            "A. Azure Bot Service",
            "B. Language Understanding",
            "C. Text Analytics",
            "D. QnA Maker"
        ],
        "answer": "D. QnA Maker",
        "explanation": "QnA Maker is a service designed to build conversational question-and-answer experiences. It processes information from documents like FAQs, Word files, and webpages to create a knowledge base for chatbots. Why D is Correct: It extracts and organizes information from FAQs and documents. Perfect for creating a chatbot that answers based on specific content. Why Not the Others? A. Azure Bot Service: Manages chatbot hosting but doesn’t process documents. B. Language Understanding: Focuses on interpreting user intent, not extracting Q&A content. C. Text Analytics: Analyzes text for sentiment or key phrases but doesn’t create Q&A systems. Key Tip: Use QnA Maker for chatbots needing Q&A data from documents and webpages. Reference: https://docs.microsoft.com/en-us/azure/cognitive-services/QnAMaker/Overview/overview"
    },
    {
        "id": 120,
        "type": "single_choice",
        "question": "You are building a Language Understanding model for an e-commerce business. You need to ensure that the model detects when utterances are outside the intended scope of the model. What should you do?",
        "options": [
            "A. Test the model by using new utterances",
            "B. Add utterances to the None intent",
            "C. Create a prebuilt task entity",
            "D. Create a new model"
        ],
        "answer": "B. Add utterances to the None intent",
        "explanation": "Answer is B. The None intent is filled with utterances that are outside of your domain. In Language Understanding (LUIS) models, the None intent is used to handle utterances that are outside the model's intended scope. Adding irrelevant or out-of-scope utterances to the None intent ensures the model can correctly identify and disregard such inputs. Why B is Correct: Adding examples of out-of-scope utterances to the None intent helps the model distinguish between valid and irrelevant inputs. Why Not the Others? A. Test the model by using new utterances: Testing helps validate the model but doesn’t directly teach it to handle out-of-scope utterances. C. Create a prebuilt task entity: Entities are for extracting data, not detecting out-of-scope inputs. D. Create a new model: A new model is unnecessary; handling out-of-scope utterances is a built-in feature using the None intent. Key Tip: Use the None intent to classify and handle irrelevant or out-of-scope utterances in LUIS models.Reference: https://docs.microsoft.com/en-us/azure/cognitive-services/LUIS/luis-concept-intent"
    },
    {
        "id": 121,
        "type": "multiple_choice",
        "question": "Which two scenarios are examples of a natural language processing workload? Each correct answer presents a complete solution.",
        "options": [
            "A. monitoring the temperature of machinery to turn on a fan when the temperature reaches a specific threshold",
            "B. a smart device in the home that responds to questions such as, 'What will the weather be like today?'",
            "C. a website that uses a knowledge base to interactively respond to users' questions",
            "D. assembly line machinery that autonomously inserts headlamps into cars"
        ],
        "answer": [
            "B. a smart device in the home that responds to questions such as, 'What will the weather be like today?'",
            "C. a website that uses a knowledge base to interactively respond to users' questions"
        ],
        "explanation": "Answer BC.  Natural language processing (NLP) is used for tasks such as sentiment analysis, topic detection, language detection, key phrase extraction, and document categorization. Reference: https://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/natural-language-processing"
    },
    {
        "id": 122,
        "type": "multiple_choice",
        "question": "You have an AI solution that provides users with the ability to control smart devices by using verbal commands. Which two types of natural language processing (NLP) workloads does the solution use? Each correct answer presents part of the solution.",
        "options": [
            "A. text-to-speech",
            "B. key phrase extraction",
            "C. speech-to-text",
            "D. language modeling",
            "E. translation"
        ],
        "answer": [
            "C. speech-to-text",
            "D. language modeling"
        ],
        "explanation": "Answer is C,D. speech-to-text and language modeling. You need to use language modeling to determine the intent of the utterance and to perform an action based on that intent. The AI solution for controlling smart devices using verbal commands involves the following NLP workloads: C. Speech-to-text: Converts spoken commands into text so they can be processed by the AI system. D. Language modeling: Helps the system understand and interpret the meaning of the transcribed text (verbal commands). Why Not the Others?A. Text-to-speech: Converts text to spoken words but is not used for understanding verbal commands. B. Key phrase extraction: Identifies important phrases in text but isn’t directly related to command processing. E. Translation: Converts text from one language to another, which isn’t required here. Key Tip: For verbal command processing, focus on Speech-to-text (conversion of spoken words) and Language modeling (understanding the command)."
    },
    {
        "id": 123,
        "type": "boolean",
        "question": "For each of the following statements, select Yes if the statement is true. Otherwise, select No.",
        "subQuestions": [
            {
                "prompt": "The Language service can identify in which language text is written.",
                "correctAnswer": true
            },
            {
                "prompt": "The Language service can detect handwritten signatures in a document.",
                "correctAnswer": false
            },
            {
                "prompt": "The Language service can identify companies and organizations mentioned in a document.",
                "correctAnswer": true
            }
        ],
        "explanation": "Box 1: Yes - Azure Cognitive Service for Language provides features including:  Language detection: This pre-configured feature evaluates text, and determines the language it was written in. It returns a language identifier and a score that indicates the strength of the analysis. Box 2: No - Handwritten detection is part of OCR (Optical Character Recognition). Box 3: Yes - Azure Cognitive Service for Language provides features including: Named Entity Recognition (NER): This pre-configured feature identifies entities in text across several predefined categories. Note: Named entity recognition is a natural language processing technique that can automatically scan entire articles and pull out some fundamental entities in a text and classify them into predefined categories. Entities may be, Organizations, Quantities, Monetary values, Percentages, and more. People's names - Company names - Geographic locations (Both physical and political) Product names - Dates and times - Amounts of money - Names of events - Reference: https://docs.microsoft.com/en-us/azure/cognitive-services/language-service/overview",
        "images": []
    },
    {
        "id": 124,
        "type": "bucket",
        "layout": "bucket",
        "question": "You plan to use Azure Cognitive Services to develop a voice controlled personal assistant app. Match the Azure Cognitive Services to the appropriate tasks. To answer, drag the appropriate service from the column on the left to its description on the right. Each service may be used once, more than once, or not at all.",

 "options": [
          { "id": "Speech", "label": "Speech" },
          { "id": "Language service", "label": "Language service"},
          { "id":  "Translator text", "label":  "Translator text" }
          
        ],
        "buckets": [
          { "id": "Convert a user's speech to text.", "accepts": ["Speech"] },
          { "id": "Identify a user's intent.", "accepts": ["Language service"] },
          { "id": "Provide a spoken response to the user.", "accepts": [ "Speech"] }
        ],  
        "explanation": "Box 1: Speech - The Speech service provides speech-to-text and text-to-speech capabilities with an Azure Speech resource. You can transcribe speech to text with high accuracy, produce natural-sounding text-to-speech voices, translate spoken audio, and use speaker recognition during conversations. Box 2: Language service - Build applications with conversational language understanding, a Cognitive Service for Language feature that understands natural language to interpret user goals and extracts key information from conversational phrases. Create multilingual, customizable intent classification and entity extraction models for your domainspecific keywords or phrases across 96 languages. Box 3: Speech - Incorrect: Not Translator text: Text translation is a cloud-based REST API feature of the Translator service that uses neural machine translation technology to enable quick and accurate source-to-target text translation in real time across all supported languages. Reference: https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/overview https://azure.microsoft.com/en-us/services/cognitive-services/conversational-language-understanding/ https://docs.microsoft.com/en-us/azure/cognitive-services/translator/text-translation-overview"
    },
    {
        "id": 125,
        "type": "single_choice",
        "question": "You need to make the written press releases of your company available in a range of languages. Which service should you use?",
        "options": [
            "A. Language service",
            "B. Translator",
            "C. Speech",
            "D. Personalizer"
        ],
        "answer": "B. Translator",
        "explanation": "Answer: B. Translator, an AI service for real-time document and text translation. Translate text instantly or in batches across more than 100 languages, powered by the latest innovations in machine translation. Support a wide range of use cases, such as translation for call centers, multilingual conversational agents, or in-app communication. Reference: https://azure.microsoft.com/en-us/services/cognitive-services/translator/4"
    },
    {
        "id": 126,
        "type": "single_choice",
        "question": "You have insurance claim reports that are stored as text. You need to extract key terms from the reports to generate summaries. Which type of AI workload should you use?",
        "options": [
            "A. anomaly detection",
            "B. natural language processing",
            "C. computer vision",
            "D. knowledge mining"
        ],
        "answer": "B. natural language processing",
        "explanation": "Answer: B. Key phrase extraction is one of the features offered by Azure Cognitive Service for Language, a collection of machine learning and AI algorithms in the cloud for developing intelligent applications that involve written language. Use key phrase extraction to quickly identify the main concepts in text. For example, in the text The food was delicious and the staff were wonderful., key phrase extraction will return the main topics: food and wonderful staff. Reference: https://docs.microsoft.com/en-us/azure/cognitive-services/language-service/key-phrase-extraction/overview"
    },
    {
        "id": 127,
        "type": "single_choice",
        "question": "You need to build an app that will read recipe instructions aloud to support users who have reduced vision. Which version service should you use?",
        "options": [
            "A. Language service",
            "B. Translator",
            "C. Speech",
            "D. Personalizer"
        ],
        "answer": "C. Speech",
        "explanation": "Answer: C. Speech, a managed service offering industry-leading speech capabilities such as speech-to-text, text-tospeech, speech translation, and speaker recognition. Reference: https://azure.microsoft.com/en-us/services/cognitive-services/speech-services/"
    },
    {
        "id": 128,
        "type": "single_choice",
        "question": "You have a webchat bot that provides responses from a QnA Maker knowledge base. You need to ensure that the bot uses user feedback to improve the relevance of the responses over time. What should you use?",
        "options": [
            "A. key phrase extraction",
            "B. sentiment analysis",
            "C. business logic",
            "D. active learning"
        ],
        "answer": "D. active learning",
        "explanation": "Answer: D. Active learning. 1.Bot gets the answer from the knowledge base with the GenerateAnswer API, using the top property to get a number of answers.2.Bot determines explicit feedback: Using your own custom business logic, filter out low scores. In the bot or client-application, display list of possible answers to the user and get user's selected answer. 3.Bot sends selected answer back to QnA Maker with the Train API. Reference: https://docs.microsoft.com/en-us/azure/cognitive-services/qnamaker/how-to/improve-knowledge-base"
    },
    {
        "id": 129,
        "type": "single_choice",
        "question": "You are developing a conversational AI solution that will communicate with users through multiple channels including email, Microsoft Teams, and webchat. Which service should you use?",
        "options": [
            "A. Text Analytics",
            "B. Azure Bot Service",
            "C. Translator",
            "D. Form Recognizer"
        ],
        "answer": "B. Azure Bot Service",
        "explanation": "Answer: B. The Azure Bot Service is specifically designed to build conversational AI solutions that can integrate with multiple communication channels such as email, Microsoft Teams, and webchat. It provides tools to create, manage, and deploy chatbots seamlessly across these platforms. Why B is Correct: Azure Bot Service allows integration with various channels and handles the communication logic needed for conversational AI. Why Not the Others? A. Text Analytics: Extracts insights from text (e.g., sentiment, key phrases) but doesn’t support building or deploying conversational bots. C. Translator: Provides translation services but does not enable chatbot development. D. Form Recognizer: Extracts data from documents and forms but is unrelated to conversational AI. Key Tip: Use Azure Bot Service for developing and deploying chatbots across multiple communication channels. Reference: https://docs.microsoft.com/en-us/azure/bot-service/bot-service-overview-introduction?view=azure-botservice-4.0"
    },
    {
        "id": 130,
        "type": "boolean",
        "question": "For each of the following statements, select Yes if the statement is true. Otherwise, select No.",
        "subQuestions": [
            {
                "prompt": "A bot that responds to queries by internal users is an example of a conversational AI workload.",
                "correctAnswer": true
            },
            {
                "prompt": "An application that displays images relating to an entered search term is an example of a conversational AI workload.",
                "correctAnswer": false
            },
            {
                "prompt": "A web form used to submit a request to reset a password is an example of a conversational AI workload.",
                "correctAnswer": false
            }
        ],
        "explanation": "Answer: Yes, No, No. Key Tip: Conversational AI involves systems capable of natural language interaction, like chatbots or virtual assistants. Regular applications, forms, or search systems do not fall under this category. Reference: https://docs.microsoft.com/en-us/azure/bot-service/bot-service-overview-introduction?view=azure-botservice-4.0",
        "images": []
    },
    {
        "id": 131,
        "type": "multiple_choice",
        "question": "You need to provide content for a business chatbot that will help answer simple user queries. What are three ways to create question and answer text by using QnA Maker?",
        "options": [
            "A. Generate the questions and answers from an existing webpage.",
            "B. Use automated machine learning to train a model based on a file that contains the questions.",
            "C. Manually enter the questions and answers.",
            "D. Connect the bot to the Cortana channel and ask questions by using Cortana.",
            "E. Import chit-chat content from a predefined data source."
        ],
        "answer": [
            "A. Generate the questions and answers from an existing webpage.",
            "C. Manually enter the questions and answers.",
            "E. Import chit-chat content from a predefined data source."
        ],
        "explanation": "Answer: ACE. Automatic extraction - Extract question-answer pairs from semi-structured content, including FAQ pages, support websites, excel files, SharePoint documents, product manuals and policies. Reference: https://docs.microsoft.com/en-us/azure/cognitive-services/qnamaker/concepts/content-types"
    },
    {
        "id": 132,
        "type": "single_choice",
        "question": "You have a frequently asked questions (FAQ) PDF file. You need to create a conversational support system based on the FAQ. Which service should you use?",
        "options": [
            "A. QnA Maker",
            "B. Text Analytics",
            "C. Computer Vision",
            "D. Language Understanding (LUIS)"
        ],
        "answer": "A. QnA Maker",
        "explanation": "Answer: A. QnA Maker is a cloud-based API service that lets you create a conversational question-and-answer layer over your existing data. Use it to build a knowledge base by extracting questions and answers from your semistructured content, including FAQs, manuals, and documents. Reference: https://azure.microsoft.com/en-us/services/cognitive-services/qna-maker/"
    },
    {
        "id": 133,
        "type": "multiple_choice",
        "question": "You need to reduce the load on telephone operators by implementing a chatbot to answer simple questions with predefined answers. Which two AI services should you use to achieve the goal?",
        "options": [
            "A. Text Analytics",
            "B. QnA Maker",
            "C. Azure Bot Service",
            "D. Translator"
        ],
        "answer": [
            "B. QnA Maker",
            "C. Azure Bot Service"
        ],
        "explanation": "Answer: BC. Bots are a popular way to provide support through multiple communication channels. You can use the QnA Maker service and Azure Bot Service to create a bot that answers user questions. Reference: https://docs.microsoft.com/en-us/learn/modules/build-faq-chatbot-qna-maker-azure-bot-service/"
    },
    {
        "id": 134,
        "type": "multiple_choice",
        "question": "Which two scenarios are examples of a conversational AI workload? Each correct answer presents a complete solution.",
        "options": [
            "A. a smart device in the home that responds to questions such as What will the weather be like today?",
            "B. a website that uses a knowledge base to interactively respond to users' questions",
            "C. assembly line machinery that autonomously inserts headlamps into cars",
            "D. monitoring the temperature of machinery to turn on a fan when the temperature reaches a specific threshold"
        ],
        "answer": [
            "A. a smart device in the home that responds to questions such as What will the weather be like today?",
            "B. a website that uses a knowledge base to interactively respond to users' questions"
        ],
        "explanation": "Answer: AB. A - smart device B - website C&D don't look like as chat-bots. Conversational AI workloads involve systems that can understand, process, and respond to human input (usually in natural language, either through text or speech). Why A and B are Correct: A. A smart device in the home that responds to questions such as What will the weather be like today?: This is a clear example of conversational AI. The smart device listens to verbal queries and provides spoken responses based on information, which is a typical function of virtual assistants (e.g., Alexa, Google Assistant). B. A website that uses a knowledge base to interactively respond to users' questions: This is another example of conversational AI. It involves an interactive Q&A system (such as a chatbot) that uses a knowledge base to respond to user queries in natural language. Why Not C and D: C. Assembly line machinery that autonomously inserts headlamps into cars: This is an example of automation, not conversational AI. It involves mechanical processes, not natural language interaction. D. Monitoring the temperature of machinery to turn on a fan when the temperature reaches a specific threshold: This is an example of environmental monitoring and automation. It doesn't involve conversational interaction or natural language processing, so it isn't considered conversational AI.Key Tip: Conversational AI systems allow users to interact with machines or software using natural language (e.g.,chatbots, virtual assistants)."
    },
    {
        "id": 135,
        "type": "single_choice",
        "question": "You have the process shown in the following exhibit. Which type of AI solution is shown in the diagram?",
        "options": [
            "A. a sentiment analysis solution",
            "B. a chatbot",
            "C. a machine learning model",
            "D. a computer vision application"
        ],
        "answer": "B. a chatbot",
        "images": [
            "https://bongawat.wordpress.com/wp-content/uploads/2026/01/ai900_q135.png?w=NaN&h="
        ],
        "explanation": "Answer:B. In the provided diagram, the process shows the interaction between a user and a bot, which responds to user queries using a knowledge base. The bot's response is based on a structured data format (JSON) that includes a specific answer with metadata. Why B is Correct (Chatbot): The bot responds to a user query (How do I programmatically update my Knowledge Base?) and provides an answer. The response comes from a knowledge base that the bot can access, which is typically part of a chatbot solution. The interaction shown—where a user asks a question and gets a structured, informative response from a knowledge base—is characteristic of a chatbot system that handles user queries. Why Not the Others? A. Sentiment analysis solution: Sentiment analysis involves understanding the sentiment or emotion behind text (e.g., positive, negative, neutral). This scenario focuses on information retrieval and not sentiment analysis. C. Machine learning model: The solution shown doesn't involve training or predictive models but is instead using predefined responses from a knowledge base, which is common in chatbots. D. Computer vision application: Computer vision is used for analyzing and processing visual data (images or videos). There is no indication of any visual data or image processing in this scenario. Key Tip: The diagram shows a chatbot interacting with a knowledge base to provide answers to queries, which is a classic conversational AI scenario."
    },
    {
        "id": 136,
        "type": "single_choice",
        "question": "You need to develop a web-based AI solution for a customer support system. Users must be able to interact with a web app that will guide them to the best resource or answer. Which service should you use?",
        "options": [
            "A. Custom Vision",
            "B. QnA Maker",
            "C. Translator Text",
            "D. Face"
        ],
        "answer": "B. QnA Maker",
        "explanation": "Answer: B. QnA Maker is a cloud-based API service that lets you create a conversational question-and-answer layer over your existing data. Use it to build a knowledge base by extracting questions and answers from your semistructured content, including FAQs, manuals, and documents. Answer users' questions with the best answers from the QnAs in your knowledge base automatically. Your knowledge base gets smarter, too, as it continually learns from user behavior. Incorrect Answers: A: Azure Custom Vision is a cognitive service that lets you build, deploy, and improve your own image classifiers. An image classifier is an AI service that applies labels (which represent classes) to images, according to their visual characteristics. Unlike the Computer Vision service, Custom Vision allows you to specify the labels to apply. D: Azure Cognitive Services Face Detection API: At a minimum, each detected face corresponds to a faceRectangle field in the response. This set of pixel coordinates for the left, top, width, and height mark the located face. Using these coordinates, you can get the location of the face and its size. In the API response, faces are listed in size order from largest to smallest. Reference: https://azure.microsoft.com/en-us/services/cognitive-services/qna-maker/"
    },
    {
        "id": 137,
        "type": "single_choice",
        "question": "Which AI service should you use to create a bot from a frequently asked questions (FAQ) document?",
        "options": [
            "A. QnA Maker",
            "B. Language Understanding (LUIS)",
            "C. Text Analytics",
            "D. Speech"
        ],
        "answer": "A. QnA Maker",
        "explanation": "Answer: A. QnA Maker is the AI service designed specifically to create bots that can answer questions based on a frequently asked questions (FAQ) document. It allows you to upload FAQs or other knowledge base documents and turns them into a question-answering bot. Why A is Correct: QnA Maker is ideal for building conversational AI systems like chatbots that respond to user queries using a pre-existing knowledge base (such as FAQs). Why Not the Others? B. Language Understanding (LUIS): LUIS is used to build language models that understand user intent and extract entities, not for creating a Q&A system from a document. C. Text Analytics: Text Analytics is used for tasks like sentiment analysis, key phrase extraction, or entity recognition, but it doesn’t specifically turn FAQs into a bot. D. Speech: Speech is used for converting speech to text or vice versa, but it doesn't build bots from FAQs or documents. Key Tip: Use QnA Maker when you need to create a bot that answers questions from documents, such as FAQs. It’s specifically built for question-answering systems."
    },

 {
        "id": 138,
        "type": "dropdown",
        "question": "The interactive answering of questions entered by a user as part of an application is an example of _____________ .",
        "options": [
          { "label": "Anomaly detection", "value": "Anomaly detection" },
          { "label": "Computer vision", "value":  "Computer vision" },
          { "label": "Conversational AI", "value": "Conversational AI" },
	  { "label":  "Forecasting", "value":  "Forecasting" }
        ],
        "correctAnswer": "Conversational AI",
        "placeholder": "— Select an option —",
        "explanation": "Conversational AI is the right answer. ...With Microsoft's Conversational AI tools developers can build, connect, deploy, and manage intelligent bots that naturally interact with their users on a website, app, Cortana, Microsoft Teams, Skype, Facebook Messenger, Slack, and more. Reference: https://azure.microsoft.com/en-in/blog/microsoft-conversational-ai-tools-enable-developers-to-build-connect-and-manage-intelligent-bots"
           },
    {
        "id": 139,
        "type": "single_choice",
        "question": "Which scenario is an example of a webchat bot?",
        "options": [
            "A. Determine whether reviews entered on a website for a concert are positive or negative, and then add a thumbs up or thumbs down emoji to the reviews.",
            "B. Translate into English questions entered by customers at a kiosk so that the appropriate person can call the customers back.",
            "C. Accept questions through email, and then route the email messages to the correct person based on the content of the message.",
            "D. From a website interface, answer common questions about scheduled events and ticket purchases for a music festival."
        ],
        "answer": "D. From a website interface, answer common questions about scheduled events and ticket purchases for a music festival.",
        "explanation": "Answer: D. A webchat bot refers to a chatbot integrated into a website, allowing users to interact with it in real-time via a chat interface. The bot typically answers questions or performs actions based on the user's input. Why D is Correct: This scenario clearly describes a chatbot on a website that answers common questions about events and ticket purchases, which is a typical use of a webchat bot. Why Not the Others? A. Determine whether reviews entered on a website for a concert are positive or negative, and then add a thumbs up or thumbs down emoji to the reviews: This is an example of sentiment analysis and not a chatbot, as it focuses on analyzing reviews rather than engaging in conversation. B. Translate into English questions entered by customers at a kiosk so that the appropriate person can call the customers back: This involves translation and human routing, not a conversational interaction with a bot. C. Accept questions through email, and then route the email messages to the correct person based on the content of the message: This describes email routing and automation but not a chatbot interaction on a website. Key Tip: A webchat bot directly interacts with users through a chat interface on a website, making it ideal for answering questions in real-time."
    },
 {
        "id": 140,
        "type": "boolean",
        "question": "For each of the following statements, select Yes if the statement is true. Otherwise, select No.",
        "subQuestions": [
            {
                "prompt": "You can use QnA Maker to query an Azure SQL database.",
                "correctAnswer": false
            },
            {
                "prompt": "You should use QnA Maker when you want a knowledge base to provide the same answer to different users who submit similar questions.",
                "correctAnswer": true
            },
            {
                "prompt": "The QnA Maker service can determine the intent of user utterance.",
                "correctAnswer": false
            }
        ],
        "explanation": "Answer: No, Yes, No. Language Understanding (LUIS) and QnA Maker solve different issues. LUIS determines the intent of a user's text (known as an utterance), while QnA Maker determines the answer to a user's text (known as a query). Reference: https://docs.microsoft.com/en-gb/azure/cognitive-services/qnamaker/concepts/data-sources-and-content https://docs.microsoft.com/en-us/azure/cognitive-services/luis/choose-natural-language-processing-service",
        "images": []
    },
    {
        "id": 141,
        "type": "boolean",
        "question": "For each of the following statements, select Yes if the statement is true. Otherwise, select No.",
        "subQuestions": [
            {
                "prompt": "You can communicate with a bot using Cortana.",
                "correctAnswer": true
            },
            {
                "prompt": "You can communicate with a bot using Microsoft Teams.",
                "correctAnswer": true
            },
            {
                "prompt": "You can communicate with a bot using a webchat interface.",
                "correctAnswer": true
            }
        ],
        "explanation": "Answer Yes, Yes, Yes. You can connect a bot via: Alexa, Office 365 email, Facebook, Kik, LINE, Teams, Skype for Business, Slack, Telegram, WeChat, Webex Reference: The Channels List for Bots: https://learn.microsoft.com/en-us/azure/bot-service/bot-service-managechannels?view=azure-bot-service-4.0 https://docs.microsoft.com/en-us/azure/bot-service/bot-service-manage-channels?view=azure-bot-service-4.0",
        "images": []
    },
    {
        "id": 142,
        "type": "boolean",
        "question": "For each of the following statements, select Yes if the statement is true. Otherwise, select No.",
        "subQuestions": [
            {
                "prompt": "A restaurant can use a chatbot to empower customers to make reservations by using a website or an app.",
                "correctAnswer": true
            },
            {
                "prompt": "A restaurent can use a chatbot to answer inquiries about business hours from webpage.",
                "correctAnswer": true
            },
            {
                "prompt": "A restaurant can use a chatbot to automate responses to customer reviews on an external website.",
                "correctAnswer": true
            }
        ],
        "explanation": "Answer Yes, Yes, Yes. 1. A restaurant can use a chatbot to empower customers to make reservations by using a website or an app. Answer: Yes Chatbots can assist users in booking tables directly through websites or apps by interacting with them in realtime. Tip: Use chatbots to simplify reservation processes, improving customer convenience. 2. A restaurant can use a chatbot to answer inquiries about business hours from a webpage. Answer: Yes Chatbots can provide instant answers to FAQs, including business hours, directly from a webpage. Tip: Program chatbots to handle common customer queries to save time and improve accessibility. 3. A restaurant can use a chatbot to automate responses to customer reviews on an external website. Answer: Yes Some external platforms may allow chatbot integration or APIs to post automated responses to customer reviews. However, this depends on the platform’s rules and permissions. For instance, Google and Facebook support limited automation via APIs, while others may not. Key Tip: If the platform allows it, you can integrate chatbots to respond to reviews. Always ensure responses are professional and platform-compliant.",
        "images": []
    },
    {
        "id": 143,
        "type": "multiple_choice",
        "question": "Which two scenarios are examples of a conversational AI workload? Each correct answer presents a complete solution.",
        "options": [
            "A. a telephone answering service that has a pre-recorded message",
            "B. a chatbot that provides users with the ability to find answers on a website by themselves",
            "C. telephone voice menus to reduce the load on human resources",
            "D. a service that creates frequently asked questions (FAQ) documents by crawling public websites"
        ],
        "answer": [
            "B. a chatbot that provides users with the ability to find answers on a website by themselves",
            "C. telephone voice menus to reduce the load on human resources"
        ],
        "explanation": "Answer: BC. B: A bot is an automated software program designed to perform a particular task. Think of it as a robot without a body. C: Automated customer interaction is essential to a business of any size. In fact, 61% of consumers prefer to communicate via speech, and most of them prefer self-service. Because customer satisfaction is a priority for all businesses, self-service is a critical facet of any customer-facing communications strategy. Incorrect Answers: D: Early bots were comparatively simple, handling repetitive and voluminous tasks with relatively straightforward algorithmic logic. An example would be web crawlers used by search engines to automatically explore and catalog web content. Reference: https://docs.microsoft.com/en-us/azure/architecture/data-guide/big-data/ai-overview https://docs.microsoft.com/en-us/azure/architecture/solution-ideas/articles/interactive-voice-response-bot"
    },
    {
        "id": 144,
        "type": "boolean",
        "question": "For each of the following statements, select Yes if the statement is true. Otherwise, select No.",
        "subQuestions": [
            {
                "prompt": "Azure Bot Service and Azure Congnitive Services can be integrated.",
                "correctAnswer": true
            },
            {
                "prompt": "Azure Bot Service engages with customers in conversational manner.",
                "correctAnswer": true
            },
            {
                "prompt": "Azure Bot Service can import frequently asked questions (FAQ) to question and answer sets.",
                "correctAnswer": false
            }
        ],
        "explanation": "Box 1: Yes - Azure bot service can be integrated with the powerful AI capabilities with Azure Cognitive Services. Box 2: Yes - Azure bot service engages with customers in a conversational manner. Box 3: No - The QnA Maker service creates knowledge base, not question and answers sets. Note: You can use the QnA Maker service and a knowledge base to add question-and-answer support to your bot. When you create your knowledge base, you seed it with questions and answers. Reference: https://docs.microsoft.com/en-us/azure/bot-service/bot-builder-tutorial-add-qna",
        "images": []
    },
    {
        "id": 145,
        "type": "boolean",
        "question": "For each of the following statements, select Yes if the statement is true. Otherwise, select No.",
        "subQuestions": [
            {
                "prompt": "A webchat bot can interact with users visting a website.",
                "correctAnswer": true
            },
            {
                "prompt": "Automatically generating captions for pre-recorded videos is an esample of conversational AI.",
                "correctAnswer": false
            },
            {
                "prompt": "A smart device in the home that responds to questions such as 'What will the weather like today' is an example of conversational AI.",
                "correctAnswer": true
            }
        ],
        "explanation": "1. A webchat bot can interact with users visiting a website. Answer: Yes Webchat bots are designed to engage with website visitors, answer queries, and perform tasks like providing information or collecting user input. Tip: Use webchat bots to improve user experience and provide quick customer support. 2. Automatically generating captions for pre-recorded videos is an example of conversational AI. Answer: No Generating captions involves speech-to-text processing, which is not conversational AI. Conversational AI focuses on interactive communication with users, like chatbots or virtual assistants. Tip: Remember, conversational AI requires interaction or dialogue, not just text generation. 3. A smart device in the home that responds to questions such as What will the weather be like today? is an example of conversational AI. Answer: Yes Smart devices like Alexa or Google Assistant use conversational AI to interpret user queries and provide responses. Tip: Smart assistants are classic examples of conversational AI, as they allow real-time interaction. Reference: https://docs.microsoft.com/en-us/azure/architecture/reference-architectures/ai/conversational-bot https://docs.microsoft.com/en-us/azure/bot-service/bot-builder-webchat-overview?view=azure-bot-service-4.0",
        "images": []
    },
    {
        "id": 146,
        "type": "single_choice",
        "question": "You have a knowledge base of frequently asked questions (FAQ). You create a bot that uses the knowledge base to respond to customer requests. You need to identify what the bot can perform without adding additional skills. What should you identify?",
        "options": [
            "A. Register customer purchases.",
            "B. Register customer complaints.",
            "C. Answer questions from multiple users simultaneously.",
            "D. Provide customers with return materials authorization (RMA) numbers."
        ],
        "answer": "C. Answer questions from multiple users simultaneously.",
        "explanation": "Answer: C. A. Register customer purchases Incorrect: Registering purchases involves transactional actions and requires integrating with a system or database to log the purchases. This is beyond the scope of an FAQ-based bot. B. Register customer complaints Incorrect: Registering complaints may require creating records in a complaint management system, which is not achievable with just an FAQ knowledge base. C. Answer questions from multiple users simultaneously Correct: An FAQ-based bot is designed to provide answers to predefined questions and can handle multiple users simultaneously without requiring additional skills or integrations. D. Provide customers with return materials authorization (RMA) numbers Incorrect: Providing RMA numbers usually involves accessing backend systems to generate or retrieve these numbers, which requires additional skills and integrations. Key Tip: An FAQ-based bot excels in answering predefined questions efficiently for multiple users but does not perform complex tasks like managing transactions or accessing external systems. Reference: https://docs.microsoft.com/en-us/azure/cognitive-services/qnamaker/overview/overview"
    },
    {
        "id": 147,
        "type": "boolean",
        "question": "For each of the following statements, select Yes if the statement is true. Otherwise, select No.",
        "subQuestions": [
            {
                "prompt": "A restaurant can use a chatbot to answer queries through Cortana.",
                "correctAnswer": true
            },
            {
                "prompt": "A restaurent can use a chatbot to answer inquiries about business hours from webpage.",
                "correctAnswer": true
            },
            {
                "prompt": "A restaurant can use a chatbot to automate responses to customer reviews on an external website.",
                "correctAnswer": true
            }
        ],
        "explanation": "Box 1: Yes - You can create and build a cortana bot using microsoft bot framework. Note: Connect Cortana Channels - Login to Azure portal > Select the All Resources > Select Channels > Select Cortana icon. Let us start to configure the Cortana Channel and follow the below steps, at the end of this article you will be able to deploy the Bot into the Cortana. Etc. Box 2: Yes - QnA Maker is an easy-to-use web-based service that makes it easy to power a question-answer application or chatbot from semi-structured content like FAQ documents and product manuals. With QnA Maker, developers can build, train, and publish question and answer bots in minutes. Box 3: Yes -",
        "images": []
    },
    {
        "id": 148,
        "type": "boolean",
        "question": "For each of the following statements, select Yes if the statement is true. Otherwise, select No.",
        "subQuestions": [
            {
                "prompt": "Chatbots can only be built by using custom code.",
                "correctAnswer": false
            },
            {
                "prompt": "The Azure Bot Service provides services that can be used to host conversational bots.",
                "correctAnswer": true
            },
            {
                "prompt": "Bots built by using the Azure Bot Service can communicate with Microsoft Teams users.",
                "correctAnswer": true
            }
        ],
        "explanation": "Box 1: No - Build conversational experiences with Power Virtual Agents and Azure Bot Service Azure Bot Service provides an integrated development environment for bot building. Its integration with Power Virtual Agents, a fully hosted low-code platform, enables developers of all technical abilities build conversational AI bots no code needed. Box 2: Yes - Box 3: Yes - You can configure your bot to communicate with people via Microsoft Teams. Reference: https://azure.microsoft.com/en-us/services/bot-services/#overview https://docs.microsoft.com/en-us/azure/bot-service/channel-connect-teams",
        "images": []
    },
 {
        "id": 149,
        "type": "dropdown",
        "question": "Computer vision capabilities can be deployed to _____________ .",
        "options": [
          { "label": "develop a text-based chatbot for a website", "value": "develop a text-based chatbot for a website" },
          { "label": "identify anomalous customer behavior on an online store", "value":  "identify anomalous customer behavior on an online store" },
          { "label": "integrate a facial recognition feature into an app", "value": "integrate a facial recognition feature into an app" },
	  { "label": "suggest automated responses to incoming email", "value": "suggest automated responses to incoming email" }
        ],
        "correctAnswer": "integrate a facial recognition feature into an app",
        "placeholder": "— Select an option —",
        "explanation": "Azure's Computer Vision service gives you access to advanced algorithms that process images and return information based on the visual features you're interested in. Optical Character Recognition (OCR), Spatial Analysis, Image Analysis The Image Analysis service extracts many visual features from images, such as objects, faces, adult content, and auto-generated text descriptions. Follow the Image Analysis quickstart to get started. Reference: https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision/overview"
            },
    {
        "id": 150,
        "type": "single_choice",
        "question": "You have an Azure Machine Learning pipeline that contains a Split Data module. The Split Data module outputs to a Train Model module and a Score Model module. What is the function of the Split Data module?",
        "options": [
            "A. scaling numeric variables so that they are within a consistent numeric range",
            "B. creating training and validation datasets",
            "C. diverting records that have missing data",
            "D. selecting columns that must be included in the model"
        ],
        "answer": "B. creating training and validation datasets",
        "explanation": "Answer: B. https://learn.microsoft.com/en-us/azure/machine-learning/component-reference/split-data?view=azuremlapi-2 Use the Split Data component to divide a dataset into two distinct sets. This component is useful when you need to separate data into training and testing sets. You can also customize the way that data is divided. Some options support randomization of data. Others are tailored for a certain data type or model type."
    },
    {
        "id": 151,
        "type": "single_choice",
        "question": "Which statement is an example of a Microsoft responsible AI principle?",
        "options": [
            "A. AI systems must use only publicly available data",
            "B. AI systems must be transparent and inclusive",
            "C. AI systems must keep personal details public",
            "D. AI systems must protect the interests of the company"
        ],
        "answer": "B. AI systems must be transparent and inclusive",
        "explanation": "Answer is B. Transparency and inclusiveness are Microsoft responsible AI principles.https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/innovate/best-practices/trustedai#inclusiveness. Inclusiveness mandates that AI should consider all human races and experiences, and inclusive design practices can help developers to understand and address potential barriers that could unintentionally exclude people. Where possible, speech-to-text, text-to-speech, and visual recognition technology should be used to empower people with hearing, visual, and other impairments."
    },
    {
        "id": 153,
"type": "multiple_choice",
        "question": "You need to reduce the load on telephone operators by implementing a chatbot to answer simple questions with predefined answers. Which two AI services should you use to achieve the goal?",
        "options": [
            "A. Azure Machine Learning",
            "B. Azure Bot Service",
            "C. Language Service",
            "D. Translator"
        ],
        "answer": [
            "B. Azure Bot Service",
            "C. Language Service"
        ],
        "explanation": "Answer: BC. B. Azure Bot Service C. Language Service - QnA is replaced by Language Service To implement a chatbot for answering simple questions with predefined answers and reduce the load on telephone operators, you should use: B. Azure Bot Service - Azure Bot Service is a platform for creating and managing chatbots, making it a suitable choice for building a chatbot to handle simple questions. C. Language Service - Language services, including natural language processing capabilities, are essential for understanding user questions and providing relevant responses. This can be integrated into your chatbot to improve its conversational abilities. Azure Machine Learning (A) and Translator (D) are not typically used as primary components for building chatbots for this specific task. While Azure Machine Learning can be used for more complex machine learning scenarios, it's not necessary for simple question and answer chatbots. Azure Translator is primarily used for language translation tasks and doesn't directly address the goal of reducing the load on telephone operators with a chatbot."
    },
    {
        "id": 154,
        "type": "bucket",
        "layout": "bucket",
        "question": "Match the principles of responsible AI to the appropriate descriptions. To answer, drag the appropriate principle from the column on the left to its description on the right. Each principle may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.",
 "options": [
         
          { "id": "Fairness", "label": "Fairness" },
          { "id": "Inclusiveness", "label": "Inclusiveness" },
          { "id": "Privacy and Security", "label": "Privacy and Security" },
          { "id": "Reliability and Safety", "label": "Reliability and Safety" }
        ],
        "buckets": [
          { "id": "AI systems must consistently operate as intended, even under unexpected conditions.", "accepts": ["Reliability and Safety"] },
          { "id": "AI systems must protect and secure personal and business information.", "accepts": ["Privacy and Security"] }
        ],  
               "explanation": "1. Reliability and safety 2. Privacy and security. https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/innovate/best-practices/trustedai#reliability-and-safety AI systems need to be reliable and safe in order to be trusted. It's important for a system to perform as it was originally designed and for it to respond safely to new situations. Its inherent resilience should resist intended or unintended manipulation. Rigorous testing and validation should be established for operating conditions to ensure that the system responds safely to edge cases, and A/B testing and champion/challenger methods should be integrated into the evaluation process."
       
    },
    {
        "id": 155,
        "type": "single_choice",
        "question": "During the process of Machine Learning, when should you review evaluation metrics?",
        "options": [
            "A. Before you train a model.",
            "B. After you clean the data.",
            "C. Before you choose the type of model.",
            "D. After you test a model on the validation data."
        ],
        "answer": "D. After you test a model on the validation data.",
        "explanation": "D is the answer. https://learn.microsoft.com/en-us/training/modules/use-automated-machine-learning/5-machine-learningsteps You can think of the steps in a machine learning process as: - Prepare data: Identify the features and label in a dataset. Pre-process, or clean and transform, the data as needed. - Train model: Split the data into two groups, a training and a validation set. Train a machine learning model using the training data set. Test the machine learning model for performance using the validation data set. - Evaluate performance: Compare how close the model's predictions are to the known labels. - Deploy a predictive service: After you train a machine learning model, you can deploy the model as anapplication on a server or device so that others can use it."
    },
    {
        "id": 156,
        "type": "single_choice",
        "question": "You have a natural language processing (NLP) model that was created by using data obtained without permission. Which Microsoft principle for responsible AI does this breach?",
        "options": [
            "A. reliability and safety",
            "B. privacy and security",
            "C. inclusiveness",
            "D. transparency"
        ],
        "answer": "B. privacy and security",
        "explanation": "Answer B. Privacy and security Using data obtained without permission breaches the Microsoft principle for responsible AI related to (privacy and security). This principle emphasizes the importance of respecting the privacy of individuals and securing their data. Data should be collected and used in a legal and ethical manner, and obtaining data without proper consent or permission can lead to privacy and security violations."
    },

 {
        "id": 157,
        "type": "dropdown",
        "question": "Ensuring an AI system does not provide a prediction when important fields contain unusual or missing values is _____________  principle for responsible AI",
        "options": [
          
          { "label": "an inclusiveness", "value":  "an inclusiveness" },
 	  { "label": "a privacy and security", "value": "a privacy and security" },
	  { "label": "a reliability and safety", "value": "a reliability and safety" },
	  { "label": "a transparency", "value": "a transparency" }
        ],
        "correctAnswer": "a reliability and safety",
        "placeholder": "— Select an option —",
        "explanation": "a reliability and safety https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/innovate/best-practices/trustedai#reliability-and-safety AI systems need to be reliable and safe in order to be trusted. It's important for a system to perform as it was originally designed and for it to respond safely to new situations. Its inherent resilience should resist intended or unintended manipulation. Rigorous testing and validation should be established for operating conditions to ensure that the system responds safely to edge cases, and A/B testing and champion/challenger methods should be integrated into the evaluation process."
       },
    {
        "id": 158,
        "type": "bucket",
        "layout": "bucket",
        "question": "Match the services to the appropriate descriptions. To answer, drag the appropriate service from the column on the left to its description on the right. Each service may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.",       
 "options": [
          { "id": "Azure Storage", "label": "Azure Storage" },
          { "id": "Azure Bot Service", "label": "Azure Bot Service" },
          { "id": "Language Service", "label": "Language Service" },
          { "id":  "Speech", "label":  "Speech" }
        ],
        "buckets": [
          { "id": "Enables the use of natural language to query a knowledge base.", "accepts": ["Language Service"] },
          { "id": "Enables the real-time transcription of speech-to-text.", "accepts": [ "Speech"] }
        ],  
       
        "explanation": "1. Language Service 2. Speech https://learn.microsoft.com/en-us/training/modules/build-faq-chatbot-qna-maker-azure-bot-service/2-getstarted- knowledge-base The Language service includes a custom question answering feature that enables you to create a knowledge base of question and answer pairs that can be queried using natural language input. https://learn.microsoft.com/en-us/azure/cognitive-services/speech-service/overview The Speech service provides speech to text and text to speech capabilities with an Speech resource. You can transcribe speech to text with high accuracy, produce natural-sounding text to speech voices, translate spoken audio, and use speaker recognition during conversations."
    },
    {
        "id": 159,
        "type": "single_choice",
        "question": "Which machine learning technique can be used for anomaly detection?",
        "options": [
            "A. A machine learning technique that classifies objects based on user supplied images.",
            "B. A machine learning technique that understands written and spoken language.",
            "C. A machine learning technique that classifies images based on their contents.",
            "D. A machine learning technique that analyzes data over time and identifies unusual changes."
        ],
        "answer": "D. A machine learning technique that analyzes data over time and identifies unusual changes.",
        "explanation": "D is the answer. https://learn.microsoft.com/en-us/azure/cognitive-services/anomaly-detector/overview Anomaly Detector is an AI service with a set of APIs, which enables you to monitor and detect anomalies in your time series data with little machine learning (ML) knowledge, either batch validation or real-time inference."
    },
    {
        "id": 160,
        "type": "single_choice",
        "question": "You have an AI-based loan approval system. During testing, you discover that the system has a gender bias. Which responsible AI principle does this violate?",
        "options": [
            "A. accountability",
            "B. reliability and safety",
            "C. transparency",
            "D. fairness"
        ],
        "answer": "D. fairness",
        "explanation": "D is the answer. https://learn.microsoft.com/en-us/azure/machine-learning/concept-responsible-ai?view=azureml-api-2#fairness-and-inclusiveness AI systems should treat everyone fairly and avoid affecting similarly situated groups of people in different ways. For example, when AI systems provide guidance on medical treatment, loan applications, or employment, they should make the same recommendations to everyone who has similar symptoms, financial circumstances, or professional qualifications."
    },
    {
        "id": 161,
        "type": "single_choice",
        "question": "You are developing a system to predict the prices of insurance for drivers in the United Kingdom. You need to minimize bias in the system. What should you do?",
        "options": [
            "A. Remove information about protected characteristics from the data before sampling.",
            "B. Take a training sample that is representative of the population in the United Kingdom.",
            "C. Create a training dataset that uses data from global insurers.",
            "D. Take a completely random training sample."
        ],
        "answer": "B. Take a training sample that is representative of the population in the United Kingdom.",
        "explanation": "Answer: B. To minimize bias in the system, it's important that your training data is representative of the population you're modeling. This helps ensure that the model's predictions are valid for the full range of drivers in the United Kingdom. While option A (Remove information about protected characteristics from the data before sampling) could help in some cases to reduce direct discrimination, it might not be sufficient to minimize all types of biases, as some of these characteristics might be indirectly encoded in the remaining features. Option C (Create a training dataset that uses data from global insurers) may introduce more bias since driving conditions, laws, and demographics vary greatly by country. Option D (Take a completely random training sample) could still introduce bias if the original data pool is not representative of the population you're interested in."
    },

 {
        "id": 162,
        "type": "dropdown",
        "question": "Azure Machine learning designer lets you create machine learning models by _____________ .",
        "options": [
          { "label": "adding and connecting modules on a visual canvas", "value": "adding and connecting modules on a visual canvas" },
          { "label": "automatically performing common data preparation tasks", "value":  "automatically performing common data preparation tasks" },
          { "label": "automatically selecting an algorithm to build the most accurate model", "value": "automatically selecting an algorithm to build the most accurate model" },
	  { "label": "using a code-first notebook experience", "value": "using a code-first notebook experience" }
        ],
        "correctAnswer": "adding and connecting modules on a visual canvas",
        "placeholder": "— Select an option —",
        "explanation": "Answer: adding and connecting modules on a visual canvas. https://learn.microsoft.com/en-us/azure/machine-learning/concept-designer?view=azureml-api-2 Azure Machine Learning designer is a drag-and-drop UI interface to build pipeline in Azure Machine Learning."
            },
    {
        "id": 163,
        "type": "single_choice",
        "question": "You have a dataset. You need to build an Azure Machine Learning classification model that will identify defective products. What should you do first?",
        "options": [
            "A. Load the dataset.",
            "B. Create a clustering model.",
            "C. Split the data into training and testing datasets.",
            "D. Create a classification model."
        ],
        "answer": "A. Load the dataset.",
        "explanation": "Answer A. First at all you need to load the dataset When building a machine learning model, the process typically follows a sequence of steps. The first step is to load the dataset so you can perform subsequent tasks like preprocessing, splitting, and model training. Without loading the data, you cannot proceed with the remaining steps. Why the other options are incorrect: B. Create a clustering model: Incorrect because clustering is not relevant here. The goal is classification, not clustering. C. Split the data into training and testing datasets: This step comes after loading the dataset. Splitting cannot occur before the data is loaded. D. Create a classification model: Creating the classification model is one of the later steps after loading and preparing the data. Key Tip: Always load and inspect your dataset as the first step in any machine learning workflow. This ensures the data is accessible and ready for preprocessing and analysis."
    },
    {
        "id": 164,
        "type": "single_choice",
        "question": "You use Azure Machine Learning designer to build a model pipeline. What should you create before you can run the pipeline?",
        "options": [
            "A. a registered model",
            "B. a compute resource",
            "C. a Jupyter notebook"
        ],
        "answer": "B. a compute resource",
        "explanation": "Answer is B. In Azure Machine Learning Designer, a compute resource is required to execute a pipeline. The compute resource provides the necessary processing power to run the pipeline steps, such as training models, performing data processing, or evaluating results. Why the other options are incorrect: A. A registered model Incorrect: A registered model is created after training and registering the model in Azure Machine Learning. It is not a prerequisite for running the pipeline. C. A Jupyter notebook Incorrect: Jupyter notebooks are used for coding and experimentation, but they are not necessary for running pipelines in the Azure Machine Learning designer. Key Tip: Always ensure you have a compute resource (like an Azure ML Compute Instance or Compute Cluster) configured and available before running pipelines in Azure Machine Learning Designer."
    },
    {
        "id": 165,
        "type": "bucket",
        "layout": "bucket",
        "question": "Match the tool to the Azure Machine Learning task. To answer, drag the appropriate tool from the column on the left to its tasks on the right. Each tool may be used once, more than once, or not at all.",  


 "options": [
          { "id": "The Azure portal", "label": "The Azure portal" },
          { "id": "Machine Learning designer", "label": "Machine Learning designer" },
          { "id": "Automated Machine learning (ML)", "label": "Automated Machine learning (ML)" }
        ],
        "buckets": [
          { "id": "Create a Machine Learning workspace.", "accepts": ["The Azure portal"] },
          { "id": "Use a drag-and-drop interface used to train and deploy models.", "accepts": ["Machine Learning designer"] },
          { "id": "Use a wizard to select configuration for a machine learning run.", "accepts": ["Automated Machine learning (ML)"] }
        ],  
       
        "explanation": "1. Azure portal 2. ML designer 3. Automated ML https://learn.microsoft.com/en-us/azure/machine-learning/quickstart-create-resources?view=azureml-api-2 https://learn.microsoft.com/en-us/azure/machine-learning/concept-designer?view=azureml-api-2 Azure Machine Learning designer is a drag-and-drop UI interface to build pipeline in Azure Machine Learning."
        
    },
    {
        "id": 166,
        "type": "single_choice",
        "question": "You need to create a customer support solution to help customers access information. The solution must support email, phone, and live chat channels. Which type of AI solution should you use?",
        "options": [
            "A. machine learning",
            "B. computer vision",
            "C. chatbot",
            "D. natural language processing (NLP)"
        ],
        "answer": "C. chatbot",
        "explanation": "Answer: C. A chatbot is the most suitable AI solution for providing a customer support solution across multiple channels such as email, phone, and live chat. Chatbots can integrate with different communication platforms and provide automated responses to customer queries using pre-defined logic or AI capabilities. Why the other options are incorrect: A. Machine learning: Incorrect: While machine learning can be a component of a chatbot, it is not a direct solution for customer support. ML is a general technique for building predictive models and patterns in data. B. Computer vision: Incorrect: Computer vision is used for image and video analysis, such as object detection or image recognition, and is not relevant for a text- or voice-based customer support solution. D. Natural Language Processing (NLP): Incorrect: NLP is a technique used by chatbots to understand and process human language. However, it is not the complete solution but rather a core component of a chatbot. Key Tip: Use chatbots for multi-channel customer support solutions and enhance them with NLP for better understanding of customer queries."
    },
    {
        "id": 167,
        "type": "bucket",
        "layout": "bucket",
        "question": "Match the types of AI workloads to the appropriate scenarios. To answer, drag the appropriate workload type from top to its scenario on the bottom. Each workload type may be used once, more than once, or not at all.",

 "options": [
          { "id": "Anomaly detection", "label": "Anomaly detection" },
          { "id": "Computer vision", "label": "Computer vision" },
          { "id": "Machine learning (Clustering)", "label": "Machine learning (Clustering)"},
          { "id":  "Natural language processing", "label":  "Natural language processing" }
        ],
        "buckets": [
          { "id": "Identify handwritten letters.", "accepts": ["Computer vision"] },
          { "id": "Predict a sentiment of a social media post.", "accepts": ["Natural language processing"] },
          { "id": "Identify unsual credit card payement.", "accepts": ["Anomaly detection"] },
          { "id": "Group animals based on multiple measurements.", "accepts": ["Machine learning (Clustering)"] }
        ],  
        
        "explanation": "1- Computer vision 2- Natural language processing 3- Anomaly detection 4- Machine learning (Clustering) https://learn.microsoft.com/en-us/azure/cognitive-services/computer-vision/overview-ocr OCR or Optical Character Recognition is also referred to as text recognition or text extraction. Machinelearning based OCR techniques allow you to extract printed or handwritten text from images, such as posters, street signs and product labels, as well as from documents like articles, reports, forms, and invoices. The text is typically extracted as words, text lines, and paragraphs or text blocks, enabling access to digital version of the scanned text. This eliminates or significantly reduces the need for manual data entry."
    },
    {
        "id": 168,
        "type": "single_choice",
        "question": "Predicting how many vehicles will travel across a bridge on a given day is an example of.",
        "options": [
            "A. regression",
            "B. translation",
            "C. classification",
            "D. clustering"
        ],
        "answer": "A. regression",
        "explanation": "Answer: A. Predicting how many vehicles will travel across a bridge involves forecasting a numerical value, which is the hallmark of a regression problem. Regression is used in machine learning to predict continuous outcomes, such as sales, temperatures, or traffic volume. Why the other options are incorrect: B. Translation: Incorrect: Translation is a Natural Language Processing (NLP) task that converts text from one language to another. It is unrelated to predicting numerical values. C. Classification: Incorrect: Classification is used for predicting discrete categories (e.g., defective vs. non-defective or yes vs. no). It does not deal with continuous numerical outputs. D. Clustering: Incorrect: Clustering groups data into clusters based on similarities but does not predict numerical values. It is used for exploratory data analysis, not forecasting. Key Tip: When predicting a numerical value, always consider regression methods such as linear regression, decision trees, or neural networks designed for regression tasks."
    },
    {
        "id": 169,
        "type": "single_choice",
        "question": "In a machine learning model, the data that is used as inputs are called ______.",
        "options": [
            "A. dataset",
            "B. labels",
            "C. variables"
        ],
        "answer": "C. variables",
        "explanation": "Answer: C. Labels is indeed the output. Would prefer to say features as well, but features and variables refer to the same thing in the context of machine learning, which is the input data used to train the model. A dataset is a collection of data points, each of which contains one or more features. So would go for C.In a machine learning model, the input data that is used to predict the output is referred to as variables (or features). These are the independent variables or predictors that the model uses to learn patterns and make predictions. Why the other options are incorrect: A. Dataset: Incorrect: A dataset refers to the entire collection of data, including both input variables and target labels. It is not just the inputs but the whole dataset. B. Labels: Incorrect: Labels are the output or target values in supervised learning, representing the result the model is trying to predict. They are not the inputs. Key Tip: In machine learning, variables (or features) are the attributes or columns in your dataset that are used to make predictions. These are the inputs that influence the model's output"
    },

 {
        "id": 170,
        "type": "dropdown",
        "question": "Using Recency, Frequency, and Monetary (RFM) values to identify segments of a customer base is an example of  _____________ .",
        "options": [
          { "label": "Clustering", "value": "Clustering" },
          { "label": "Regression", "value":  "Regression" },
          { "label": "Classification", "value": "Classification" },
	  { "label": "Regularization", "value": "Regularization" }
        ],
        "correctAnswer": "Clustering",
        "placeholder": "— Select an option —",
        "explanation": "Answer: Clustering. RFM analysis does not involve training a machine learning model to predict a specific outcome, it does involve clustering data based on specific criteria, which makes it more of a form of clustering rather than classification. So would go for clustering."
       
    },
    {
        "id": 171,
        "type": "bucket",
        "layout": "bucket",
        "question": "You plan to deploy an Azure Machine Learning model by using the Machine Learning designer. Which four actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order",
   
 "options": [
          { "id": "Train the model", "label": "Train the model" },
          { "id": "Split the data randomly into training data and validation data", "label": "Split the data randomly into training data and validation data" },
          { "id": "Evaluate the model against the orginal dataset", "label": "Evaluate the model against the orginal dataset"},
          { "id": "Evaluate the model against the validation dataset", "label": "Evaluate the model against the validation dataset" },
          { "id": "ingest and prepare a dataset", "label": "ingest and prepare a dataset" }
        ],
        "buckets": [
          { "id": "Task Number 1.", "accepts": ["ingest and prepare a dataset"] },
          { "id": "Task Number 2", "accepts": ["Split the data randomly into training data and validation data"] },
          { "id": "Task Number 3.", "accepts": ["Train the model"] },
          { "id": "Task Number 4", "accepts": ["Evaluate the model against the validation dataset"] }
        ],  
        "explanation": "Box-1 Ingest and prepare dataset. Box-2 Split data randomly into training and validation data. Box-3 Train model. Box 4- Evaluate model against validation dataset. https://learn.microsoft.com/en-us/training/modules/use-automated-machine-learning/5-machine-learningsteps"
       
    },
    {
        "id": 172,
        "type": "boolean",
        "question": "For each of the following statements, select Yes if the statement is true. Otherwise, select No.",
        "subQuestions": [
            {
                "prompt": "Organizing documents into groups based on different usage statistics is an example of clustering .",
                "correctAnswer": true
            },
            {
                "prompt": "Grouping similar patients based on symptoms and diagnostic test results is an example of clustering.",
                "correctAnswer": true
            },
            {
                "prompt": "Predicting whether a person will develop mild, moderate or severe allergy symptoms based on pollen count is an example of clustering",
                "correctAnswer": false
            }
        ],
        "explanation": "Yes, Yes, No is the answer. https://learn.microsoft.com/en-us/training/modules/create-clustering-model-azure-machine-learningdesigner/ 2-clustering-scenarios Clustering is a form of machine learning that is used to group similar items into clusters based on their features. For example, a researcher might take measurements of penguins, and group them based on similarities in their proportions. https://learn.microsoft.com/en-us/training/modules/create-classification-model-azure-machine-learningdesigner/ classification-scenarios Classification is a form of machine learning that is used to predict which category, or class, an item belongs to. This machine learning technique can be applied to binary and multi-class scenarios. For example, a health clinic might use the characteristics of a patient (such as age, weight, blood pressure, and so on) to predict whether the patient is at risk of diabetes. In this case, the characteristics of the patient are the features, and the label is a binary classification of either 0 or 1, representing non-diabetic or diabetic.",
        "images": []
    },

 {
        "id": 173,
        "type": "dropdown",
        "question": "Regression is a form of machine learning used to predict a  _____________  label or outcome based on variables or features.",
        "options": [
          { "label": "Boolean", "value": "Boolean" },
          { "label": "Datetime", "value":  "Datetime" },
          { "label": "Numeric", "value": "Numeric" },
	  { "label": "Text", "value": "Text" }
        ],
        "correctAnswer": "Numeric",
        "placeholder": "— Select an option —",
        "explanation": "Answer: Numeric. https://learn.microsoft.com/en-us/training/modules/create-regression-model-azure-machine-learningdesigner/ 2-regression-scenarios Regression is a form of machine learning used to understand the relationships between variables to predict a desired outcome. Regression predicts a numeric label or outcome based on variables, or features. For example, an automobile sales company might use the characteristics of a car (such as engine size, number of seats, mileage, and so on) to predict its likely selling price. In this case, the characteristics of the car are the features, and the selling price is the label"
       },
    {
        "id": 174,
        "type": "single_choice",
        "question": "You need to create a clustering model and evaluate the model by using Azure Machine Learning designer. What should you do?",
        "options": [
            "A. Split the original dataset into a dataset for training and a dataset for testing. Use the testing dataset for evaluation.",
            "B. Use the original dataset for training and evaluation.",
            "C. Split the original dataset into a dataset for features and a dataset for labels. Use the features dataset for evaluation.",
            "D. Split the original dataset into a dataset for training and a dataset for testing. Use the training dataset for evaluation."
        ],
        "answer": "A. Split the original dataset into a dataset for training and a dataset for testing. Use the testing dataset for evaluation.",
        "explanation": "Answer: A. Split the original dataset into a dataset for training and a dataset for testing. Use the testing dataset for evaluation. In machine learning, it's essential to evaluate your model using data it hasn't seen during training to assess its performance accurately. This approach helps in understanding how the model generalizes to new, unseen data. Correct Answer: A. Training Dataset: Used to train the model, allowing it to learn patterns and relationships within the data. Testing Dataset: Used to evaluate the model's performance after training. This dataset should not overlap with the training data to ensure an unbiased assessment of the model's generalization capability. Why Not the Other Options? B. Use the original dataset for training and evaluation: Using the same dataset for both training and evaluation can lead to overfitting, where the model performs well on the training data but poorly on new, unseen data. C. Split the original dataset into a dataset for features and a dataset for labels. Use the features dataset for evaluation: This approach doesn't separate the data into training and testing sets, which is crucial for unbiased evaluation. D. Split the original dataset into a dataset for training and a dataset for testing. Use the training dataset for evaluation: Evaluating the model on the training dataset can result in overly optimistic performance metrics and doesn't provide insight into how the model will perform on new data. Key Tip: Always use a separate testing dataset to evaluate your model's performance to ensure it generalizes well to new, unseen data."
    },
    {
        "id": 175,
        "type": "single_choice",
        "question": "You have a dataset that contains the columns shown in the following table. You have a machine learning model that predicts the value of ColumnE based on the other numeric columns. Which type of model is this?",
        "options": [
            "A. Analysis",
            "B. Clustering",
            "C. Regression"
        ],
        "answer": "C. Regression",
        "images": [
            "https://bongawat.wordpress.com/wp-content/uploads/2026/01/ai900_q174.png?w=NaN&h="
        ],
        "explanation": "Answer: C. https://learn.microsoft.com/en-us/training/modules/create-regression-model-azure-machine-learningdesigner/2-regression-scenarios Regression is a form of machine learning used to understand the relationships between variables to predict a desired outcome. Regression predicts a numeric label or outcome based on variables, or features. For example, an automobile sales company might use the characteristics of a car (such as engine size, number of seats, mileage, and so on) to predict its likely selling price. In this case, the characteristics of the car are the features, and the selling price is the label."
    },
    {
        "id": 176,
        "type": "single_choice",
        "question": "You need to track multiple versions of a model that was trained by using Azure Machine Learning. What should you do?",
        "options": [
            "A. Explain the model.",
            "B. Register the model.",
            "C. Register the training data.",
            "D. Provision an inference cluster."
        ],
        "answer": "B. Register the model.",
        "explanation": "Answer: B. Register the model. To effectively track multiple versions of a model trained using Azure Machine Learning, you should register the model. Model registration allows you to store and version your models within your Azure Machine Learning workspace, facilitating organized management and easy retrieval of different model versions. Correct Answer: B. A. Explain the model: This involves interpreting the model's decisions and is not related to version tracking. B. Register the model: Correct. Registering the model enables version control and management within Azure Machine Learning. C. Register the training data: While registering training data is important for reproducibility, it doesn't directly facilitate version tracking of the model itself. D. Provision an inference cluster: This step is related to deploying the model for inference and doesn't pertain to version tracking. Key Tip: Always register your models in Azure Machine Learning to maintain a clear version history, ensuring reproducibility and effective model management."
    },
    {
        "id": 177,
        "type": "single_choice",
        "question": "You need to identify groups of rows with similar numeric values in a dataset. Which type of machine learning should you use?",
        "options": [
            "A. Clustering",
            "B. Regression",
            "C. Classification"
        ],
        "answer": "A. Clustering",
        "explanation": "Answer: A. Clustering is a type of unsupervised learning used to group rows of data with similar patterns or numeric values without needing predefined labels. It identifies natural groupings or patterns in the dataset. Why the other options are incorrect: B. Regression Regression is used for predicting a continuous numeric value based on input features. For example, predicting house prices or temperature. It’s not suitable for identifying groups of similar rows, as it focuses on finding a relationship between variables, not grouping. C. Classification. Classification is a supervised learning method used to assign predefined categories or labels to data points. For example, classifying emails as spam or not spam. It requires labeled data, so it’s not applicable when  grouping similar rows without prior labels."
    },

 {
        "id": 178,
        "type": "dropdown",
        "question": "A banking system that predicts whether a loan will be repaid is an example of the _____________  type of machine learning.",
        "options": [
          { "label": "clustering", "value": "clustering" },
          { "label": "regression", "value":  "regression" },
	  { "label": "classification", "value": "classification" }
        ],
        "correctAnswer": "classification",
        "placeholder": "— Select an option —",
        "explanation": "Answer: Classification, because there would be an answer like yes or no and not a value like $10.000 https://learn.microsoft.com/en-us/training/modules/create-classification-model-azure-machine-learningdesigner/classification-scenarios Classification is a form of machine learning that is used to predict which category, or class, an item belongs to. This machine learning technique can be applied to binary and multi-class scenarios. For example, a health clinic might use the characteristics of a patient (such as age, weight, blood pressure, and so on) to predict whether the patient is at risk of diabetes. In this case, the characteristics of the patient are the features, and the label is a binary classification of either 0 or 1, representing non-diabetic or diabetic."
       },

 {
        "id": 179,
        "type": "dropdown",
        "question": " _____________   models can be used to predict the sale price of auctioned items.",
        "options": [
          { "label": "Classification", "value": "Classification" },
          { "label": "Clustering", "value":  "Clustering" },
	  { "label":  "Regression", "value":  "Regression" }
        ],
        "correctAnswer":  "Regression",
        "placeholder": "— Select an option —",
        "explanation": "Answer:Regression. https://learn.microsoft.com/en-us/training/modules/create-regression-model-azure-machine-learningdesigner/2-regression-scenarios Regression is a form of machine learning used to understand the relationships between variables to predict a desired outcome. Regression predicts a numeric label or outcome based on variables, or features. For example, an automobile sales company might use the characteristics of a car (such as engine size, number ofseats, mileage, and so on) to predict its likely selling price. In this case, the characteristics of the car are the features, and the selling price is the label."
           },
    {
        "id": 180,
        "type": "single_choice",
        "question": "A historian can use ________ to digitize newspaper articles.",
        "options": [
            "A. Object detection",
            "B. Facial recognition",
            "C. Image classification",
            "D. Optical character recognition (OCR)"
        ],
        "answer": "D. Optical character recognition (OCR)",
        "explanation": "Answer: D. Optical Character Recognition (OCR) is the technology used to digitize printed or handwritten text, such as newspaper articles. It converts images of text into machine-readable text, making it searchable and editable. Why the other options are incorrect: A. Object detection Object detection identifies and locates objects (like cars, faces, or animals) in an image but doesn’t focus on extracting text.B. Facial recognition Facial recognition is specifically designed to identify or verify people’s faces, which is unrelated to digitizing text. C. Image classification Image classification categorizes entire images into predefined labels (e.g., cat or dog) but doesn’t extract or digitize text from images. Key Tip: Use OCR when you need to extract and digitize text from images, such as newspapers, books, or handwritten documents."
    },
    {
        "id": 181,
        "type": "boolean",
        "question": "For each of the following statements, select Yes if the statement is true. Otherwise, select No.",
        "subQuestions": [
            {
                "prompt": "Object detection can identify the location of a damaged product in an image.",
                "correctAnswer": true
            },
            {
                "prompt": "Object detection can identify multiple instances of a damaged product in an image.",
                "correctAnswer": true
            },
            {
                "prompt": "Object detection can identify multiple types of damaged products in an image.",
                "correctAnswer": true
            }
        ],
        "explanation": "Yes, Yes, Yes. Here are the statements with their corresponding answers: Object detection can identify the location of a damaged product in an image. True: Object detection can be used to locate and outline the position of a specific object or region in an image, including a damaged product. Object detection can identify multiple instances of a damaged product in an image.True: Object detection is capable of identifying and locating multiple instances of a specified object, such as multiple damaged products in an image. Object detection can identify multiple types of damaged products in an image. True: Object detection can identify and classify different types of objects, including multiple types of damaged products in an image, if it has been trained to do so.",
        "images": []
    },
    {
        "id": 182,
        "type": "single_choice",
        "question": "You need to create a model that labels a collection of your personal digital photographs. Which Azure Cognitive Services service should you use?",
        "options": [
            "A. Form Recognizer",
            "B. Custom Vision",
            "C. Language",
            "D. Computer Vision"
        ],
        "answer": "B. Custom Vision",
        "explanation": "Correct Answer: B. With Custom Vision, users can upload their own images and labels, and train a model to recognize specific objects or patterns Custom Vision is the correct choice because it allows you to train a custom model tailored to label or classify your personal digital photographs. You can define your own categories (labels) and train the model using your specific dataset. Why the other options are incorrect: A. Form Recognizer Form Recognizer extracts data from structured documents like forms and invoices. It’s not used for labeling photographs. C. Language The Language service is designed for natural language processing tasks, such as text analysis or language understanding, not image labeling. D. Computer Vision Computer Vision provides general pre-built image analysis capabilities (e.g., detecting objects or describing images), but it doesn’t let you create a custom model tailored to your specific labels. Key Tip: Use Custom Vision when you need to create a tailored image classification model for your unique dataset. Use Computer Vision for general-purpose image analysis."
    },

 {
        "id": 183,
        "type": "dropdown",
        "question": " _____________  is used to identify multiple types of items in one image.",
        "options": [
          { "label": "Image Classification", "value": "Image Classification" },
          { "label": "Image Description", "value":  "Image Description" },
          { "label":  "Object Detection", "value":  "Object Detection" },
	  { "label": "Object Character Recognition (OCR)", "value": "Object Character Recognition (OCR)" }
        ],
        "correctAnswer":  "Object Detection",
        "placeholder": "— Select an option —",
        "explanation": "Object detection https://learn.microsoft.com/en-us/azure/cognitive-services/computer-vision/concept-object-detection Object detection is similar to tagging, but the API returns the bounding box coordinates (in pixels) for each object found in the image. For example, if an image contains a dog, cat and person, the Detect operation will list those objects with their coordinates in the image. You can use this functionality to process the relationships between the objects in an image. It also lets you determine whether there are multiple instancesof the same object in an image."
           },

 {
        "id": 184,
        "type": "dropdown",
        "question": "Identifying whether a kiosk user is annoyed by monitoring a video feed from the kiosk is an example of _____________ .",
        "options": [
          { "label": "Face detection", "value": "Face detection" },
          { "label": "Facial Analysis", "value":  "Facial Analysis" },
          { "label": "Facial Recognition", "value": "Facial Recognition" },
	  { "label":  "Optical Character Recognition (OCR)", "value": "Optical Character Recognition (OCR)"}
        ],
        "correctAnswer": "Facial Analysis",
        "placeholder": "— Select an option —",
        "explanation": "Microsoft has retired facial recognition capabilities that can be used to try to infer emotional states and identity attributes which, if misused, can subject people to stereotyping, discrimination or unfair denial of services. These include capabilities that predict emotion, gender, age, smile, facial hair, hair and makeup.Read more about this decision here. https://azure.microsoft.com/en-us/blog/responsible-ai-investments-and-safeguards-for-facial-recognition/"
            },
    {
        "id": 185,
        "type": "bucket",
        "layout": "bucket",
        "question": "Match the Azure Cognitive Services to the appropriate actions. To answer, drag the appropriate service from the column on the left to its action on the right. Each service may be used once, more than once, or not at all.",
     
 "options": [
          { "id":  "Custom Vision", "label":  "Custom Vision" },
          { "id":  "Face", "label":  "Face" },
          { "id": "Form Recognizer", "label": "Form Recognizer" }
        ],
        "buckets": [
          { "id": "Identify Objects in an image.", "accepts": [ "Custom Vision"] },
          { "id": "Automatically import data from an invoice to a database.", "accepts": ["Form Recognizer"] },
          { "id": "Identify people in an image.", "accepts": [ "Face"] }
        ],  
       
        "explanation": "Answer: 1. Custom Vision 2. Form Recognized 3. Face https://learn.microsoft.com/en-us/azure/cognitive-services/custom-vision-service/overview Azure Custom Vision is an image recognition service that lets you build, deploy, and improve your own image identifier models. An image identifier applies labels to images, according to their visual characteristics. Each label represents a classification or object. Unlike the Computer Vision service, Custom Vision allows you to specify your own labels and train custom models to detect them."
    },
 {
        "id": 186,
        "type": "dropdown",
        "question": "An AI solution that helps photographers take better portrait phtographs by providing feedback on exposure, noise, and occlusion is an example of Facial _____________ .",
        "options": [
          { "label": "Analysis", "value": "Analysis" },
          { "label": "Detection", "value":  "Detection" },
	  { "label": "Recognition", "value": "Recognition" }
        ],
        "correctAnswer": "Analysis",
        "placeholder": "— Select an option —",
        "explanation": "Answer : Analysis. An AI solution that helps photographers to take better pictures by providing feedback on exposure, noise, and occlusion is an example of facial analysis or photo analysis. In this context, the AI system is analyzing the photo quality and characteristics, such as exposure (brightness and contrast), noise (graininess or pixel-level disturbances), and occlusion (obstructions or unwanted objects). The AI's analysis can then provide feedback to the photographer to improve the composition and settings, resulting in better photographs. While this specific example focuses on photo quality aspects, facial analysis may also involve analyzing and detecting faces within images, recognizing facial expressions, emotions, or attributes, but that doesn't seem to be the focus in this particular case."
       },
    {
        "id": 187,
        "type": "single_choice",
        "question": "Your company manufactures widgets. You have 1,000 digital photos of the widgets. You need to identify the location of the widgets within the photos. What should you use?",
        "options": [
            "A. Computer Vision Spatial Analysis",
            "B. Custom Vision object detection",
            "C. Computer Vision Image Analysis",
            "D. Custom Vision classification"
        ],
        "answer": "B. Custom Vision object detection",
        "explanation": "Custom Vision Object Detection is the best choice because it can locate and identify specific objects (like widgets) within images by drawing bounding boxes around them. This helps pinpoint their location in thephotos. Why the other options are incorrect: A. Computer Vision Spatial Analysis Spatial analysis is typically used for analyzing movement and presence in physical spaces (e.g., people counting in a room). It’s not meant for identifying specific objects in images. C. Computer Vision Image Analysis Image analysis provides a general description of the image (e.g., This image contains a car and a tree) but does not identify the exact location of objects. D. Custom Vision Classification Classification assigns labels to entire images (e.g., This is an image of a widget) but does not locate objectsor provide bounding boxes.Key Tip: Use Object Detection when you need to identify and locate objects in images. Use Classification for labeling entire images without location details."
    },
    {
        "id": 188,
        "type": "single_choice",
        "question": "You need to convert handwritten notes into digital text. Which type of computer vision should you use?",
        "options": [
            "A. Facial detection",
            "B. Optical character recognition (OCR)",
            "C. Image classification",
            "D. Object detection"
        ],
        "answer": "B. Optical character recognition (OCR)",
        "explanation": "Optical Character Recognition (OCR) is the correct choice because it is designed to convert handwritten or printed text into digital, machine-readable text. It’s widely used for digitizing documents, notes, and forms. Why the other options are incorrect: A. Facial detection Facial detection identifies and locates human faces in images but does not process or convert text. C. Image classification Image classification assigns labels to entire images based on their content (e.g., dog or cat) but does not recognize or extract text. D. Object detection Object detection identifies and locates objects within images (e.g., a car or a chair) but is not suitable for text recognition. Key Tip: Use OCR for tasks involving text extraction from handwritten or printed sources, such as notes, books, or forms."
    },
    {
        "id": 190,
        "type": "single_choice",
        "question": "You need to develop a mobile app for employees to scan and store their expenses while travelling. Which type of computer vision should you use?",
        "options": [
            "A. Face detection",
            "B. Image classification",
            "C. Object detection",
            "D. Optical character recognition (OCR)"
        ],
        "answer": "D. Optical character recognition (OCR)",
        "explanation": "Optical Character Recognition (OCR) is the correct choice because it can scan and extract text from receipts or invoices (e.g., amounts, dates, item descriptions) and convert it into digital, machine-readable text to store in the app. Why the other options are incorrect: A. Face detection Face detection identifies and locates human faces in images but is unrelated to scanning expenses or extracting text. B. Image classification Image classification categorizes entire images (e.g., receipt or not a receipt) but does not extract or digitize the text from the receipt. C. Object detection Object detection identifies and locates objects in an image (e.g., a car or a phone) but cannot recognize or extract text from receipts. Key Tip: Use OCR whenever you need to scan, extract, and digitize text from physical documents like receipts, invoices, or forms."
    },

 {
        "id": 191,
        "type": "dropdown",
        "question": " _____________  service to train an object detection model by using your own images.",
        "options": [
          { "label": "Computer Vision", "value": "Computer Vision" },
          { "label": "Custom Vision", "value":  "Custom Vision" },
          { "label": "Form Recognizer", "value": "Form Recognizer" },
	  { "label":  "Azure Video Analyzer for Media", "value":  "Azure Video Analyzer for Media" }
        ],
        "correctAnswer": "Custom Vision",
        "placeholder": "— Select an option —",
        "explanation": "Right answer is Custom vision."
         },
    {
        "id": 194,
        "type": "single_choice",
        "question": "You need to implement a pre-built solution that will identify well-known brands in digital photographs. Which Azure Cognitive Services service should you use?",
        "options": [
            "A. Custom Vision",
            "B. Form Recognizer",
            "C. Face",
            "D. Computer Vision"
        ],
        "answer": "D. Computer Vision",
        "explanation": "Computer Vision is the correct choice because it provides pre-built functionality to analyze images and identify well-known brands, logos, or objects in digital photographs. It includes capabilities like image tagging, brand detection, and general image analysis. Why the other options are incorrect: A. Custom Vision Custom Vision is used for creating custom models to classify or detect objects specific to your needs. It requires training with your dataset and is not pre-built for brand recognition. B. Form Recognizer Form Recognizer is designed to extract data from structured documents like forms or receipts, but it does not analyze photographs or identify brands. C. Face The Face service is specifically for detecting and analyzing human faces, such as identifying facial features or verifying identities. It cannot recognize brands or logos. Key Tip: Use Computer Vision for pre-built solutions like brand/logo recognition, image tagging, and content description without the need for custom training."
    },
    {
        "id": 195,
        "type": "single_choice",
        "question": "Natural language processing can be used to ________.",
        "options": [
            "A. Analyze video content",
            "B. Generate speech",
            "C. Classify email messages as work-related or personal.",
            "D. Classify images"
        ],
        "answer": "C. Classify email messages as work-related or personal.",
        "explanation": "Answer C. Classify email messages as work-related or personal. Natural Language Processing (NLP) is a field of AI that enables machines to understand, interpret, and process human language. It can classify email messages based on their content (e.g., work-related or personal), making C the correct choice. Why the other options are incorrect: A. Analyze video content Analyzing video content falls under computer vision, not NLP, as it deals with visual data rather than text or language. B. Generate speechGenerating speech is related to speech synthesis (text-to-speech), which is a different domain of AI focused on converting text to audio, not text analysis. D. Classify images Classifying images is a task for computer vision, which analyzes visual data, not language or text. Key Tip: Use NLP for tasks involving language processing, such as text classification, sentiment analysis, and language translation. For images or video, rely on computer vision techniques."
    },
    {
        "id": 196,
        "type": "multiple_choice",
        "question": "You plan to develop a bot that will enable users to query a knowledge base by using natural language processing. Which two services should you include in the solution?",
        "options": [
            "A. Language Service",
            "B. Azure Bot Service",
            "C. Form Recognizer",
            "D. Anomaly Detector"
        ],
        "answer": [
            "A. Language Service",
            "B. Azure Bot Service"
        ],
        "explanation": "Answer A,B. To develop a bot that enables users to query a knowledge base using natural language processing, you need the following services: A. Language Service The Language Service (formerly known as Text Analytics or QnA Maker) processes and understands natural language queries. It enables querying a knowledge base by interpreting user input in natural language. B. Azure Bot Service Azure Bot Service provides the framework and tools to build, deploy, and manage the bot itself, allowing users to interact with the bot via chat or other interfaces. Why the other options are incorrect: C. Form Recognizer Form Recognizer is designed to extract information from structured documents like forms or invoices, not to process natural language queries or enable bot interactions. D. Anomaly Detector Anomaly Detector is used for identifying anomalies in time-series data, which is unrelated to bots or natural language processing. Key Tip: Use Language Service for natural language understanding and Azure Bot Service for building conversational bots. These two services work together to create intelligent, user-friendly bot solutions."
    },
    {
        "id": 197,
        "type": "boolean",
        "question": "For each of the following statements, select Yes if the statement is true. Otherwise, select No.",
        "subQuestions": [
            {
                "prompt": "The following service call will accept English text as an input an output Italian and French text. /translate?from=it&to=Fr&to=en.",
                "correctAnswer": false
            },
            {
                "prompt": "The following service call will accept English text as an input an output Italian and French text. /translate?from=en&to=Fr&to=it.",
                "correctAnswer": true
            },
            {
                "prompt": "The translator service can be used to translate documents from English to French.",
                "correctAnswer": true
            }
        ],
        "explanation": "No, Yes, Yes is the answer. https://learn.microsoft.com/en-us/azure/cognitive-services/translator/document-translation/overview Document Translation is a cloud-based feature of the Azure Translator service and is part of the Azure Cognitive Service family of REST APIs. The Document Translation API can be used to translate multiple and complex documents across all supported languages and dialects, while preserving original document structure and data format.",
        "images": []
    },
    {
        "id": 198,
        "type": "single_choice",
        "question": "An app that analyzes social media posts to identify their tone is an example of which type of natural language processing (NLP) workload?",
        "options": [
            "A. Sentiment analysis",
            "B. Speech recognition",
            "C. Key phrase extraction",
            "D. Entity recognition"
        ],
        "answer": "A. Sentiment analysis",
        "explanation": "Sentiment Analysis is the correct answer because it determines the tone or emotional intent of a text, such as whether a social media post expresses positivity, negativity, or neutrality. This is a common application of NLP in analyzing user opinions and attitudes. Why the other options are incorrect: B. Speech recognition Speech recognition converts spoken words into text. It does not analyze the tone or sentiment of written posts. C. Key phrase extraction Key phrase extraction identifies important phrases or keywords in a text (e.g., product quality, delivery time). It doesn't focus on the tone or emotional intent. D. Entity recognition Entity recognition identifies specific entities like names, dates, or locations in a text (e.g., Microsoft, New York). It doesn’t assess the tone or sentiment."
    },
    {
        "id": 199,
        "type": "single_choice",
        "question": "You are building a chatbot that will use natural language processing (NLP) to perform the following actions based on the text input of a user: - Accept customer orders, - Retrieve support documents, - Retrieve order status updates. Which type of NLP should you use?",
        "options": [
            "A. Sentiment analysis",
            "B. Named entity recognition",
            "C. Translation",
            "D. Language modeling"
        ],
        "answer": "B. Named entity recognition",
        "explanation": " Answer is B. Named Entity Recognition (NER) is an NLP technique used to identify and extract specific entities in text, such as names, dates, product names, or order IDs. In this case, NER would be appropriate for recognizing entities like: Customer orders: Identify product names, quantities, or order details. Support documents: Recognize document titles or topics. Order status updates: Extract order IDs or customer names from user input. Why the other options are incorrect: A. Sentiment Analysis: Sentiment analysis identifies the emotional tone (positive, negative, or neutral) in text, which is not relevant to the actions described. C. Translation: Translation converts text from one language to another. It does not help with understanding and acting on user input. D. Language Modeling: Language modeling predicts the next word or sequence of words in a sentence. While useful for chatbot generation, it is not the primary technique for extracting specific entities like order details."
    },
    {
        "id": 200,
        "type": "bucket",
        "layout": "bucket",
        "question": "Match the Azure Cognitive Services service to the appropriate actions. To answer, drag the appropriate service from the column on the left to its action on the right. Each service may be used once, more than once, or not at all.",
 "options": [
          { "id": "Language Service", "label": "Language Service" },
          { "id":  "Speech", "label":  "Speech" },
          { "id":  "Translator", "label":  "Translator" }
        ],
        "buckets": [
          { "id": "Convert spoken requests into text.", "accepts": [ "Speech"] },
          { "id": "Identify the intent of a user's request.", "accepts": ["Language Service"] },
          { "id": "Apply intent to entities and utternaces.", "accepts": ["Language Service"] }
        ],  
        "explanation": "-Speech The Speech service is used for speech-to-text functionality, which converts spoken words into textual data., - Language Service, The Language service (formerly known as LUIS Language Understanding Intelligent Service) is designed to identify the intent behind user inputs (e.g., 'Book a flight')."
    },
    {
        "id": 201,
        "type": "boolean",
        "question": "For each of the following statements, select Yes if the statement is true. Otherwise, select No.",
        "subQuestions": [
            {
                "prompt": "A webchat bot can interact with users visiting a website.",
                "correctAnswer": true
            },
            {
                "prompt": "Speech and Conversational AI workloads come under NLP.",
                "correctAnswer": true
            },
            {
                "prompt": "A smart device in the home that responds to questions such as 'What will the weather be like today?' is an example of natural language processing.",
                "correctAnswer": true
            }
        ],
        "explanation": "Yes, Yes, Yes is the answer.1. A webchat bot can interact with users visiting a website: Yes, a webchat bot can be implemented on a website to provide automated responses and interact with users in a conversational manner. 2. Speech and Conversational AI workloads come under NLP. 3. A smart device in the home that responds to questions such as (What will the weather be like today?) is an example of natural language processing: Yes, a smart device that can understand and respond to natural language queries or commands, such as asking about the weather, relies on natural language processing techniques to interpret and process the user's input and provide a relevant response. (Chat GPT)",
        "images": []
    },
    {
        "id": 202,
        "type": "single_choice",
        "question": "You have a website that includes customer reviews. You need to store the reviews in English and present the reviews to users in their respective language by recognizing each user's geographical location. Which type of natural language processing workload should you use?",
        "options": [
            "A. Key phrase extraction",
            "B. Speech recognition",
            "C. Language modeling",
            "D. Translation"
        ],
        "answer": "D. Translation",
        "explanation": "Answer is D. This should be translation. for this specific use case, the best option would be the Azure Translator Text API. The Translator Text API is a cloud-based machine translation service that can translate text between multiple languages in real-time. It supports a wide range of languages and provides language detection capabilities that can automatically identify the language of the input text. It can also detect the user's location based on their IP address or browser settings and automatically translate the reviews to the user's preferred language."
    },
    {
        "id": 203,
        "type": "boolean",
        "question": "For each of the following statements, select Yes if the statement is true. Otherwise, select No.",
        "subQuestions": [
            {
                "prompt": "Chatbot can support voice input.",
                "correctAnswer": true
            },
            {
                "prompt": "A separate chatbot is required for each communication channel.",
                "correctAnswer": false
            },
            {
                "prompt": "Chatbots manage conversation flows by using a combination of natural language and constrained option responses.",
                "correctAnswer": true
            }
        ],
        "explanation": "Answer is Yes, No, Yes. https://learn.microsoft.com/en-us/azure/bot-service/bot-service-overview?view=azure-bot-service-4.0#connect Bot Framework does most of the work necessary to send and receive messages from all of these different platforms—your bot application receives a unified, normalized stream of messages regardless of the number and type of channels it's connected to.",
        "images": []
    },
    {
        "id": 204,
        "type": "boolean",
        "question": "For each of the following statements, select Yes if the statement is true. Otherwise, select No.",
        "subQuestions": [
            {
                "prompt": "A bot that responds to queries by internal users is an example of a natural language processing (NLP) workload.",
                "correctAnswer": true
            },
            {
                "prompt": "A mobile application that displays images relating to an entered search term is an example of a natural language processing workload.",
                "correctAnswer": false
            },
            {
                "prompt": "A web form used to submit a request to reset a password is an example of a natural language processing workload.",
                "correctAnswer": false
            }
        ],
        "explanation": "Yes, No, No. Second one is Azure cognitive search 1 ->A bot that responds to queries by internal users is an example of a natural language processing (NLP) workload. Correct Answer: Yes This involves understanding and generating human language, which is the core of NLP. Responding to queries requires processing and interpreting language. 2 ->A mobile application that displays images relating to an entered search term is an example of a natural language processing workload. Correct Answer: No While the app accepts a search term (text), it focuses on retrieving and displaying images, not processing or analyzing the language itself. This is more aligned with search engine or image processing technologies, not NLP. 3 ->A web form used to submit a request to reset a password is an example of a natural language processing workload. Correct Answer: No Submitting a password reset request typically involves filling out a form and does not involve any language processing or understanding. It’s a simple data input/output task. Important Tip: When identifying NLP workloads, look for tasks involving understanding, interpreting, generating, or translating human language. Examples include chatbots, translation tools, sentiment analysis, and text summarization.",
        "images": []
    },
    {
        "id": 205,
        "type": "single_choice",
        "question": "You have a solution that analyzes social media posts to extract the mentions of city names and the city names discussed most frequently. Which type of natural language processing (NLP) workload does the solution use?",
        "options": [
            "A. Speech recognition",
            "B. Sentiment analysis",
            "C. Key phrase extraction",
            "D. Entity recognition"
        ],
        "answer": "D. Entity recognition",
        "explanation": "Entity recognition is the correct answer because it identifies and extracts specific entities, such as city names, from a text. This workload allows you to detect predefined categories like locations, dates, or people mentioned in social media posts.Why the other options are incorrect: A. Speech recognition Speech recognition converts spoken words into text but does not analyze or extract entities from the text.B. Sentiment analysis Sentiment analysis determines the emotional tone of text (e.g., positive, negative, or neutral) but does not extract specific entities like city names. C. Key phrase extraction Key phrase extraction identifies important phrases in the text but does not classify or specifically extract entities like city names."
    },

 {
        "id": 206,
        "type": "boolean",
        "question": "For each of the following statements, select Yes if the statement is true. Otherwise, select No.",
        "subQuestions": [
            {
                "prompt": "You can use Language Service's question answering to query an Azure SQL databases.",
                "correctAnswer": false
            },
            {
                "prompt": "You should use Language Service's question answering when you want a knowledge base to provide the same answer to different users who submit similar questions.",
                "correctAnswer": true
            },
            {
                "prompt": "Language Service's question answering can determine the intent of a user utterances.",
                "correctAnswer": false
            }
        ],
        "explanation": "No. Language Service's question answering is primarily designed to extract answers from pre-defined knowledge bases or documents, not directly query databases. Yes. Language Service's question answering can provide consistent answers to similar questions by leveraging the knowledge base and understanding the intent of the user's question. No. Language Service's question answering is focused on extracting answers from text and documents, rather than determining the intent of user utterances. Intent recognition is typically handled by other NLP components or services."
    },
    {
        "id": 207,
        "type": "single_choice",
        "question": "You are developing a solution that uses the Language service. You need to identify the main talking points in a collection of documents. Which type of natural language processing should you use?",
        "options": [
            "A. Language detection",
            "B. Sentiment analysis",
            "C. Entity recognition",
            "D. Key phrase extraction"
        ],
        "answer": "D. Key phrase extraction",
        "explanation": "Answer: D. Broad entity extraction: Identify important concepts in text, including key. Key phrase extraction/ Broad entity extraction: Identify important concepts in text, including key phrases and named entities such as people, places, and organizations. Reference: https://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/natural-languageprocessing"
    },
    {
        "id": 208,
        "type": "bucket",
        "layout": "bucket",
        "question": "You are designing a system that will generate insurance quotes automatically. Match the Microsoft responsible AI principles to the appropriate requirements. To answer, drag the appropriate principle from the column on the left to its requirement on the right. Each principle may be used once, more than once, or not at all.",

 "options": [
          { "id": "Accountability", "label": "Accountability" },
          { "id": "Fairness", "label": "Fairness" },
          { "id": "Inclusiveness", "label": "Inclusiveness" },
          { "id": "Privacy and Security", "label": "Privacy and Security" },
          { "id": "Reliability and Safety", "label": "Reliability and Safety" },
          { "id":  "Transparency", "label":  "Transparency" }
        ],
        "buckets": [
          { "id": "A customer’s personal information must be visible only to staff who are involved in the decision-making process.", "accepts": ["Privacy and Security"] },
          { "id": "The decision-making process must be recorded so that staff can identify the reasoning behind a particular quote.", "accepts": ["Transparency"] },
          { "id": "The system must be accessible to customers who use screen readers or other assistive technology.", "accepts": ["Inclusiveness"] }
        ],  
        "explanation": "Privacy and security: A customer's personal information must be visible only to staff who are involved in the decision-making process to ensure the privacy and security of sensitive data. Transparency: The decisionmaking process must be recorded so that staff can identify the reasoning behind a particular quote, promoting transparency and accountability. Inclusiveness: The system must be accessible to customers who use screen readers or other assistive technology, ensuring inclusiveness and providing equal access to all users."
    },
    {
        "id": 209,
        "type": "single_choice",
        "question": "Which type of natural language processing (NLP) entity is used to identify a phone number?",
        "options": [
            "A. Regular expression",
            "B. Machine-learned",
            "C. List",
            "D. Pattern.any"
        ],
        "answer": "A. Regular expression",
        "explanation": "A is the answer. https://learn.microsoft.com/en-us/azure/cognitive-services/luis/reference-entity-regular-expression?tabs=V2A regular expression entity extracts an entity based on a regular expression pattern you provide. Regular expressions are commonly used to define patterns for identifying phone numbers. By defining a regular expression pattern that matches the format of a phone number, you can effectively extract phone numbers from text using NLP techniques."
    },

 {
        "id": 210,
        "type": "dropdown",
        "question": "Returning a bounding box that indicates the location of a vehicle in an image is an example of _____________ .",
        "options": [
          { "label": "Object detection", "value": "Object detection" },
          { "label": "Image classification", "value":  "Image classification" },
          { "label": "Optical character recognition (OCR)", "value": "Optical character recognition (OCR)" },
	  { "label": "Facial detection", "value":  "Facial detection" }
        ],
        "correctAnswer": "Object detection",
        "placeholder": "— Select an option —",
        "explanation": "Correct Answer: Object Detection. Why? Object detection involves identifying the presence of objects (like vehicles) in an image and specifying their locations, often by drawing bounding boxes. This is exactly what the question describes. Why Other Options Are Incorrect: Image Classification: This assigns a label to the entire image (e.g., vehicle), without locating it in the image. It doesn't involve bounding boxes. Optical Character Recognition (OCR): OCR extracts text from images, not objects like vehicles. Facial Detection: This specifically identifies human faces in an image, not vehicles or other objects. Important Tip: Key Difference: Object detection provides both identification and localization (bounding boxes), whereas image classification only identifies the presence of an object in the entire image."
       
    },
    {
        "id": 211,
        "type": "single_choice",
        "question": "Your company is exploring the use of voice recognition technologies in its smart home devices. The company wants to identify any barriers that might unintentionally leave out specific user groups. This is an example of which Microsoft guiding principle for responsible AI?",
        "options": [
            "A. Accountability",
            "B. Fairness",
            "C. Privacy and security",
            "D. Inclusiveness"
        ],
        "answer": "D. Inclusiveness",
        "explanation": "Correct Answer: D. Inclusiveness Why? Inclusiveness focuses on designing AI systems that are accessible and usable by people of all backgrounds, abilities, and demographics. Ensuring no user group is unintentionally left out directly reflects this principle. Why Other Options Are Incorrect: A. Accountability: Accountability refers to ensuring that AI systems have clear responsibilities and oversight mechanisms. While important, this scenario is more about inclusive design than assigning responsibility. B. Fairness: Fairness is about mitigating bias and ensuring equitable outcomes for all users. While related, the scenario is broader in scope, addressing inclusivity rather than focusing solely on fairness. C. Privacy and Security: This principle pertains to safeguarding user data and ensuring the confidentiality of personal information. It is not the primary concern in this context. Important Tip: Inclusiveness ensures that AI systems are designed to accommodate the widest range of users, making them accessible and equitable for all. This principle is critical in diverse and global applications like smart home technologies."
    },
    {
        "id": 212,
        "type": "bucket",
        "layout": "bucket",
        "question": "You have a large dataset that contains motor vehicle sales data. You need to train an automated machine learning (automated ML) model to predict vehicle sale values based on the type of vehicle. Which task should you select? To answer, select the appropriate task in the answer area.",
        "images": [
            "https://bongawat.wordpress.com/wp-content/uploads/2026/01/ai900_q208.png?w=NaN&h="
        ],
"options": [
          { "id": "Classification", "label": "Classification" },
          { "id": "Regression", "label": "Regression" },
          { "id": "Time series forecasting", "label": "Time series forecasting" },
          { "id": "Natural Language Processing", "label": "Natural Language Processing" },
          { "id": "Computer Vision", "label": "Computer Vision" },
	  { "id": "View Additional configuration settings", "label": "View Additional configuration settings" },
	  { "id": "view featurization settings", "label": "view featurization settings" }
        ],
        "buckets": [
          { "id": "Selection Number 1.", "accepts": ["Regression"] },
          { "id": "Selection Number 2.", "accepts": ["View Additional configuration settings"] }
        ],  
        "explanation": "Correct Task Selection:Regression: Why? Regression is used to predict continuous numeric values, such as prices, sales, or other measurements. Since vehicle sale values are numeric, this is the appropriate choice. Why Other Options Are Incorrect: Classification: Used for predicting categories or labels (e.g., Car Type: SUV, Sedan, Truck), not continuous values. Time Series Forecasting: Used for predicting values over time (e.g., future sales trends), but the scenario does not specify a time-based prediction. Natural Language Processing (NLP): Used for text-based tasks (e.g., sentiment analysis, text classification), irrelevant to predicting numeric values. Computer Vision: Used for image-based tasks like object detection or image classification, unrelated to the scenario. Important Tip: Always match the task type to the nature of the prediction target: Continuous numeric values → Regression Categorical values → Classification Time-based trends → Time Series Forecasting."
       
    },

 {
        "id": 213,
        "type": "dropdown",
        "question": "When evaluating the performance of a model, the _____________  displays the predicted and actual positives and negatives by using a grid of 0 and 1 values.",
        "options": [
          { "label": "AUC metric", "value": "AUC metric" },
          { "label": "confusion matrix", "value":  "confusion matrix" },
          { "label": "ROC curve", "value": "ROC curve"},
	  { "label":  "threshold", "value":  "threshold" }
        ],
        "correctAnswer": "confusion matrix",
        "placeholder": "— Select an option —",
        "explanation": "Answer: confusion matrix. https://learn.microsoft.com/en-us/azure/machine-learning/how-to-understand-automated-ml?view=azuremlapi-2#confusion-matrix-for-a-good-model"
   
    },
    {
        "id": 214,
        "type": "single_choice",
        "question": "You need to convert receipts into transactions in a spreadsheet. The spreadsheet must include the date of the transaction, the merchant, the total spent, and any taxes paid. Which Azure AI service should you use?",
        "options": [
            "A. Custom Vision",
            "B. Form Recognizer",
            "C. Face",
            "D. Language"
        ],
        "answer": "B. Form Recognizer",
        "explanation": "Correct Answer: B. Form Recognizer Why? Azure Form Recognizer is specifically designed to extract structured data from forms, receipts, and invoices. It can identify key-value pairs and tables in scanned or digital documents, making it ideal for converting receipt data into a spreadsheet. Why Other Options Are Incorrect: A. Custom Vision: Used for image classification and object detection tasks, not for extracting structured text or data from documents. C. Face: Focused on facial recognition tasks such as identifying or verifying individuals, which is unrelated to this scenario. D. Language: Used for text-based natural language processing tasks like sentiment analysis, translation, or entity recognition. While it deals with language, it doesn't specialize in extracting structured data from forms. Important Tip: Use Azure Form Recognizer whenever you need to extract and analyze data from structured or semistructured documents like receipts, invoices, or business forms."
    },
 {
        "id": 215,
        "type": "dropdown",
        "question": "Predicting how many vehicles will travel across a bridge on a given day is an example of _____________ .",
        "options": [
          { "label": "Regression", "value": "Regression" },
          { "label": "Classification", "value":  "Classification" },
	  { "label": "Clustering", "value": "Clustering" }
        ],
        "correctAnswer": "Regression",
        "placeholder": "— Select an option —",
        "explanation": "Predicting how many vehicles will travel across a bridge on a given day is an example of regression because it involves predicting a numeric value (the number of vehicles). Why the other options are incorrect: Classification Classification is used to predict categories or labels (e.g., low traffic vs. high traffic), not numeric values. Clustering Clustering groups data points based on similarity but does not predict values. It’s unsupervised learning used for finding patterns or groups in data."
            },

{
        "id": 216,
        "type": "dropdown",
        "question": "In a machine learning model, the data that is used as inputs are called  _____________ .",
        "options": [
          { "label": "dataset", "value": "dataset" },
          { "label": "labels", "value":  "labels" },
	  { "label":  "features", "value":  "features" }
        ],
        "correctAnswer":  "features",
        "placeholder": "— Select an option —",
        "explanation": "Correct answer is Features. The inputs of a machine learning model are called features. Reference: https://learn.microsoft.com/en-us/dotnet/machine-learning/how-does-mldotnet-work#basic"
           },
    {
        "id": 217,
        "type": "single_choice",
        "question": "You have a security system that analyzes images from CCTV to provide authorized staff entry into restricted area. Which type of computer vision does the system use?",
        "options": [
            "A. Optical character recognition (OCR)",
            "B. Semantic segmentation",
            "C. Facial detection and facial recognition",
            "D. Image analysis"
        ],
        "answer": "C. Facial detection and facial recognition",
        "explanation": "Correct Answer: C.  Why? Facial detection identifies the presence of a face in the image. Facial recognition verifies or identifies the individual by comparing the detected face with stored authorized records. This combination is specifically designed for systems like security authentication and access control. Why Other Options Are Incorrect: A. Optical Character Recognition (OCR): OCR is used to extract text from images, such as reading license plates or documents. It is not relevant to identifying or verifying faces. B. Semantic Segmentation: Semantic segmentation is used for pixel-level categorization in images, such as segmenting objects in an image. It doesn’t involve face detection or recognition. D. Image Analysis: Image analysis is a broader category that involves understanding image content (e.g., colors, objects, etc.). While it may include face detection, it does not specifically handle face verification or recognition. Important Tip: Facial detection and recognition are widely used in access control systems, ensuring security by verifying identities in real-time. Always associate these terms with applications like surveillance, authentication, and biometric verification."
    },
    {
        "id": 218,
        "type": "multiple_choice",
        "question": "For which two workloads can you use computer vision? Each correct answer presents a complete solution.",
        "options": [
            "A. Assigning the color pixels in an image to object names",
            "B. Detecting inconsistencies and anomalies in a stream of data",
            "C. Creating visual representations of numerical data",
            "D. Creating photorealistic images by using three-dimensional models",
            "E. Describing the contents of an image"
        ],
        "answer": [
            "A. Assigning the color pixels in an image to object names",
            "E. Describing the contents of an image"
        ],
        "allow_multiple": true,
        "max_selections": 2,
        "explanation": "Correct Answers: A. Assigning the color pixels in an image to object names Why? This is an example of semantic segmentation, a computer vision task where each pixel in an image is classified as part of a specific object or category. E. Describing the contents of an image Why? This is an example of image analysis, where computer vision systems generate descriptions or captions for the contents of an image, such as identifying objects, scenes, or activities. Why Other Options Are Incorrect: B. Detecting inconsistencies and anomalies in a stream of data This is not related to computer vision. It falls under data analytics or anomaly detection in time-series data. C. Creating visual representations of numerical data This is related to data visualization, not computer vision. Tasks like creating graphs or charts involve visualization tools, not image analysis. D. Creating photorealistic images by using three-dimensional models This relates to 3D rendering or computer graphics, not computer vision. Computer vision analyzes images, while 3D rendering generates images. Important Tip: Computer vision focuses on understanding and interpreting visual data from images or videos. Common tasks include object detection, image classification, semantic segmentation, and generating image descriptions."
    },
    {
        "id": 219,
        "type": "single_choice",
        "question": "You have an app that identifies the coordinates of a product in an image of a supermarket shelf. Which service does the app use?",
        "options": [
            "A. Custom Vision classification",
            "B. Custom Vision object detection",
            "C. Computer Vision Read",
            "D. Computer Vision optical character recognition (OCR)"
        ],
        "answer": "B. Custom Vision object detection",
        "explanation": "Correct Answer: B. Why? Object detection identifies objects in an image and provides their coordinates (bounding boxes). Custom Vision object detection is specifically designed to detect and locate objects based on custom-trained models. Why Other Options Are Incorrect: A. Custom Vision classification: Classification assigns a label to an entire image (e.g., 'contains product X') but does not identify object locations or coordinates. C. Computer Vision Read: The 'Read' feature in Computer Vision is used for extracting text from images, not detecting objects or providing coordinates. D. Computer Vision optical character recognition (OCR): OCR extracts text from images (e.g., price tags or product labels), but it does not detect objects or their locations. Important Tip: For tasks involving detecting and localizing objects in an image, use object detection. If the task is about identifying objects without location or handling text extraction, other services like classification or OCR are more appropriate."
    },
 {
        "id": 220,
        "type": "dropdown",
        "question": "A traffic monitoring system that collects vehicle registration numbers from CCTV footage is an example of _____________ .",
        "options": [
          { "label": "object detection", "value": "object detection" },
          { "label": "image classification", "value":  "image classification" },
          { "label": "text extraction (OCR)", "value": "text extraction (OCR)"},
	  { "label": "spatial analysis", "value": "spatial analysis" }
        ],
        "correctAnswer": "text extraction (OCR)",
        "placeholder": "— Select an option —",
        "explanation": "the answer is text extraction Since the goal is to extract the vehicle registration numbers from the CCTV footage, text extraction using OCR algorithms would be the most appropriate choice. OCR algorithms can recognize and extract text information from images, making it possible to retrieve the registration numbers from the captured video frames. This enables automated monitoring and analysis of the traffic data by extracting the relevant textual information. A traffic monitoring system that collects vehicle registration numbers from CCTV footage is an example of text extraction because the system identifies and extracts the alphanumeric text (license plate numbers) from the images. Why the other options are incorrect: Image classification Image classification categorizes entire images into predefined classes (e.g., car or truck) but does not extract text. Object detection Object detection identifies and locates objects (e.g., cars) in an image but does not handle text extraction from those objects. Spatial Analysis is used to analyze movement or presence in physical spaces (e.g., people counting) and is not designed for extracting text. Key Tip: Use text extraction (OCR) when dealing with tasks involving reading and digitizing text from images or videos, such as license plates or documents."
          },
    {
        "id": 221,
        "type": "single_choice",
        "question": "You need to build an image tagging solution for social media that tags images of your friends automatically. Which Azure Cognitive Services service should you use?",
        "options": [
            "A. Face",
            "B. Form Recognizer",
            "C. Language",
            "D. Computer Vision"
        ],
        "answer": "A. Face",
        "explanation": "A. Face is the correct answer because the Face service is specifically designed to detect and recognize human faces in images. It can match faces to a database of your friends and automatically tag their names in the images. Why the other options are incorrect: B. Form Recognizer  is used for extracting information from structured documents like forms or receipts. It’s not relevant for tagging images of people. C. Language service is designed for natural language processing tasks, such as analyzing or processing text. It doesn’t handle image or face recognition. D. Computer Vision While Computer Vision provides general image analysis (e.g., object detection, scene description), it doesn’t specialize in identifying and tagging specific faces. For facial recognition, the Face service is required. Key Tip: Use the Face service when you need to detect, recognize, or analyze human faces, such as tagging friends in."
    },
    {
        "id": 223,
        "type": "hotspot",
        "question": "You have an app that identifies birds in images. The app performs the following tasks: Identifies the location of the birds in the image; Identifies the species of the birds in the image. Which type of computer vision does each task use?",
        "options": [
            "Automated captioning",
            "Image classification",
            "Object detection",
            "Optical character recognition (OCR)"
        ],
        "explanation": "1. Locate the birds: Correct Answer: Object detection identifies and locates objects within an image, such as birds, by drawing bounding boxes around them. This allows the system to find where the birds are in the image. 2. Identify the species of the birds: Correct Answer: Image classification categorizes an entire image into predefined classes (e.g., different bird species) based on its content. It does not locate the birds but determines what kind of bird species they are. Why the other options are incorrect: Automated captioning: generates descriptive text for an image, but it doesn’t locate or classify specific objects like birds. Optical character recognition (OCR): OCR extracts text from images but is not applicable for locating or identifying birds.Key Tip: Use object detection for locating objects in images. Use image classification for identifying or categorizing objects within an image.",
        "interactivity": {
            "hotspot_areas": [
                {
                    "prompt": "Locate the birds [Answer Choice].",
                    "answer": "Object detection"
                },
                {
                    "prompt": "Identify the species [Answer Choice].",
                    "answer": "Image classification"
                }
            ]
        }
    },
    {
        "id": 224,
        "type": "single_choice",
        "question": "You have a solution that reads manuscripts in different languages and categorizes the manuscripts based on topic. Which types of natural language processing (NLP) workloads does the solution use?",
        "options": [
            "A. Speech recognition and entity recognition",
            "B. Speech recognition and language modeling",
            "C. Translation and key phrase extraction",
            "D. Translation and sentiment analysis"
        ],
        "answer": "C. Translation and key phrase extraction",
        "explanation": "The correct answer is C. because: Translation is needed to read manuscripts in different languages and convert them into a common language for further processing. Key phrase extraction is used to identify important terms or topics within the text, enabling the categorization of the manuscripts by topic. Why the other options are incorrect: A. Speech recognition and entity recognition, Speech recognition converts spoken language into text, which is irrelevant since manuscripts are already written. Entity recognition focuses on extracting specific entities like names or locations, not categorizing topics. B. Speech recognition and language modeling Speech recognition is not relevant as the task involves manuscripts (written text). Language modeling predicts the likelihood of text sequences but does not categorize text by topic. D. Translation is correct, but sentiment analysis measures the emotional tone (positive/negative) of the text, which is unrelated to categorizing topics. Key Tip: For tasks involving text in multiple languages and categorization by topic, focus on translation for language standardization and key phrase extraction to identify the main themes."
    },

 {
        "id": 225,
        "type": "dropdown",
        "question": "The interactive answering of questions entered by a user as part of an application is an example of _____________ .",
        "options": [
          { "label": "anomaly detection", "value": "anomaly detection" },
          { "label": "computer vision", "value":  "computer vision" },
          { "label": "forecasting", "value": "forecasting" },
	  { "label": "natural language processing (NLP)", "value": "natural language processing (NLP)" }
        ],
        "correctAnswer": "natural language processing (NLP)",
        "placeholder": "— Select an option —",
       "explanation": "NLP is the correct answer The interactive answering of questions entered by a user as part of an application is an example of natural language processing (NLP). NLP is used to understand, process, and respond to human language in a meaningful way, which is the core functionality of interactive question answering. Why the other options are incorrect: Anomaly detection: This is used to identify unusual patterns or deviations in data, such as fraud detection, not for answering questions. Computer vision: This focuses on analyzing and interpreting visual data (images and videos), not processing textual queries. Forecasting: Forecasting predicts future outcomes based on historical data, such as weather predictions or sales forecasts, but it does not handle language interaction. Key Tip: Use natural language processing (NLP) for any task involving understanding, interpreting, or responding to human language. Examples include chatbots, virtual assistants, and text analysis."
          },
    {
        "id": 226,
        "type": "single_choice",
        "question": "You have 100 instructional videos that do NOT contain any audio. Each instructional video has a script. You need to generate a narration audio file for each video based on the script. Which type of workload should you use?",
        "options": [
            "A. Language modeling",
            "B. Speech recognition",
            "C. Speech synthesis",
            "D. Translation"
        ],
        "answer": "C. Speech synthesis",
        "explanation": "The correct answer is C. Speech synthesis because speech synthesis converts text (the script) into spoken audio, generating narration for the instructional videos. Why the other options are incorrect: A. Language modeling: Language modeling predicts the next word in a sequence or processes language for other NLP tasks, but it does not generate audio. B. Speech recognition: Speech recognition converts spoken words into text, which is the opposite of what’s needed here (text to speech). D. Translation: Translation converts text from one language to another. It does not create audio files from scripts. Key Tip: Use speech synthesis (also known as text-to-speech) to generate audio narrations from written text, especially for applications like instructional videos, audiobooks, or accessibility features."
    },
 {
        "id": 227,
        "type": "dropdown",
        "question": "Natural language processing (NLP) can be used to _____________ .",
        "options": [
          { "label": "classify email messages as work-related or personal", "value": "classify email messages as work-related or personal" },
          { "label": "predict the number of future car rentals", "value":  "predict the number of future car rentals" },
          { "label": "predict which website visitors will make a transaction", "value": "predict which website visitors will make a transaction" },
	  { "label": "stop a process in a factory when extremely high temperatures are registered", "value": "stop a process in a factory when extremely high temperatures are registered" }
        ],
        "correctAnswer": "classify email messages as work-related or personal",
        "placeholder": "— Select an option —",
        "explanation": "The correct answer is classify email messages as work-related or personal because natural language processing (NLP) can analyze and understand the content of emails to categorize them based on their context. Why the other options are incorrect: Predict the number of future car rentals: This task involves forecasting or regression analysis, not NLP. Predict which website visitors will make a transaction: This is a use case for predictive analytics, possibly involving machine learning models like classification, but not NLP unless the prediction is based on textual input. Stop a process in a factory when extremely high temperatures are registered: This task involves anomaly detection or monitoring, unrelated to NLP.Key Tip: Use natural language processing (NLP) for tasks that involve analyzing, understanding, or classifying text or speech, such as email classification, sentiment analysis, or chatbot responses."
            },
    {
        "id": 228,
        "type": "single_choice",
        "question": "Which AI service can you use to extract intent from a user input such as 'Call me back later'?",
        "options": [
            "A. Azure Cognitive Search",
            "B. Translator",
            "C. Language",
            "D. Speech"
        ],
        "answer": "C. Language",
        "explanation": "C. Language You can use the Azure Cognitive Services Language service to extract intent from a user input like Call me back later. This service provides natural language processing (NLP) capabilities, including intent recognition, and can be used to understand the meaning and intent behind user text input. It's commonly used for tasks like chatbots, virtual assistants, and other applications that involve processing and understanding usergenerated text. The other options, Azure Cognitive Search (A), Translator (B), and Speech (D), are not specifically designed for intent extraction from user inputs."
    },
 {
        "id": 230,
        "type": "dropdown",
        "question": " _____________  can be used to build no-code apps that use built-in natural language processing models.",
        "options": [
          { "label": "Power Virtual Agents", "value": "Power Virtual Agents" },
          { "label": "Azure Health Bot", "value":  "Azure Health Bot" },
	  { "label":  "Microsoft Bot Framework", "value":  "Microsoft Bot Framework" }
        ],
        "correctAnswer": "Power Virtual Agents",
        "placeholder": "— Select an option —",
        "explanation": "The correct answer is Power Virtual Agents, as it is a no-code platform that allows users to create intelligent chatbots using built-in natural language processing (NLP) models without requiring any coding expertise. Why the other options are incorrect: Azure Health Bot: This is a specialized bot framework designed for healthcare scenarios, but it is not a general-purpose, no-code platform like Power Virtual Agents. Microsoft Bot Framework: While the Microsoft Bot Framework provides extensive tools and SDKs for building bots, it requires programming and is not a no-code solution. Key Tip: Use Power Virtual Agents for building chatbots quickly and easily with minimal technical expertise, especiallyfor scenarios where no coding is desired."
          },
    {
        "id": 231,
        "type": "boolean",
        "question": "For each of the following statements, select Yes if the statement is true. Otherwise, select No.",
        "subQuestions": [
            {
                "prompt": "A smart device in the home that responds to questions such as 'When is my next appointment?' is an example of conversational AI.",
                "correctAnswer": true
            },
            {
                "prompt": "An interactive webchat feature on a company website can be implemented by using Azure Bot Service.",
                "correctAnswer": true
            },
            {
                "prompt": "Automatically generating captions for pre-recorded videos is an example of conversation AI.",
                "correctAnswer": false
            }
        ],
        "explanation": "Yes, Yes, No"
    },
    {
        "id": 232,
        "type": "single_choice",
        "question": "What is an example of the Microsoft responsible AI principle of transparency?",
        "options": [
            "A.Ensuring that opportunities are allocated equally to all applicants",
            "B. Helping users understand the decisions made by an AI system",
            "C. Ensuring that developers are accountable for the solutions they create",
            "D. Ensuring that the privileged data of users is stored in a secure manner"
        ],
        "answer": "B. Helping users understand the decisions made by an AI system",
        "explanation": "The correct answer is B. Transparency is a key principle in Microsoft's Responsible AI guidelines. It emphasizes that AI systems should provide clear and understandable information about how they function, including the decisions they make. This helps users trust the AI and ensures they can effectively use it. Why the other options are incorrect:A. ensuring that opportunities are allocated equally to all applicants:This is an example of the Fairness principle, not Transparency. Fairness ensures AI systems do not create or reinforce biases. C. ensuring that developers are accountable for the solutions they create: This aligns with the principle of Accountability, where developers and organizations must take responsibility for their AI solutions. D. ensuring that the privileged data of users is stored in a secure manner: This relates to the Privacy and Security principle, which focuses on safeguarding user data. Key Tip: Transparency ensures users can understand and trust AI systems by providing clear explanations of how decisions are made."
    },
    {
        "id": 233,
        "type": "single_choice",
        "question": "You need to provide customers with the ability to query the status of orders by using phones, social media, or digital assistants. What should you use?",
        "options": [
            "A. An Azure Machine Learning model",
            "B. The Translator service",
            "C. A Form Recognizer model",
            "D. Azure Bot Service"
        ],
        "answer": "D. Azure Bot Service",
        "explanation": "Answer: D. Because Azure Bot Service provides an integrated environment that is purpose-built for bot development, enabling you to build, connect, test, deploy, and manage intelligent bots, all from one place. You can build bots that can interact naturally with users using a range of channels, including social media platforms and digital assistants, which fits your requirement."
    },
    {
        "id": 234,
        "type": "single_choice",
        "question": "You plan to build a conversational AI solution that can be surfaced in Microsoft Teams, Microsoft Cortana, and Amazon Alexa. Which service should you use?",
        "options": [
            "A. Azure Bot Service",
            "B. Azure Cognitive Search",
            "C. Speech",
            "D. Language service"
        ],
        "answer": "A. Azure Bot Service",
        "explanation": "Answer is A. Bots created using Azure Bot Service can be integrated with voice-based solutions such as Cortana, Alexa, or Google Assistant. Azure Bot Service provides the necessary channels and interfaces to connect bots to these services, allowing them to process and respond to voice commands. This integration enables users to interact with the bots through natural language voice commands, extending the bot’s capabilities to various voiceenabled platforms."
    },
    {
        "id": 235,
        "type": "boolean",
        "question": "For each of the following statements, select Yes if the statement is true. Otherwise, select No.",
        "subQuestions": [
            {
                "prompt": "An interactive webchat feature on a company website can be implemented by using Azure Bot Service.",
                "correctAnswer": true
            },
            {
                "prompt": "Automatically generating captions for pre-recorded videos is an example of conversational AI.",
                "correctAnswer": false
            },
            {
                "prompt": "A smart device in the home that responds to questions such as “When is my next appointment?” is an example of conversational AI.",
                "correctAnswer": true
            }
        ],
        "explanation": "Yes, No, Yes. Because Azure Bot Service is designed to create conversational AI experiences, such as chatbots, which can be integrated into websites for customer interaction. for the second question, Automatically generating captions falls under speech-to-text processing, which is not conversational AI. Conversational AI involves interacting with users through natural language. For the third question This is an example of conversational AI, as it involves interaction between the user and the device using natural language to provide relevant responses."
    },
    {
        "id": 236,
        "type": "single_choice",
        "question": "Which Azure Cognitive Services service can be used to identify documents that contain sensitive information?",
        "options": [
            "A. Custom Vision",
            "B. Conversational Language Understanding",
            "C. Form Recognizer"
        ],
        "answer": "C. Form Recognizer",
        "explanation": "Answer is C: Azure Form Recognizer is designed to analyze and extract information from documents, including structured and unstructured data. It can identify and process sensitive information by analyzing the content of documents and categorizing data, making it an ideal service for scenarios where identifying sensitive information is required. Other options: Custom Vision is used for image classification and object detection, not document analysis. Conversational Language Understanding is used to understand and process conversational text but is not meant for document analysis or sensitive information detection."
    },

 {
        "id": 237,
        "type": "dropdown",
        "question": "Detecting unusual temperature fluctuations for a large machine is an example of: _____________ .",
        "options": [
          { "label": "a computer vision workload", "value": "a computer vision workload" },
          { "label": "a knowledge mining workload", "value":  "a knowledge mining workload" },
          { "label": "a natural language processing (NLP) workload", "value":"a natural language processing (NLP) workload" },
	  { "label": "an anomaly detection workload", "value": "an anomaly detection workload"}
        ],
        "correctAnswer": "an anomaly detection workload",
        "placeholder": "— Select an option —",
        "explanation": "Detecting unusual temperature fluctuations for a large machine is an example of an anomaly detection workload. Anomaly detection refers to identifying data points or patterns that do not conform to expected behavior. This is commonly applied to monitor systems like machines, where detecting deviations (e.g., temperature changes) can indicate potential issues or failures. The other workloads listed (computer vision, knowledge mining, and NLP) are unrelated to monitoring numerical or sensor data for anomalies."
           },
    {
        "id": 238,
        "type": "single_choice",
        "question": "A smart device that responds to the question 'What is the stock price of Contoso Ltd.?' is an example of which AI workload?",
        "options": [
            "A. Knowledge mining",
            "B. Natural language processing",
            "C. Computer vision",
            "D. Anomaly detection"
        ],
        "answer": "B. Natural language processing",
        "explanation": "Correct Answer B. A smart device that responds to the question 'What is the stock price of Contoso. Ltd.?' is an example of an AI workload related to natural language processing1. Natural language processing (NLP) is a branch of artificial intelligence that focuses on the interaction between computers and humans through natural language. It involves the ability of a computer program to understand, interpret, and generate human language in a valuable way. Knowledge mining is a process that involves extracting useful information from unstructured data sources such as text documents, images, and videos1. Computer vision refers to the field of computer science that focuses on enabling computers to gain high-level understanding from digital images or videos1. Anomaly detection is a technique used to identify patterns or data points that deviate significantly from the normal behavioUr or expected values1."
    },
    {
        "id": 239,
        "type": "bucket",
        "layout": "bucket",
        "question": "Match the machine learning models to the appropriate descriptions. To answer, drag the appropriate model from the column on the left to its description on the right. Each model may be used once, more than once, or not at all.",
 "options": [
          { "id": "Classification", "label": "Classification" },
          { "id": "Clustering", "label": "Clustering" },
          { "id": "Regression", "label": "Regression" }
        ],
        "buckets": [
          { "id": "A supervised machine learning model used to predict numeric values.", "accepts": ["Regression"] },
          { "id": "A supervised machine learning model used to predict categories.", "accepts": ["Classification"] },
          { "id": "An unsupervised machine learning model used to group similar entities based on features.", "accepts": ["Clustering"] }
        ],  
        "explanation": "Regression is used in supervised learning tasks where the target variable is continuous (e.g., predicting house prices or temperatures). Classification is used in supervised learning tasks where the target variable is categorical (e.g., identifying spam vs. non-spam emails). Clustering is an unsupervised learning technique used to group data into clusters based on similarity (e.g., customer segmentation)."
        
    },
    {
        "id": 240,
        "type": "single_choice",
        "question": "You are building a tool that will process images from retail stores and identify the products of competitors. The solution must be trained on images provided by your company. Which Azure AI service should you use?",
        "options": [
            "A. Form Recognizer",
            "B. Custom Vision",
            "C. Face",
            "D. Computer Vision"
        ],
        "answer": "B. Custom Vision",
        "explanation": "B - Custom Vision is the correct answer, because you have to use your own images as input to train the model. Custom Vision is the appropriate Azure AI service for this scenario because it allows you to train a custom. model on images provided by your company. This service is specifically designed for image classification and object detection tasks where custom datasets are used, such as identifying specific products in retail stores. Why not the others? A. Form Recognizer: This is used for extracting structured data from documents like forms and receipts, not for identifying products in images. C. Face: This is used for facial recognition tasks, such as detecting and identifying human faces, not for identifying products. D. Computer Vision: While it provides general image analysis capabilities, it is not customizable like Custom Vision and wouldn't allow you to train a model specifically for identifying competitor products. Custom Vision is best suited for scenarios requiring tailored object detection and classification models based on specific training data."
    },
 {
        "id": 241,
        "type": "dropdown",
        "question": "Predicting how many hours of overtime a delivery person will work based on the number of orders received is an example of: _____________ .",
        "options": [
          { "label": "Classification", "value": "Classification" },
          { "label": "Clustering", "value":  "Clustering"},
	  { "label": "Regression", "value": "Regression" }
        ],
        "correctAnswer": "Regression",
        "placeholder": "— Select an option —",
        "explanation": "Predicting how many hours of overtime a delivery person will work based on the number of orders received is an example of a regression model. Regression is used for predicting numerical values based on input data, which in this case is the number of hours of overtime based on the number of orders."
        },
    {
        "id": 242,
        "type": "single_choice",
        "question": "Predicting agricultural yields based on weather conditions and soil quality measurements is an example of which type of machine learning model?",
        "options": [
            "A. Classification",
            "B. Regression",
            "C. Clustering"
        ],
        "answer": "B. Regression",
        "explanation": "Answer is B: Regression models are used to predict continuous numeric values, such as agricultural yields, based on input features like weather conditions and soil quality measurements. These models are ideal for problems where the output is a numeric quantity.Why not the others? A. Classification: Classification is used to predict discrete categories (e.g., 'healthy' or 'unhealthy' crop), not numeric values like yield. C. Clustering: Clustering is an unsupervised learning approach used to group data points based on their features, not for predicting numeric outcomes. Thus, regression is the appropriate choice for predicting agricultural yields."
    },
    {
        "id": 243,
        "type": "single_choice",
        "question": "You need to identify street names based on street signs in photographs. Which type of computer vision should you use?",
        "options": [
            "A. Object detection",
            "B. Optical character recognition (OCR)",
            "C. Image classification",
            "D. Facial recognition"
        ],
        "answer": "B. Optical character recognition (OCR)",
        "explanation": "B-OCR, because you are extracting test from images by identifying the characters. OCR (Optical Character Recognition) is specifically designed to extract and recognize text from images, such as street names from street signs in photographs. It processes the visual data to identify characters, letters, and words, making it the ideal choice for identifying street names. Why not the others? A. Object detection: Object detection is used to identify and locate objects (e.g., cars, traffic lights) in an image but does not process text. C. Image classification: Image classification assigns a label to the entire image (e.g., 'street sign') but cannot extract specific text like street names. D. Facial recognition: Facial recognition is used to identify or verify individuals' faces, which is unrelated to recognizing text. Thus, OCR is the appropriate solution for identifying street names on street signs."
    },
    {
        "id": 244,
         "type": "bucket",
        "layout": "bucket",
        "question": "Match the types of computer vision workloads to the appropriate scenarios. To answer, drag the appropriate workload type from the column on the left to its scenario on the right. Each workload type may be used once, more than once, or not at all.",
    
 "options": [
          { "id": "Image classification", "label": "Image classification" },
          { "id": "Object detection", "label": "Object detection" },
          { "id":  "Optical character recognition (OCR)", "label":  "Optical character recognition (OCR)" }
        ],
        "buckets": [
          { "id": "Generate captions for images.", "accepts": ["Image classification"] },
          { "id": "Extract movie title names from movie poster images.", "accepts": [ "Optical character recognition (OCR)"] },
          { "id": "Locate vehicles in images.", "accepts": ["Object detection"] }
        ],  
        "explanation": "While this could be related to automated captioning, image classification is often used as part of creating captions by identifying what the image contains. OCR is specifically designed to extract text from images, such as the titles on movie posters. Object detection identifies and locates specific objects (e.g., vehicles) within images."
      
    },
    {
        "id": 245,
        "type": "single_choice",
        "question": "You have a bot that identifies the brand names of products in images of supermarket shelves. Which service does the bot use?",
        "options": [
            "A. AI enrichment for Azure Search capabilities",
            "B. Computer Vision Image Analysis capabilities",
            "C. Custom Vision Image Classification capabilities",
            "D. Language Understanding capabilities"
        ],
        "answer": "B. Computer Vision Image Analysis capabilities",
        "explanation": "Answer is B. Computer Vision. Brand detection is a specialized mode of object detection that uses a database of thousands of global logos to identify commercial brands in images or video. You can use this feature, for example, to discover which brands are most popular on social media or most prevalent in media product placement. https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/concept-brand-detection"
    },
    {
        "id": 246,
        "type": "single_choice",
        "question": "You are developing a chatbot solution in Azure. Which service should you use to determine a user’s intent?",
        "options": [
            "A. Translator",
            "B. Language",
            "C. Azure Cognitive Search",
            "D. Speech"
        ],
        "answer": "B. Language",
        "explanation": "Answer is B. Language (formerly part of Language Understanding Intelligent Service or LUIS) is a service designed to analyze natural language text and determine a user’s intent. This is essential for chatbots because understanding what the user wants to achieve is the first step in providing an appropriate response. Why not the other options? A. Translator: Translator is used for language translation, not intent recognition. C. Azure Cognitive Search: This service is used to perform searches on structured and unstructured data but does not analyze user intent in conversations. D. Speech: Speech is used for speech-to-text and text-to-speech processing, but it does not determine intent from the user's input. Using Language ensures your chatbot can recognize user intents and respond accordingly."
    },
{
        "id": 300,
        "type": "single_choice",
        "question": "Which type of artificial intelligence (AI) workload provides the ability to classify individual pixels in an image depending on the object that they represent?",
        "options": [
            "A. image analysis.",
            "B. image classification.",
            "C. object detection.",
            "D. semantic segmentation."
        ],
        "answer": "D. semantic segmentation.",
        "explanation": "Correct Answer is D. Semantic segmentation provides the ability to classify individual pixels in an image depending on the object that they represent. The other answer choices also process images, but their outcomes are different. Link: https://learn.microsoft.com/en-us/training/modules/introduction-computer-vision/",
        "images": [],
        "interactivity": null
    },

{
        "id": 301,
        "type": "single_choice",
        "question": "Which AI service can be integrated into chat applications and generate content in the form of text?",
        "options": [
            "A. Azure AI Language.",
            "B. Azure AI Metrics Advisor.",
            "C. Azure AI Vision.",
            "D. Azure OpenAI."
        ],
        "answer": "D. Azure OpenAI.",
        "explanation": "Correct Answer is D. Azure OpenAI is the only service capable of generating text that can be used in chat applications to create conversational experiences. The other workloads are Azure Cognitive Services used for different purposes, but not for generating text used in chat applications. Link: https://learn.microsoft.com/en-us/training/modules/fundamentals-generative-ai/",
        "images": [],
        "interactivity": null
    },

{
        "id": 302,
        "type": "multiple_choice",
        "question": "Which two artificial intelligence (AI) workload scenarios are examples of natural language processing (NLP)? Each correct answer presents a complete solution.",
        "options": [
            "A. extracting handwritten text from online images",
            "B. generating tags and descriptions for images",
            "C. monitoring network traffic for sudden spikes",
            "D. performing sentiment analysis on social media data",
            "E. translating text between different languages from product reviews"
        ],
        "answer": [
            "D. performing sentiment analysis on social media data",
            "E. translating text between different languages from product reviews"
        ],
        "allow_multiple": true,
        "explanation": "Translating text between different languages from product reviews is an NLP workload that uses the Azure AI Translator service and is part of Azure AI Services. It can provide text translation of supported languages in real time. Performing sentiment analysis on social media data is an NLP task that uses the sentiment analysis feature of the Azure AI Service for Language. It can provide sentiment labels—such as negative, neutral, and positive—for text-based sentences and documents. Link: https://learn.microsoft.com/en-us/training/modules/introduction-language/",
        "images": [],
        "interactivity": null
    },

{
        "id": 303,
        "type": "single_choice",
        "question": "Which principle of responsible artificial intelligence (AI) defines the framework of governance and organization principles that meet ethical and legal standards of AI solutions?",
        "options": [
            "A. accountability.",
            "B. fairness.",
            "C. inclusiveness.",
            "D. transparency."
        ],
        "answer": "A. accountability.",
        "explanation": "Correct Answer is A. Accountability defines the framework of governance and organizational principles, which are meant to ensure that AI solutions meet ethical and legal standards that are clearly defined. The other answer choices do not define the framework of governance and organization principles, but provide guidance regarding the ethical and legal aspects of the corresponding standards. Link: https://learn.microsoft.com/en-us/training/modules/get-started-ai-fundamentals/7-responsible-ai?pivots=text",
        "images": [],
        "interactivity": null
    },

{
        "id": 304,
        "type": "multiple_choice",
        "question": "Which two principles of responsible artificial intelligence (AI) are most important when designing an AI system to manage healthcare data? Each correct answer presents part of the solution.",
        "options": [
            "A. accountability",
            "B. fairness",
            "C. inclusiveness",
            "D. privacy and security"
        ],
        "answer": [
            "A. accountability",
            "D. privacy and security"
        ],
        "allow_multiple": true,
        "explanation": "Correct Answers A,D: The accountability principle states that AI systems are designed to meet any ethical and legal standards that are applicable. The system must be designed to ensure that privacy of healthcare data is of the highest importance, including anonymizing data where applicable. The fairness principle ensures systems treat users fairly. The inclusiveness principle ensures AI systems empower people in a positive and engaging way. Link: https://learn.microsoft.com/en-us/training/modules/get-started-ai-fundamentals/7-responsible-ai?pivots=text",
        "images": [],
        "interactivity": null
    },
{
        "id": 305,
        "type": "single_choice",
        "question": "At which layer can you apply content filters to suppress prompts and responses for a responsible generative AI solution?",
        "options": [
            "A. metaprompt and grounding.",
            "B. model.",
            "C. safety system.",
            "D. user experience."
        ],
        "answer": "C. safety system.",
        "explanation": "Correct Answer is C. The safety system layer includes platform‑level configurations and capabilities that help mitigate harm. For example, the Azure OpenAI service includes support for content filters that apply criteria to suppress prompts and responses based on the classification of content into four severity levels (safe, low, medium, and high) for four categories of potential harm (hate, sexual, violence, and self‑harm). Link: https://learn.microsoft.com/en-us/training/modules/get-started-generative-ai-azure/6-observability",
        "images": [],
        "interactivity": null
    },
   

{
        "id": 306,
        "type": "single_choice",
        "question": "Which generative AI model is used to generate images based on natural language prompts?",
        "options": [
            "A. DALL-E",
            "B. Embeddings",
            "C. GPT-3.5",
            "D. GPT-4",
	    "E. Whisper"
        ],
        "answer": "A. DALL-E",
        "explanation": "Correct Answer is A. DALL‑E is a model that can generate images from natural language. GPT‑4 and GPT‑3.5 can understand and generate natural language and code but not images. Embeddings can convert text into numerical vector form to facilitate text similarity. Whisper can transcribe and translate speech to text. Link: https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/dall-e?view=foundry-classic&tabs=gpt-image-1",
        "images": [],
        "interactivity": null
    },

{
        "id": 307,
        "type": "single_choice",
        "question": "You plan to develop an image processing solution that will use DALL‑E as a generative AI model. Which capability is NOT supported by the DALL‑E model?",
        "options": [
            "A. image description",
            "B. image editing",
            "C. image generation",
            "D. image variations"
        ],
        "answer":  "A. image description",
        "explanation": "Correct Answer is A. Image description is not a capability included in the DALL‑E model; therefore, it is not a use case that can be implemented by using DALL‑E, while the other three capabilities are offered by DALL‑E in Azure OpenAI. Link: https://ai.azure.com/catalog/models/dall-e-3",
        "images": [],
        "interactivity": null
    },


        {
        "id": 308,
        "type": "boolean",
        "question": "For each of the following statements, select Yes if the statement is true. Otherwise, select No.",
        "subQuestions": [
            {
                "prompt": "The Difference between large language models (LLMs) and small language models (SLMs) is based on the volume of data.",
                "correctAnswer": true
            },
            {
                "prompt": "The Difference between large language models (LLMs) and small language models (SLMs) is the number of variables in the model.",
                "correctAnswer": true
            }
        ],
        "explanation": "Correct Answer is Yes, Yes.  Reference: https://learn.microsoft.com/en-us/training/modules/get-started-ai-fundamentals/2-generative-ai?pivots=text",
        "images": []
    },
{
        "id": 309,
        "type": "dropdown",
        "question": " _____________  can be used to identify constraints and styles for the responses of a generative AI model.",
        "options": [
          { "label": "Data grounding", "value": "Data grounding" },
          { "label": "Embeddings", "value":  "Embeddings" },
          { "label": "System messages", "value": "System messages" },
          { "label": "Tokenization", "value": "Tokenization" }
        ],
        "correctAnswer": "System messages",
        "placeholder": "— Select an option —",
        "explanation": "Correct Answer is System messages. System messages should be used to set the context for the model by describing expectations. Based on system messages, the model knows how to respond to prompts. The other techniques are also used in generative AI models, but for other use cases.Link https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/advanced-prompt-engineering?view=foundry-classic"
      }


]
  }
  </script>

  <script>
    /* ---------- Polyfill for CSS.escape (some browsers) ---------- */
    (function() {
      if (!window.CSS) window.CSS = {};
      if (!CSS.escape) CSS.escape = function(value) { return String(value).replace(/[^a-zA-Z0-9_\-]/g, '\\$&'); };
    })();

    /* ------------------------- State ------------------------- */
    let questions = [];
    let originalQuestions = [];
    let reviewingMarked = false;
    let idx = 0;

    const userAnswers = {};
    const reviewMarks = {};

    // Timer
    let examDuration = 180 * 60;
    let timerInterval = null;
    let timeLeft = examDuration;
    let examStarted = false;

    // Pass threshold (default 70; can be overridden by JSON top-level passScore)
    let examPassScore = 70;

    const questionContainer = document.getElementById('questionContainer');
    const feedbackContainer = document.getElementById('feedbackContainer');
    const progressPill = document.getElementById('progressPill');

    /* ----------------------- Theme Toggle -------------------- */
    const themeBtn = document.getElementById('themeBtn');
    function setTheme(theme) {
      document.documentElement.setAttribute('data-theme', theme);
      themeBtn.textContent = `Theme: ${theme.charAt(0).toUpperCase() + theme.slice(1)}`;
      try { localStorage.setItem('ai900_theme', theme); } catch {}
    }
    (function initTheme() {
      const saved = (typeof localStorage !== 'undefined') ? localStorage.getItem('ai900_theme') : null;
      setTheme(saved === 'light' ? 'light' : 'dark');
    })();
    themeBtn.addEventListener('click', () => {
      const curr = document.documentElement.getAttribute('data-theme') || 'dark';
      setTheme(curr === 'dark' ? 'light' : 'dark');
    });

    /* ----------------------- VIEW Toggle -------------------- */
    const viewBtn = document.getElementById('viewBtn');
    function setView(view) {
      document.documentElement.setAttribute('data-view', view);
      viewBtn.textContent = `UI: ${view.charAt(0).toUpperCase() + view.slice(1)}`;
      try { localStorage.setItem('ai900_view', view); } catch {}
    }
    (function initView() {
      const saved = (typeof localStorage !== 'undefined') ? localStorage.getItem('ai900_view') : null;
      const defaultView = (window.innerWidth <= 899) ? 'mobile' : 'desktop';
      setView(saved === 'mobile' || saved === 'desktop' ? saved : defaultView);
    })();
    viewBtn.addEventListener('click', () => {
      const curr = document.documentElement.getAttribute('data-view') || 'desktop';
      setView(curr === 'desktop' ? 'mobile' : 'desktop');
    });
    window.addEventListener('resize', () => {
      const saved = (typeof localStorage !== 'undefined') ? localStorage.getItem('ai900_view') : null;
      if (saved) return;
      const auto = (window.innerWidth <= 899) ? 'mobile' : 'desktop';
      setView(auto);
    });




    /* ----------------------- Timer --------------------------- */
    const startExamBtn = document.getElementById('startExamBtn');
    const durationInput = document.getElementById('durationInput');
    const timerPanel = document.getElementById('timerPanel');
    const timerDisplay = document.getElementById('timerDisplay');
    const timerBadge = document.getElementById('timerBadge');
    const pauseBtn = document.getElementById('pauseBtn');
    let isPaused = false;


    function formatTime(secs) {
      const m = Math.floor(secs / 60);
      const s = secs % 60;
      return `${m}:${s.toString().padStart(2, '0')}`;
    }
   

function updateTimerDisplay() {
 
const baseText = `Time left: ${formatTime(timeLeft)}`;
const text = examStarted ? (isPaused ? `${baseText} (paused)` : baseText) : 'Time left: --:--';  

  if (examStarted) {
    if (timerDisplay) timerDisplay.textContent = text;
    if (timerBadge) timerBadge.textContent = text;
  } else {
    if (timerDisplay) timerDisplay.textContent = '';
    if (timerBadge) timerBadge.textContent = 'Time left: --:--';
  }
}

    function disableNav(disabled) {
      ['prevBtn','nextBtn','submitBtn','resetBtn','shuffleBtn','pickJsonBtn','loadJsonBtn','reviewMarkedBtn','calcScoreBtn']
        .forEach(id => { const el = document.getElementById(id); if (el) el.disabled = !!disabled; });
    }
    function showPanels(show) {
      document.getElementById('questionPanel').style.display = show ? '' : 'none';
      document.getElementById('metaPanel').style.display = show ? '' : 'none';
      document.querySelector('.footer').style.display = show ? '' : 'none';
    }

   

function startTimer() {
  examStarted = true;
  isPaused = false;
  updateTimerDisplay();

  // Enable Pause button once the exam starts
  if (pauseBtn) {
    pauseBtn.disabled = false;
   pauseBtn.style.display = '';   // <-- make it visible if it was hidden
    pauseBtn.textContent = 'Pause';
    pauseBtn.setAttribute('aria-pressed', 'false');
  }

  timerInterval = setInterval(() => {
    if (!isPaused) {
      timeLeft--;
      updateTimerDisplay();
      if (timeLeft <= 0) {
        clearInterval(timerInterval);
        disableNav(true);
        if (pauseBtn) {
          pauseBtn.disabled = true;
          pauseBtn.textContent = 'Pause';
          pauseBtn.setAttribute('aria-pressed', 'false');
        }
        showFeedback(false, 'Time is up! You can review marked questions or calculate your score.');
        document.getElementById('reviewMarkedBtn')?.focus();
      }
    }
  }, 1000);
}


function togglePause() {
  if (!examStarted) return;
  isPaused = !isPaused;
  if (isPaused) {
    pauseBtn.textContent = 'Resume';
    pauseBtn.setAttribute('aria-pressed', 'true');

    // Optional: disable navigation while paused
   disableNav(true);
  } else {
    pauseBtn.textContent = 'Pause';
    pauseBtn.setAttribute('aria-pressed', 'false');

    // Optional: re-enable navigation
   disableNav(false);
  }
  updateTimerDisplay();
}

// Wire the button
if (pauseBtn) {
  pauseBtn.addEventListener('click', togglePause);
}

    startExamBtn.addEventListener('click', () => {
      const mins = Math.max(1, Math.min(300, parseInt(durationInput.value, 10) || 180));
      examDuration = mins * 60;
      timeLeft = examDuration;
      startTimer();

      // Show panels and enable UI
      timerPanel.style.display = 'none';
      showPanels(true);
      disableNav(false);

      // Load embedded questions ONLY now
      loadEmbeddedQuestions();
    });



    /* ----------------------- Utilities ----------------------- */
    function updateProgress() {
      progressPill.textContent = `Question ${idx + 1} / ${questions.length || 0}`;
    }
    function shuffleInPlace(arr) {
      for (let i = arr.length - 1; i > 0; i--) {
        const j = Math.floor(Math.random() * (i + 1));
        [arr[i], arr[j]] = [arr[j], arr[i]];
      }
      return arr;
    }
    function clearFeedback() { feedbackContainer.innerHTML = ''; }
    function showFeedback(ok, text) {
      const div = document.createElement('div');
      div.className = `alert ${ok ? 'ok' : 'bad'}`;
      div.textContent = text;
      feedbackContainer.innerHTML = '';
      feedbackContainer.appendChild(div);
    }
    function showExplanation(q) {
      if (!q.explanation) return;
      const div = document.createElement('div');
      div.className = 'alert ok';
      div.textContent = q.explanation;
      feedbackContainer.appendChild(div);
    }

    /* -------------------- Type normalization -------------------- */
    function normalizeType(tRaw) {
      if (!tRaw) return '';
      const t = String(tRaw).trim().toLowerCase();
      const compact = t.replace(/[\s-]+/g, '_');
      switch (compact) {
        case 'single_choice':
        case 'singlechoice':
        case 'radio': return 'single_choice';
        case 'multiple_choice':
        case 'multiplechoice':
        case 'checkbox':
        case 'multi_select':
        case 'multi_selection': return 'multiple_choice';
        case 'drag_and_drop':
        case 'dragdrop':
        case 'drag_anddrop': return 'drag_and_drop';
        case 'hotspot':
        case 'hot_spot': return 'hotspot';
        case 'boolean':
        case 'yes_no':
        case 'true_false': return 'boolean';
        case 'dropdown':
        case 'drop_down':
        case 'select':
        case 'pulldown': return 'dropdown';
        default: return compact;
      }
    }
    function onAnswerChange(qKey, value) { userAnswers[qKey] = value; }

    /* --------------------- Image Handling --------------------- */
    function resolveImageSources(q) {
      const out = [];
      const toDataUrl = (src, mimeHint='image/png') => {
        if (!src) return null;
        const s = String(src);
        if (s.startsWith('data:image/')) return s;
        if (/^https?:\/\//i.test(s)) return s;
        return `data:${mimeHint};base64,${s}`;
      };
      if (q.imageBase64 && typeof q.imageBase64 === 'object') {
        const url = toDataUrl(q.imageBase64.data, q.imageBase64.mime || 'image/png');
        if (url) out.push(url);
      }
      if (q.image) {
        const url = toDataUrl(q.image);
        if (url) out.push(url);
      }
      if (Array.isArray(q.images)) {
        q.images.forEach((item) => {
          if (!item) return;
          if (typeof item === 'string') {
            const url = toDataUrl(item);
            if (url) out.push(url);
          } else if (typeof item === 'object' && item.data) {
            const url = toDataUrl(item.data, item.mime || 'image/png');
            if (url) out.push(url);
          }
        });
      }
      return out;
    }
    function renderQuestionImages(q, container) {
      const urls = resolveImageSources(q);
      if (!urls.length) return;
      const wrap = document.createElement('div'); wrap.className = 'q-images';
      urls.forEach((u, i) => {
        const img = document.createElement('img');
        img.src = u; img.alt = q.question ? `Question image ${i+1}` : `Image ${i+1}`;
        wrap.appendChild(img);
      });
      container.appendChild(wrap);
    }

    /* ----------------------- Mark for Review ------------------ */
    function renderMarkForReview(q, container) {
      const markDiv = document.createElement('div');
      markDiv.className = 'mark-review';
      const chk = document.createElement('input');
      chk.type = 'checkbox';
      chk.id = `review_${q.id}`;
      chk.checked = !!reviewMarks[q.id];
      chk.addEventListener('change', () => { reviewMarks[q.id] = chk.checked; });
      const lbl = document.createElement('label');
      lbl.htmlFor = chk.id;
      lbl.textContent = 'Mark to review later';
      markDiv.appendChild(chk);
      markDiv.appendChild(lbl);
      container.appendChild(markDiv);
    }

    /* ----------------------- Renderers ----------------------- */
    function renderUnsupportedQuestion(question, container) {
      console.error('Unsupported type:', question.type, '→ normalized:', normalizeType(question.type), question);
      container.innerHTML = `<div class="alert bad">Unsupported type: ${question.type}</div>`;
    }

    function renderMultipleChoiceQuestion(q, container) {
      container.innerHTML = '';
      const title = document.createElement('div'); title.className = 'question-text';
      title.textContent = q.question || 'Question'; container.appendChild(title);
      renderQuestionImages(q, container);
      const list = document.createElement('div'); list.className = 'options';
      (q.options || []).forEach((opt, i) => {
        const id = `q${q.id}_opt${i}`;
        const row = document.createElement('label'); row.className = 'option';
        const cb = document.createElement('input'); cb.type = 'checkbox'; cb.id = id; cb.value = opt.value ?? opt;
        const prev = userAnswers[q.id]; if (Array.isArray(prev) && prev.includes(cb.value)) cb.checked = true;
        cb.addEventListener('change', () => {
          let next = Array.isArray(userAnswers[q.id]) ? [...userAnswers[q.id]] : [];
          if (cb.checked) { if (!next.includes(cb.value)) next.push(cb.value); }
          else { next = next.filter(v => v !== cb.value); }
          onAnswerChange(q.id, next);
        });
        row.appendChild(cb); row.appendChild(document.createTextNode(opt.label ?? String(opt)));
        list.appendChild(row);
      });
      container.appendChild(list);
      renderMarkForReview(q, container);
    }

    function renderSingleChoiceQuestion(q, container) {
      container.innerHTML = '';
      const title = document.createElement('div'); title.className = 'question-text';
      title.textContent = q.question || 'Question'; container.appendChild(title);
      renderQuestionImages(q, container);
      const list = document.createElement('div'); list.className = 'options';
      (q.options || []).forEach((opt, i) => {
        const id = `q${q.id}_opt${i}`;
        const row = document.createElement('label'); row.className = 'option';
        const rb = document.createElement('input'); rb.type = 'radio'; rb.name = `q${q.id}`; rb.id = id; rb.value = opt.value ?? opt;
        if (userAnswers[q.id] === rb.value) rb.checked = true;
        rb.addEventListener('change', () => onAnswerChange(q.id, rb.value));
        row.appendChild(rb); row.appendChild(document.createTextNode(opt.label ?? String(opt)));
        list.appendChild(row);
      });
      container.appendChild(list);
      renderMarkForReview(q, container);
    }

    function renderDropdownQuestion(q, container) {
      container.innerHTML = '';
      const title = document.createElement('div'); title.className = 'question-text';
      title.textContent = q.question || 'Question'; container.appendChild(title);
      renderQuestionImages(q, container);
      const wrap = document.createElement('div'); wrap.className = 'options select-wrap';
      const sel = document.createElement('select'); sel.name = `q${q.id}`;
      const hasPlaceholder = q.placeholder !== false;
      const placeholder = (typeof q.placeholder === 'string') ? q.placeholder : '— Select —';
      if (hasPlaceholder) { const opt = document.createElement('option'); opt.value = ''; opt.textContent = placeholder; sel.appendChild(opt); }
      const prev = userAnswers[q.id];
      (q.options || []).forEach((opt) => {
        const o = document.createElement('option');
        if (typeof opt === 'object' && opt !== null) { const v = (opt.value ?? opt.label ?? String(opt)); o.value = v; o.textContent = (opt.label ?? String(v)); }
        else { o.value = String(opt); o.textContent = String(opt); }
        if (prev !== undefined && String(prev) === String(o.value)) { o.selected = true; }
        sel.appendChild(o);
      });
      sel.addEventListener('change', () => onAnswerChange(q.id, sel.value));
      wrap.appendChild(sel); container.appendChild(wrap);
      renderMarkForReview(q, container);
    }

    function renderDragAndDropQuestion(q, container) {
      container.innerHTML = '';
      const title = document.createElement('div'); title.className = 'question-text';
      title.textContent = q.question || 'Drag & Drop'; container.appendChild(title);
      renderQuestionImages(q, container);
      const layout = String(q.layout || '').toLowerCase();
      if (layout === 'bucket') { renderDndBuckets(q, container); } else { renderDndReorder(q, container); }
      renderMarkForReview(q, container);
    }

    function renderDndReorder(q, container) {
      const items = (q.items || []).map(it => (typeof it === 'object' && it)
        ? { label: it.label ?? String(it.value ?? it), value: it.value ?? it.label ?? String(it) }
        : { label: String(it), value: String(it) });
      const list = document.createElement('div'); list.className = 'dnd-list'; container.appendChild(list);
      const currentOrder = Array.isArray(userAnswers[q.id]) ? [...userAnswers[q.id]] : items.map(it => it.value);
      const byValue = new Map(items.map(it => [it.value, it.label]));
      currentOrder.forEach(val => { const label = byValue.get(val) ?? String(val); const el = createDraggableItem(q.id, val, label); list.appendChild(el); });
      list.querySelectorAll('.dnd-item').forEach(el => {
        el.addEventListener('dragstart', (ev) => { ev.dataTransfer.setData('text/plain', el.dataset.value); el.classList.add('dragging'); });
        el.addEventListener('dragend', () => el.classList.remove('dragging'));
        el.addEventListener('dragover', (ev) => { ev.preventDefault(); el.classList.add('drag-over'); });
        el.addEventListener('dragleave', () => el.classList.remove('drag-over'));
        el.addEventListener('drop', (ev) => {
          ev.preventDefault();
          const draggedVal = ev.dataTransfer.getData('text/plain');
          const dragEl = list.querySelector(`.dnd-item[data-value="${CSS.escape(draggedVal)}"]`);
          el.classList.remove('drag-over');
          if (!dragEl || dragEl === el) return;
          const rect = el.getBoundingClientRect();
          const before = (ev.clientY - rect.top) < rect.height / 2;
          list.insertBefore(dragEl, before ? el : el.nextSibling);
          const order = Array.from(list.querySelectorAll('.dnd-item')).map(n => n.dataset.value);
          onAnswerChange(q.id, order);
        });
      });
      const initialOrder = Array.from(list.querySelectorAll('.dnd-item')).map(n => n.dataset.value);
      if (!Array.isArray(userAnswers[q.id]) || userAnswers[q.id].length !== initialOrder.length) {
        onAnswerChange(q.id, initialOrder);
      }
    }

    function renderDndBuckets(q, container) {
      const items = (q.items || []).map(it => (typeof it === 'object' && it)
        ? { label: it.label ?? String(it.value ?? it), value: it.value ?? it.label ?? String(it) }
        : { label: String(it), value: String(it) });
      const buckets = (q.buckets || []).map(b => ({ id: b.id ?? String(b.label ?? b), label: b.label ?? String(b.id ?? b) }));
      let map = userAnswers[q.id]; if (!map || typeof map !== 'object') { map = {}; onAnswerChange(q.id, map); }

      const unassignedWrap = document.createElement('div'); unassignedWrap.className = 'dnd-buckets'; container.appendChild(unassignedWrap);
      const unassignedTitle = document.createElement('div'); unassignedTitle.className = 'dnd-bucket-title'; unassignedTitle.textContent = 'Items'; unassignedWrap.appendChild(unassignedTitle);
      const unassignedZone = document.createElement('div'); unassignedZone.className = 'dnd-bucket dnd-unassigned';
      const unassignedItems = document.createElement('div'); unassignedItems.className = 'dnd-bucket-items';
      unassignedZone.appendChild(unassignedItems); unassignedWrap.appendChild(unassignedZone);

      const bucketsWrap = document.createElement('div'); bucketsWrap.className = 'dnd-buckets'; container.appendChild(bucketsWrap);
      buckets.forEach(b => {
        const bucketEl = document.createElement('div'); bucketEl.className = 'dnd-bucket';
        const titleEl = document.createElement('div'); titleEl.className = 'dnd-bucket-title'; titleEl.textContent = b.label; bucketEl.appendChild(titleEl);
        const itemsEl = document.createElement('div'); itemsEl.className = 'dnd-bucket-items'; bucketEl.appendChild(itemsEl);

        [bucketEl, unassignedZone].forEach(zone => {
          zone.addEventListener('dragover', (ev) => { ev.preventDefault(); zone.classList.add('over'); });
          zone.addEventListener('dragleave', () => zone.classList.remove('over'));
          zone.addEventListener('drop', (ev) => {
            ev.preventDefault(); zone.classList.remove('over');
            const val = ev.dataTransfer.getData('text/plain'); if (!val) return;
            const node = container.querySelector(`.dnd-item[data-value="${CSS.escape(val)}"]`); if (!node) return;
            const targetItems = zone.querySelector('.dnd-bucket-items') ?? zone; targetItems.appendChild(node);
            const isUnassigned = zone.classList.contains('dnd-unassigned'); map[val] = isUnassigned ? '' : b.id; onAnswerChange(q.id, { ...map });
          });
        });

        bucketsWrap.appendChild(bucketEl); b._itemsEl = itemsEl;
      });

      items.forEach(it => {
        const el = createDraggableItem(q.id, it.value, it.label);
        const bucketId = map[it.value];
        if (!bucketId) { unassignedItems.appendChild(el); return; }
        const bucket = buckets.find(b => b.id === bucketId);
        (bucket?.['_itemsEl'] ?? unassignedItems).appendChild(el);
      });
    }

    function createDraggableItem(qId, value, label) {
      const el = document.createElement('div'); el.className = 'dnd-item';
      el.draggable = true; el.dataset.value = String(value); el.textContent = label ?? String(value);
      el.addEventListener('dragstart', (ev) => { ev.dataTransfer.setData('text/plain', el.dataset.value); el.classList.add('dragging'); });
      el.addEventListener('dragend', () => el.classList.remove('dragging'));
      return el;
    }

    function renderHotspotQuestion(q, container) {
      container.innerHTML = '';
      const title = document.createElement('div'); title.className = 'question-text';
      title.textContent = q.question || 'Hotspot'; container.appendChild(title);
      renderQuestionImages(q, container);
      const areas = (q.interactivity && Array.isArray(q.interactivity.hotspot_areas)) ? q.interactivity.hotspot_areas : [];
      if (areas.length > 1) {
        areas.forEach((area, i) => {
          const areaDiv = document.createElement('div'); areaDiv.className = 'hotspot-area';
          const areaTitle = document.createElement('div'); areaTitle.className = 'hotspot-area-title'; areaTitle.textContent = area.prompt || `Area ${i + 1}`; areaDiv.appendChild(areaTitle);
          const list = document.createElement('div'); list.className = 'options';
          const groupName = `hotspot_${q.id}_${i}`; const prev = userAnswers[groupName];
          (q.options || []).forEach((opt, j) => {
            const id = `${groupName}_opt${j}`;
            const row = document.createElement('label'); row.className = 'option';
            const rb = document.createElement('input'); rb.type = 'radio'; rb.name = groupName; rb.id = id; rb.value = opt;
            if (prev === opt) rb.checked = true;
            rb.addEventListener('change', () => onAnswerChange(groupName, rb.value));
            row.appendChild(rb); row.appendChild(document.createTextNode(opt)); list.appendChild(row);
          });
          areaDiv.appendChild(list); container.appendChild(areaDiv);
        });
      } else {
        const promptText = areas[0]?.prompt || '';
        if (promptText) { const promptDiv = document.createElement('div'); promptDiv.className = 'hotspot-prompt'; promptDiv.textContent = promptText; container.appendChild(promptDiv); }
        const list = document.createElement('div'); list.className = 'options';
        const groupName = `hotspot_${q.id}`; const prev = userAnswers[q.id];
        (q.options || []).forEach((opt, i) => {
          const id = `${groupName}_opt${i}`;
          const row = document.createElement('label'); row.className = 'option';
          const rb = document.createElement('input'); rb.type = 'radio'; rb.name = groupName; rb.id = id; rb.value = opt;
          if (prev === opt) rb.checked = true;
          rb.addEventListener('change', () => onAnswerChange(q.id, rb.value));
          row.appendChild(rb); row.appendChild(document.createTextNode(opt)); list.appendChild(row);
        });
        container.appendChild(list);
      }
      renderMarkForReview(q, container);
    }

    function mapYesNoOutbound(val) { return val === 'yes'; }
    function mapYesNoInbound(stored) {
      if (stored === true) return 'yes';
      if (stored === false) return 'no';
      if (typeof stored === 'string') { const s = stored.toLowerCase(); if (s === 'yes' || s === 'true') return 'yes'; if (s === 'no' || s === 'false') return 'no'; }
      return '';
    }
    function renderYesNoQuestion(question, container) {
      container.innerHTML = '';
      const wrapper = document.createElement('div'); wrapper.className = 'yn-group';
      const label = document.createElement('div'); label.className = 'question-text'; label.textContent = question.question ?? 'Yes or No?';
      wrapper.appendChild(label); renderQuestionImages(question, wrapper);

      if (Array.isArray(question.subQuestions)) {
        question.subQuestions.forEach((subQ, i) => {
          const qKey = `${question.id}_${i}`; const priorKey = mapYesNoInbound(userAnswers[qKey]);
          const item = document.createElement('div'); item.className = 'yn-item';
          const prompt = document.createElement('div'); prompt.textContent = subQ.prompt ?? `Statement ${i + 1}`; item.appendChild(prompt);
          const row = document.createElement('div'); row.className = 'yn-options';
          const yesLbl = document.createElement('label'); yesLbl.className = 'yn-option';
          const yesInput = document.createElement('input'); yesInput.type = 'radio'; yesInput.name = qKey; yesInput.value = 'yes'; yesInput.checked = priorKey === 'yes';
          yesInput.addEventListener('change', () => onAnswerChange(qKey, mapYesNoOutbound('yes')));
          yesLbl.appendChild(yesInput); yesLbl.appendChild(document.createTextNode('Yes'));
          const noLbl = document.createElement('label'); noLbl.className = 'yn-option';
          const noInput = document.createElement('input'); noInput.type = 'radio'; noInput.name = qKey; noInput.value = 'no'; noInput.checked = priorKey === 'no';
          noInput.addEventListener('change', () => onAnswerChange(qKey, mapYesNoOutbound('no')));
          noLbl.appendChild(noInput); noLbl.appendChild(document.createTextNode('No'));
          row.appendChild(yesLbl); row.appendChild(noLbl); item.appendChild(row); wrapper.appendChild(item);
        });
      } else {
        const qKey = `${question.id}`; const priorKey = mapYesNoInbound(userAnswers[qKey]);
        const row = document.createElement('div'); row.className = 'yn-options';
        const yesLbl = document.createElement('label'); const yesInput = document.createElement('input');
        yesInput.type = 'radio'; yesInput.name = qKey; yesInput.value = 'yes'; yesInput.checked = priorKey === 'yes';
        yesInput.addEventListener('change', () => onAnswerChange(qKey, mapYesNoOutbound('yes')));
        yesLbl.appendChild(yesInput); yesLbl.appendChild(document.createTextNode('Yes'));
        const noLbl = document.createElement('label'); const noInput = document.createElement('input');
        noInput.type = 'radio'; noInput.name = qKey; noInput.value = 'no'; noInput.checked = priorKey === 'no';
        noInput.addEventListener('change', () => onAnswerChange(qKey, mapYesNoOutbound('no')));
        noLbl.appendChild(noInput); noLbl.appendChild(document.createTextNode('No'));
        row.appendChild(yesLbl); row.appendChild(noLbl); wrapper.appendChild(row);
      }
      questionContainer.appendChild(wrapper);
      renderMarkForReview(question, wrapper);
    }

    /* ===================== Bucket Question (NEW type) ===================== */
    function normalizeBucketItems(q) {
      const src = Array.isArray(q.items) ? q.items : (Array.isArray(q.options) ? q.options : []);
      return src.map(it => {
        if (typeof it === 'object' && it) {
          return {
            label: it.label ?? String(it.value ?? it.id ?? it),
            value: it.value ?? it.id ?? it.label ?? String(it)
          };
        }
        return { label: String(it), value: String(it) };
      });
    }
    function normalizeBuckets(q) {
      return (q.buckets || []).map(b => ({
        id: String(b.id ?? b.label ?? b),
        label: String(b.label ?? b.id ?? b),
        accepts: Array.isArray(b.accepts) ? b.accepts.map(v => String(v)) : null
      }));
    }
    function buildCorrectMapping(q) {
      const m = q.correctMapping || {};
      const out = {};
      const buckets = normalizeBuckets(q);
      const idSet = new Set(buckets.map(b => b.id.toLowerCase()));
      const labelToId = new Map(buckets.map(b => [b.label.toLowerCase(), b.id]));
      Object.keys(m).forEach(k => {
        const kNorm = String(k).toLowerCase();
        const bucketId = idSet.has(kNorm) ? buckets.find(b => b.id.toLowerCase() === kNorm)?.id
                       : (labelToId.get(kNorm) || k);
        out[bucketId] = new Set((m[k] || []).map(v => String(v)));
      });
      return out; // { bucketId: Set(items) }
    }
    function createLibraryItem(value, label) {
      const el = document.createElement('div');
      el.className = 'dnd-item';
      el.draggable = true;
      el.dataset.value = String(value);
      el.textContent = label ?? String(value);
      el.addEventListener('dragstart', (ev) => {
        ev.dataTransfer.setData('text/plain', el.dataset.value);
        el.classList.add('dragging');
      });
      el.addEventListener('dragend', () => el.classList.remove('dragging'));
      return el;
    }
    function createBucketClone(value, label, isValid, onRemove) {
      const el = document.createElement('div');
      el.className = 'dnd-item clone';
      el.dataset.value = String(value);
      el.textContent = label ?? String(value);
      if (!isValid) el.classList.add('invalid');
      const removeBtn = document.createElement('button');
      removeBtn.type = 'button';
      removeBtn.className = 'remove-btn';
      removeBtn.textContent = '×';
      removeBtn.title = 'Remove';
      removeBtn.addEventListener('click', () => {
        el.parentElement?.removeChild(el);
        if (typeof onRemove === 'function') onRemove(value);
      });
      el.appendChild(removeBtn);
      return el;
    }
    function computeValidity(bucket, itemValue, correctMap) {
      if (Array.isArray(bucket.accepts)) {
        return bucket.accepts.map(v => String(v).toLowerCase())
                            .includes(String(itemValue).toLowerCase());
      }
      if (correctMap && correctMap[bucket.id] instanceof Set) {
        return correctMap[bucket.id].has(String(itemValue));
      }
      return true;
    }
    function renderBucketQuestion(q, container) {
      container.innerHTML = '';
      const title = document.createElement('div');
      title.className = 'question-text';
      title.textContent = q.question || 'Bucket';
      container.appendChild(title);
      renderQuestionImages(q, container);
      const items = normalizeBucketItems(q);
      const buckets = normalizeBuckets(q);
      const correctMap = buildCorrectMapping(q);
      const policy = {
        allowUnassignedItems: q.selectionPolicy?.allowUnassignedItems !== false,
        allowDuplicateAssignments: q.selectionPolicy?.allowDuplicateAssignments !== false
      };
      let ans = userAnswers[q.id];
      if (!ans || typeof ans !== 'object' || Array.isArray(ans)) {
        ans = {}; onAnswerChange(q.id, ans);
      }
      const libraryWrap = document.createElement('div');
      libraryWrap.className = 'dnd-buckets';
      container.appendChild(libraryWrap);
      const libraryTitle = document.createElement('div');
      libraryTitle.className = 'dnd-bucket-title';
      libraryTitle.textContent = 'Items';
      libraryWrap.appendChild(libraryTitle);
      const libraryZone = document.createElement('div');
      libraryZone.className = 'dnd-bucket dnd-unassigned';
      const libraryItems = document.createElement('div');
      libraryItems.className = 'dnd-bucket-items';
      libraryZone.appendChild(libraryItems);
      libraryWrap.appendChild(libraryZone);
      items.forEach(it => { libraryItems.appendChild(createLibraryItem(it.value, it.label)); });

      const bucketsWrap = document.createElement('div');
      bucketsWrap.className = 'dnd-buckets';
      container.appendChild(bucketsWrap);

      buckets.forEach(b => {
        const bucketEl = document.createElement('div');
        bucketEl.className = 'dnd-bucket';
        const titleEl = document.createElement('div');
        titleEl.className = 'dnd-bucket-title';
        titleEl.textContent = b.label;
        bucketEl.appendChild(titleEl);
        const itemsEl = document.createElement('div');
        itemsEl.className = 'dnd-bucket-items';
        bucketEl.appendChild(itemsEl);

        const existing = Array.isArray(ans[b.id]) ? ans[b.id] : [];
        existing.forEach(val => {
          const src = items.find(i => String(i.value) === String(val));
          const isValid = computeValidity(b, val, correctMap);
          const clone = createBucketClone(val, src?.label ?? String(val), isValid, (removedVal) => {
            ans[b.id] = (ans[b.id] || []).filter(v => String(v) !== String(removedVal));
            onAnswerChange(q.id, { ...ans });
          });
          itemsEl.appendChild(clone);
        });

        ['dragover', 'dragleave', 'drop'].forEach(name => {
          bucketEl.addEventListener(name, ev => {
            if (name === 'dragover') {
              ev.preventDefault(); bucketEl.classList.add('over');
            } else if (name === 'dragleave') {
              bucketEl.classList.remove('over');
            } else if (name === 'drop') {
              ev.preventDefault(); bucketEl.classList.remove('over');
              const val = ev.dataTransfer.getData('text/plain');
              if (!val) return;
              const src = items.find(i => String(i.value) === String(val));
              const label = src?.label ?? String(val);
              const isValid = computeValidity(b, val, correctMap);
              const arr = ans[b.id] || [];
              if (!policy.allowDuplicateAssignments) {
                const already = arr.some(v => String(v) === String(val));
                if (already) { showFeedback(false, `Item "${label}" is already in bucket "${b.label}".`); return; }
              }
              const clone = createBucketClone(val, label, isValid, (removedVal) => {
                ans[b.id] = (ans[b.id] || []).filter(v => String(v) !== String(removedVal));
                onAnswerChange(q.id, { ...ans });
              });
              itemsEl.appendChild(clone);
              ans[b.id] = [...arr, val];
              onAnswerChange(q.id, { ...ans });
            }
          });
        });

        bucketsWrap.appendChild(bucketEl);
      });

      renderMarkForReview(q, container);
    }

    /* ---------------------- Dispatcher ---------------------- */
    function renderQuestion(question, container) {
      clearFeedback();
      const t = normalizeType(question.type);
      if (t === 'multiple_choice') return renderMultipleChoiceQuestion(question, container);
      if (t === 'single_choice')   return renderSingleChoiceQuestion(question, container);
      if (t === 'dropdown')        return renderDropdownQuestion(question, container);
      if (t === 'drag_and_drop')   return renderDragAndDropQuestion(question, container);
      if (t === 'bucket')          return renderBucketQuestion(question, container);
      if (t === 'hotspot')         return renderHotspotQuestion(question, container);
      if (t === 'boolean')         return renderYesNoQuestion(question, container);
      return renderUnsupportedQuestion(question, container);
    }

    /* ---------------- Validation / Grading ------------------ */
    function isAnswered(q) {
      const t = normalizeType(q.type);
      if (t === 'hotspot') {
        const areas = (q.interactivity && Array.isArray(q.interactivity.hotspot_areas)) ? q.interactivity.hotspot_areas : [];
        if (areas.length > 1) return areas.every((_, i) => { const v = userAnswers[`hotspot_${q.id}_${i}`]; return typeof v === 'string' && v.length > 0; });
        const v = userAnswers[q.id]; return typeof v === 'string' && v.length > 0;
      }
      if (t === 'boolean') {
        if (Array.isArray(q.subQuestions)) return q.subQuestions.every((_, i) => {
          const v = userAnswers[`${q.id}_${i}`]; return v === true || v === false || v === 'Yes' || v === 'No';
        });
        const v = userAnswers[`${q.id}`]; return v === true || v === false || v === 'Yes' || v === 'No';
      }
      if (t === 'dropdown') { const v = userAnswers[q.id]; return typeof v === 'string' && v.length > 0; }
      if (t === 'drag_and_drop') {
        const layout = String(q.layout || '').toLowerCase();
        if (layout === 'bucket') {
          const items = (q.items || []).map(it => (typeof it === 'object' && it) ? (it.value ?? it.label ?? String(it)) : String(it));
          const map = userAnswers[q.id] || {}; return items.every(v => typeof map[v] === 'string' && map[v].length > 0);
        }
        const order = userAnswers[q.id]; return Array.isArray(order) && order.length === (q.items || []).length;
      }
      if (t === 'bucket') {
        const map = userAnswers[q.id] || {};
        const totalAssigned = Object.values(map).reduce((sum, arr) => sum + (Array.isArray(arr) ? arr.length : 0), 0);
        const requiresAll = q.selectionPolicy?.requireAllItemsAssigned === true;
        if (requiresAll) {
          const items = normalizeBucketItems(q);
          const uniqueAssigned = new Set(Object.values(map).flat().map(v => String(v)));
          return uniqueAssigned.size >= items.length;
        }
        return totalAssigned > 0;
      }
      const v = userAnswers[q.id]; if (Array.isArray(v)) return v.length > 0;
      return v !== undefined && v !== null && String(v).trim?.() !== '';
    }

    // Helper: normalize option values to string for comparisons
  function normVal(v) {
    if (v === undefined || v === null) return '';
    if (typeof v === 'object') {
      // Support {label, value} fallbacks
      const candidate = v.value ?? v.label ?? String(v);
      return String(candidate).trim().toLowerCase();
    }
    return String(v).trim().toLowerCase();
  }

  function gradeCurrent() {
    const q = questions[idx];
    const t = normalizeType(q.type);

    // ---- SINGLE CHOICE ----
    if (t === 'single_choice') {
      const sel = userAnswers[q.id];
      const expected = q.answer ?? q.correctAnswer;
      const ok = expected !== undefined && normVal(sel) === normVal(expected);
      showFeedback(ok, ok ? 'Correct' : 'Incorrect');
      showExplanation(q);
      return;
    }

    // ---- MULTIPLE CHOICE ----
    if (t === 'multiple_choice') {
      // Require exact match (same items, same count), ignoring order — same rule as calcScoreAll()
      const sel = Array.isArray(userAnswers[q.id]) ? userAnswers[q.id] : [];
      let expected = q.answers ?? q.correctAnswers ?? q.correctAnswer ?? q.answer;
      if (!Array.isArray(expected)) expected = [expected].filter(Boolean);

      const selNorm = sel.map(normVal).filter(Boolean);
      const expNorm = expected.map(normVal).filter(Boolean);

      let ok = false;
      if (expNorm.length) {
        if (selNorm.length === expNorm.length) {
          ok = expNorm.every(v => selNorm.includes(v));
        }
      }
      showFeedback(ok, ok ? 'Correct' : 'Incorrect');
      showExplanation(q);
      return;
    }

    // ---- DROPDOWN ----
    if (t === 'dropdown') {
      const sel = userAnswers[q.id];
      let expected = q.correctAnswer ?? q.answer;
      let expectedValues = [];
      if (Array.isArray(expected)) {
        expectedValues = expected.map(normVal).filter(Boolean);
      } else if (expected !== undefined) {
        expectedValues = [normVal(expected)];
      }
      const ok = expectedValues.length ? expectedValues.includes(normVal(sel)) : false;
      showFeedback(ok, ok ? 'Correct' : 'Incorrect');
      showExplanation(q);
      return;
    }

    // ---- BOOLEAN (Yes/No) ----
    if (t === 'boolean') {
      let ok = false;
      if (Array.isArray(q.subQuestions)) {
        ok = q.subQuestions.every((subQ, i) => {
          const v = userAnswers[`${q.id}_${i}`];
          const boolV = (v === true || v === 'Yes' || v === 'yes');
          const expected = !!subQ.correctAnswer;
          return boolV === expected;
        });
      } else if (typeof q.correctAnswer !== 'undefined' || typeof q.answer !== 'undefined') {
        const v = userAnswers[`${q.id}`];
        const boolV = (v === true || v === 'Yes' || v === 'yes');
        const expected = !!(q.correctAnswer ?? q.answer);
        ok = boolV === expected;
      }
      showFeedback(ok, ok ? 'Correct' : 'Incorrect');
      showExplanation(q);
      return;
    }

    // ---- HOTSPOT ----
    if (t === 'hotspot') {
      const areas = (q.interactivity && Array.isArray(q.interactivity.hotspot_areas))
        ? q.interactivity.hotspot_areas : [];

      if (areas.length > 1) {
        const allOk = areas.every((area, i) => {
          const sel = userAnswers[`hotspot_${q.id}_${i}`];
          return normVal(sel) === normVal(area.answer);
        });
        showFeedback(allOk, allOk ? 'Correct' : 'Incorrect');
        showExplanation(q);
        return;
      } else {
        const sel = userAnswers[q.id];
        const expected = (typeof q.answer !== 'undefined') ? q.answer : (areas[0]?.answer);
        const ok = normVal(sel) === normVal(expected);
        showFeedback(ok, ok ? 'Correct' : 'Incorrect');
        showExplanation(q);
        return;
      }
    }

    // ---- DRAG & DROP (Reorder) ----
    if (t === 'drag_and_drop') {
      const layout = String(q.layout || '').toLowerCase();
      if (layout === 'bucket') {
        // This branch is kept for backward compatibility with any old drag_and_drop bucket layout
        const map = userAnswers[q.id] || {};
        const correct = q.correctMapping || {};
        const items = (q.items || []).map(it =>
          (typeof it === 'object' && it) ? (it.value ?? it.label ?? String(it)) : String(it));
        const allOk = items.every(v => {
          const expected = normVal(correct[v] ?? '');
          const got = normVal(map[v] ?? '');
          return expected && expected === got;
        });
        showFeedback(allOk, allOk ? 'Correct' : 'Incorrect');
        showExplanation(q);
        return;
      } else {
        const order = userAnswers[q.id] || [];
        const expected = (q.correctOrder || []).map(normVal);
        const got = order.map(normVal);
        const ok = expected.length && expected.every((v, i) => v === got[i]);
        showFeedback(ok, ok ? 'Correct' : 'Incorrect');
        showExplanation(q);
        return;
      }
    }

    // ---- BUCKET (New type) ----
    if (t === 'bucket') {
      const map = userAnswers[q.id] || {};
      const correctMap = buildCorrectMapping(q);
      const buckets = normalizeBuckets(q);
      const items = normalizeBucketItems(q);

      // If requireAllItemsAssigned is true, enforce assignment completeness
      const requireAll = q.selectionPolicy?.requireAllItemsAssigned === true;
      const uniqueAssigned = new Set(Object.values(map).flat().map(v => String(v)));
      const allAssignedOk = !requireAll || (uniqueAssigned.size >= items.length);

      const allValid = buckets.every(b => {
        const arr = Array.isArray(map[b.id]) ? map[b.id] : [];
        return arr.every(val => computeValidity(b, val, correctMap));
      });

      const ok = allAssignedOk && allValid;
      showFeedback(ok, ok ? 'Correct' : 'Incorrect');
      showExplanation(q);
      return;
    }

    // Fallback (unsupported)
    showFeedback(false, 'Unsupported question type.');
  }



    /* ------------ Calculate Score (all questions) + PASS/FAIL ------------- */
    function calcScoreAll() {
      if (!questions.length) {
        showFeedback(false, 'No questions loaded.');
        return;
      }
      let totalQuestions = questions.length;
      let correctCount = 0;

      function isCorrect(q) {
        const t = normalizeType(q.type);

        if (t === 'single_choice') {
          const sel = userAnswers[q.id];
          const expected = q.answer ?? q.correctAnswer;
          if (typeof expected === 'undefined') return false;
          return String(sel).trim().toLowerCase() === String(expected).trim().toLowerCase();
        }

        if (t === 'multiple_choice') {
          const sel = userAnswers[q.id] || [];
          let expected = q.answers ?? q.correctAnswers ?? q.correctAnswer ?? q.answer;
          if (!Array.isArray(expected)) expected = [expected].filter(Boolean);
          const norm = (arr) => arr.map(v => (typeof v === 'object' && v !== null) ? (v.value ?? v.label ?? String(v)) : String(v)).map(s => s.trim().toLowerCase());
          const selNorm = norm(Array.isArray(sel) ? sel : [sel]);
          const expNorm = norm(expected);
          if (!expNorm.length) return false;
          const sameLen = selNorm.length === expNorm.length;
          const allIncluded = expNorm.every(v => selNorm.includes(v));
          return sameLen && allIncluded;
        }

        if (t === 'dropdown') {
          const sel = userAnswers[q.id];
          let expected = q.correctAnswer ?? q.answer;
          let expectedValues = [];
          if (Array.isArray(expected)) expectedValues = expected.map(e => (typeof e === 'object' && e !== null) ? (e.value ?? e.label ?? String(e)) : String(e));
          else if (typeof expected === 'object' && expected !== null) expectedValues = [ expected.value ?? expected.label ?? String(expected) ];
          else if (typeof expected !== 'undefined') expectedValues = [ String(expected) ];
          return expectedValues.length ? expectedValues.some(ev => String(ev).trim().toLowerCase() === String(sel).trim().toLowerCase()) : false;
        }

        if (t === 'boolean') {
          if (Array.isArray(q.subQuestions)) {
            return q.subQuestions.every((subQ, i) => {
              const v = userAnswers[`${q.id}_${i}`];
              const expected = !!subQ.correctAnswer;
              const boolV = (v === true || v === 'Yes' || v === 'yes');
              return boolV === expected;
            });
          } else {
            const v = userAnswers[`${q.id}`];
            const expected = !!(q.correctAnswer ?? q.answer);
            const boolV = (v === true || v === 'Yes' || v === 'yes');
            return boolV === expected;
          }
        }

        if (t === 'hotspot') {
          const areas = (q.interactivity && Array.isArray(q.interactivity.hotspot_areas)) ? q.interactivity.hotspot_areas : [];
          if (areas.length > 1) {
            return areas.every((area, i) => {
              const sel = userAnswers[`hotspot_${q.id}_${i}`];
              return String(sel).trim().toLowerCase() === String(area.answer).trim().toLowerCase();
            });
          } else {
            const sel = userAnswers[q.id];
            const expected = (typeof q.answer !== 'undefined') ? q.answer : (areas[0]?.answer);
            return String(sel).trim().toLowerCase() === String(expected).trim().toLowerCase();
          }
        }

        if (t === 'drag_and_drop') {
          const layout = String(q.layout || '').toLowerCase();
          if (layout === 'bucket') {
            const map = userAnswers[q.id] || {};
            const correct = q.correctMapping || {};
            const items = (q.items || []).map(it => (typeof it === 'object' && it) ? (it.value ?? it.label ?? String(it)) : String(it));
            return items.every(v => {
              const expected = String(correct[v] ?? '');
              const got = String(map[v] ?? '');
              return expected && expected.toLowerCase() === got.toLowerCase();
            });
          }
          const order = userAnswers[q.id] || [];
          const expected = (q.correctOrder || []).map(v => String(v).trim().toLowerCase());
          const got = order.map(v => String(v).trim().toLowerCase());
          return expected.length && expected.every((v, i) => v === got[i]);
        }

        if (t === 'bucket') {
          const map = userAnswers[q.id] || {};
          const correctMap = buildCorrectMapping(q);
          const buckets = normalizeBuckets(q);
          const items = normalizeBucketItems(q);
          const uniqueAssigned = new Set(Object.values(map).flat().map(v => String(v)));
          const requireAll = q.selectionPolicy?.requireAllItemsAssigned === true;
          const allAssignedOk = !requireAll || (uniqueAssigned.size >= items.length);
          const allValid = buckets.every(b => {
            const arr = Array.isArray(map[b.id]) ? map[b.id] : [];
            return arr.every(val => computeValidity(b, val, correctMap));
          });
          return allAssignedOk && allValid;
        }

        return false;
      }

      questions.forEach(q => { if (isCorrect(q)) correctCount++; });

      const percent = Math.round((correctCount / totalQuestions) * 100);
      const passed = percent >= examPassScore;
      const summaryText = `Score: ${correctCount} / ${totalQuestions} (${percent}%)`;
      const answered = questions.filter(isAnswered).length;
      document.getElementById('resultSummary').innerHTML =
        `<div class="pill">Answered: ${answered} / ${totalQuestions}</div>
         <div class="pill" style="margin-top:.5rem;">${summaryText}</div>
         <div class="pill ${passed ? 'pass' : 'fail'}" style="margin-top:.5rem;">${passed ? 'PASS' : 'FAIL'} (threshold: ${examPassScore}%)</div>`;
      showFeedback(passed, summaryText);
    }

    function summarize() {
      const answered = questions.filter(isAnswered).length;
      const total = questions.length;
      document.getElementById('resultSummary').innerHTML = `<div class="pill">Answered: ${answered} / ${total}</div>`;
    }

    /* ------------------ Loading JSON Sources ----------------- */
    function loadEmbeddedQuestions() {
      try {
        const raw = document.getElementById('embedded-questions').textContent;
        const data = JSON.parse(raw);

        // Read passScore if provided
        if (typeof data.passScore === 'number' && !Number.isNaN(data.passScore)) {
          examPassScore = Math.max(0, Math.min(100, Math.round(data.passScore)));
        } else {
          examPassScore = 70; // default
        }

        questions = Array.isArray(data.questions) ? [...data.questions] : [];
        originalQuestions = [...questions];
        reviewingMarked = false;
        idx = 0;
        updateProgress();
        if (questions.length) {
          renderQuestion(questions[idx], questionContainer);
          ['shuffleBtn','reviewMarkedBtn','submitBtn','resetBtn','prevBtn','nextBtn','calcScoreBtn','pickJsonBtn','loadJsonBtn']
            .forEach(id => { const el = document.getElementById(id); if (el) el.disabled = false; });
        } else {
          showFeedback(false, 'No questions in embedded JSON.');
        }
        summarize();
      } catch (err) {
        console.error('Embedded JSON parse error:', err);
        showFeedback(false, 'Embedded questions JSON is invalid. Please fix or load a JSON file.');
      }
    }

    function loadQuestionsFromObject(data) {
      if (!data || !Array.isArray(data.questions)) {
        showFeedback(false, 'Invalid JSON: expected an object with "questions" array.');
        return;
      }
      if (typeof data.passScore === 'number' && !Number.isNaN(data.passScore)) {
        examPassScore = Math.max(0, Math.min(100, Math.round(data.passScore)));
      }

      questions = [...data.questions]; originalQuestions = [...questions]; reviewingMarked = false;
      idx = 0; updateProgress(); renderQuestion(questions[idx], questionContainer); summarize();
      showFeedback(true, 'JSON file loaded.');
      ['shuffleBtn','reviewMarkedBtn','submitBtn','resetBtn','prevBtn','nextBtn','calcScoreBtn']
        .forEach(id => { const el = document.getElementById(id); if (el) el.disabled = false; });
    }

    // File input
    const fileInput = document.getElementById('fileInput');
    document.getElementById('pickJsonBtn').addEventListener('click', () => fileInput.click());
    fileInput.addEventListener('change', async (e) => {
      const file = e.target.files?.[0]; if (!file) return;
      try { const text = await file.text(); const data = JSON.parse(text); loadQuestionsFromObject(data); }
      catch (err) { console.error(err); showFeedback(false, 'Failed to read JSON file. Ensure it is valid JSON.'); }
      finally { fileInput.value = ''; }
    });

   

/* ----------------------- Review Marked (shared handler) ------------------- */
function handleReviewMarked() {
  const markedIds = Object.keys(reviewMarks).filter(id => reviewMarks[id]);
  if (!markedIds.length) { showFeedback(false, 'No questions marked for review.'); return; }
  questions = originalQuestions.filter(q => markedIds.includes(String(q.id)));
  reviewingMarked = true; idx = 0; updateProgress(); renderQuestion(questions[idx], questionContainer);
  showFeedback(true, `Reviewing ${questions.length} marked question(s).`);
}

// Desktop meta panel button
const reviewMarkedBtn = document.getElementById('reviewMarkedBtn');
if (reviewMarkedBtn) reviewMarkedBtn.addEventListener('click', handleReviewMarked);

// Mobile footer button
const reviewMarkedBtnMobile = document.getElementById('reviewMarkedBtnMobile');
if (reviewMarkedBtnMobile) reviewMarkedBtnMobile.addEventListener('click', handleReviewMarked);

    /* ----------------------- Controls ----------------------- */
    document.getElementById('loadJsonBtn').addEventListener('click', loadEmbeddedQuestions);
    document.getElementById('shuffleBtn').addEventListener('click', () => {
      if (!questions.length) return; shuffleInPlace(questions); idx = 0; updateProgress(); renderQuestion(questions[idx], questionContainer);
      showFeedback(true, 'Questions shuffled.');
    });
    document.getElementById('prevBtn').addEventListener('click', () => {
      if (!questions.length) return; idx = Math.max(0, idx - 1); updateProgress(); renderQuestion(questions[idx], questionContainer);
    });
    document.getElementById('nextBtn').addEventListener('click', () => {
      if (!questions.length) return; idx = Math.min(questions.length - 1, idx + 1); updateProgress(); renderQuestion(questions[idx], questionContainer);
    });
    document.getElementById('submitBtn').addEventListener('click', () => {
      if (!questions.length) return;
      if (!isAnswered(questions[idx])) { showFeedback(false, 'Please answer the question before submitting.'); return; }
      gradeCurrent(); summarize(); if (idx === questions.length - 1) { document.getElementById('reviewMarkedBtn')?.focus(); }
    });
    document.getElementById('resetBtn').addEventListener('click', () => {
      for (const k of Object.keys(userAnswers)) delete userAnswers[k];
      for (const k of Object.keys(reviewMarks)) delete reviewMarks[k];
      questions = [...originalQuestions]; reviewingMarked = false; idx = 0;
      showFeedback(true, 'All answers and marks cleared.');
      if (questions.length) renderQuestion(questions[idx], questionContainer);
      summarize();
    });

    // Calculate Score button
    document.getElementById('calcScoreBtn').addEventListener('click', () => {
      if (!questions.length) { showFeedback(false, 'No questions loaded.'); return; }
      calcScoreAll();
    });

    /* ------------------------ Boot ------------------------- */
    // Do NOT auto-load embedded questions; wait for Start Exam.
    window.addEventListener('DOMContentLoaded', () => {
      updateProgress();
    });

  </script>
</body>
</html>
